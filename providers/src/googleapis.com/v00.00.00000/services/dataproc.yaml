openapi: 3.1.0
info:
  contact:
    name: StackQL Studios
    url: https://github.com/stackql/google-discovery-to-openapi
    email: info@stackql.io
  title: Cloud Dataproc API
  description: Manages Hadoop-based clusters and jobs on Google Cloud Platform.
  version: v1
  x-discovery-doc-revision: '20251104'
  x-generated-date: '2025-12-10'
externalDocs:
  url: https://cloud.google.com/dataproc/
servers:
  - url: https://dataproc.googleapis.com
components:
  securitySchemes:
    Oauth2:
      type: oauth2
      description: Oauth 2.0 implicit authentication
      flows:
        implicit:
          authorizationUrl: https://accounts.google.com/o/oauth2/auth
          scopes: &ref_0
            https://www.googleapis.com/auth/cloud-platform: >-
              See, edit, configure, and delete your Google Cloud data and see
              the email address for your Google Account.
    Oauth2c:
      type: oauth2
      description: Oauth 2.0 authorization code authentication
      flows:
        authorizationCode:
          authorizationUrl: https://accounts.google.com/o/oauth2/auth
          tokenUrl: https://accounts.google.com/o/oauth2/token
          scopes: *ref_0
  schemas:
    DriverSchedulingConfig:
      properties:
        vcores:
          description: Required. The number of vCPUs the driver is requesting.
          type: integer
          format: int32
        memoryMb:
          format: int32
          description: Required. The amount of memory in MB the driver is requesting.
          type: integer
      description: Driver scheduling configuration.
      type: object
      id: DriverSchedulingConfig
    KerberosConfig:
      id: KerberosConfig
      properties:
        kmsKeyUri:
          description: Optional. The URI of the KMS key used to encrypt sensitive files.
          type: string
        crossRealmTrustKdc:
          description: >-
            Optional. The KDC (IP or hostname) for the remote trusted realm in a
            cross realm trust relationship.
          type: string
        kdcDbKeyUri:
          type: string
          description: >-
            Optional. The Cloud Storage URI of a KMS encrypted file containing
            the master key of the KDC database.
        keyPasswordUri:
          description: >-
            Optional. The Cloud Storage URI of a KMS encrypted file containing
            the password to the user provided key. For the self-signed
            certificate, this password is generated by Dataproc.
          type: string
        keystoreUri:
          description: >-
            Optional. The Cloud Storage URI of the keystore file used for SSL
            encryption. If not provided, Dataproc will provide a self-signed
            certificate.
          type: string
        keystorePasswordUri:
          description: >-
            Optional. The Cloud Storage URI of a KMS encrypted file containing
            the password to the user provided keystore. For the self-signed
            certificate, this password is generated by Dataproc.
          type: string
        realm:
          description: >-
            Optional. The name of the on-cluster Kerberos realm. If not
            specified, the uppercased domain of hostnames will be the realm.
          type: string
        tgtLifetimeHours:
          type: integer
          format: int32
          description: >-
            Optional. The lifetime of the ticket granting ticket, in hours. If
            not specified, or user specifies 0, then default value 10 will be
            used.
        crossRealmTrustAdminServer:
          description: >-
            Optional. The admin server (IP or hostname) for the remote trusted
            realm in a cross realm trust relationship.
          type: string
        truststoreUri:
          description: >-
            Optional. The Cloud Storage URI of the truststore file used for SSL
            encryption. If not provided, Dataproc will provide a self-signed
            certificate.
          type: string
        truststorePasswordUri:
          description: >-
            Optional. The Cloud Storage URI of a KMS encrypted file containing
            the password to the user provided truststore. For the self-signed
            certificate, this password is generated by Dataproc.
          type: string
        crossRealmTrustSharedPasswordUri:
          description: >-
            Optional. The Cloud Storage URI of a KMS encrypted file containing
            the shared password between the on-cluster Kerberos realm and the
            remote trusted realm, in a cross realm trust relationship.
          type: string
        crossRealmTrustRealm:
          description: >-
            Optional. The remote realm the Dataproc on-cluster KDC will trust,
            should the user enable cross realm trust.
          type: string
        rootPrincipalPasswordUri:
          description: >-
            Optional. The Cloud Storage URI of a KMS encrypted file containing
            the root principal password.
          type: string
        enableKerberos:
          description: >-
            Optional. Flag to indicate whether to Kerberize the cluster
            (default: false). Set this field to true to enable Kerberos on a
            cluster.
          type: boolean
      description: Specifies Kerberos related configuration.
      type: object
    AnalyzeBatchRequest:
      id: AnalyzeBatchRequest
      description: A request to analyze a batch workload.
      type: object
      properties:
        requestorId:
          deprecated: true
          type: string
          description: >-
            Optional. The requestor ID is used to identify if the request comes
            from a GCA investigation or the old Ask Gemini Experience. 
        requestId:
          description: >-
            Optional. A unique ID used to identify the request. If the service
            receives two AnalyzeBatchRequest
            (http://cloud/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.AnalyzeBatchRequest)s
            with the same request_id, the second request is ignored and the
            Operation that corresponds to the first request created and stored
            in the backend is returned.Recommendation: Set this value to a UUID
            (https://en.wikipedia.org/wiki/Universally_unique_identifier).The
            value must contain only letters (a-z, A-Z), numbers (0-9),
            underscores (_), and hyphens (-). The maximum length is 40
            characters.
          type: string
    TaskData:
      properties:
        speculative:
          type: boolean
        taskId:
          type: string
          format: int64
        partitionId:
          type: integer
          format: int32
        taskMetrics:
          $ref: '#/components/schemas/TaskMetrics'
        status:
          type: string
        hasMetrics:
          type: boolean
        taskLocality:
          type: string
        attempt:
          type: integer
          format: int32
        launchTime:
          type: string
          format: google-datetime
        index:
          format: int32
          type: integer
        durationMillis:
          type: string
          format: int64
        stageAttemptId:
          type: integer
          format: int32
        executorLogs:
          type: object
          additionalProperties:
            type: string
        gettingResultTimeMillis:
          format: int64
          type: string
        errorMessage:
          type: string
        accumulatorUpdates:
          type: array
          items:
            $ref: '#/components/schemas/AccumulableInfo'
        resultFetchStart:
          format: google-datetime
          type: string
        executorId:
          type: string
        stageId:
          format: int64
          type: string
        schedulerDelayMillis:
          type: string
          format: int64
        host:
          type: string
      description: Data corresponding to tasks created by spark.
      id: TaskData
      type: object
    AccumulableInfo:
      properties:
        name:
          type: string
        value:
          type: string
        update:
          type: string
        accumullableInfoId:
          type: string
          format: int64
      type: object
      id: AccumulableInfo
    SearchSessionSparkApplicationExecutorsResponse:
      type: object
      description: List of Executors associated with a Spark Application.
      properties:
        nextPageToken:
          type: string
          description: >-
            This token is included in the response if there are more results to
            fetch. To fetch additional results, provide this value as the
            page_token in a subsequent
            SearchSessionSparkApplicationExecutorsRequest.
        sparkApplicationExecutors:
          type: array
          description: Details about executors used by the application.
          items:
            $ref: '#/components/schemas/ExecutorSummary'
      id: SearchSessionSparkApplicationExecutorsResponse
    OutputQuantileMetrics:
      type: object
      id: OutputQuantileMetrics
      properties:
        recordsWritten:
          $ref: '#/components/schemas/Quantiles'
        bytesWritten:
          $ref: '#/components/schemas/Quantiles'
    KubernetesClusterConfig:
      description: The configuration for running the Dataproc cluster on Kubernetes.
      properties:
        kubernetesSoftwareConfig:
          description: >-
            Optional. The software configuration for this Dataproc cluster
            running on Kubernetes.
          $ref: '#/components/schemas/KubernetesSoftwareConfig'
        gkeClusterConfig:
          $ref: '#/components/schemas/GkeClusterConfig'
          description: Required. The configuration for running the Dataproc cluster on GKE.
        kubernetesNamespace:
          type: string
          description: >-
            Optional. A namespace within the Kubernetes cluster to deploy into.
            If this namespace does not exist, it is created. If it exists,
            Dataproc verifies that another Dataproc VirtualCluster is not
            installed into it. If not specified, the name of the Dataproc
            Cluster is used.
      type: object
      id: KubernetesClusterConfig
    AccessSparkApplicationJobResponse:
      description: Details of a particular job associated with Spark Application
      type: object
      id: AccessSparkApplicationJobResponse
      properties:
        jobData:
          description: Output only. Data corresponding to a spark job.
          $ref: '#/components/schemas/JobData'
          readOnly: true
    ResourceInformation:
      type: object
      properties:
        addresses:
          type: array
          items:
            type: string
        name:
          type: string
      id: ResourceInformation
    SearchSparkApplicationExecutorStageSummaryResponse:
      properties:
        sparkApplicationStageExecutors:
          items:
            $ref: '#/components/schemas/ExecutorStageSummary'
          type: array
          description: Details about executors used by the application stage.
        nextPageToken:
          type: string
          description: >-
            This token is included in the response if there are more results to
            fetch. To fetch additional results, provide this value as the
            page_token in a subsequent
            SearchSparkApplicationExecutorsListRequest.
      type: object
      id: SearchSparkApplicationExecutorStageSummaryResponse
      description: List of Executors associated with a Spark Application Stage.
    RuntimeInfo:
      description: Runtime information about workload execution.
      type: object
      id: RuntimeInfo
      properties:
        currentUsage:
          readOnly: true
          description: Output only. Snapshot of current workload resource usage.
          $ref: '#/components/schemas/UsageSnapshot'
        endpoints:
          additionalProperties:
            type: string
          type: object
          readOnly: true
          description: >-
            Output only. Map of remote access endpoints (such as web interfaces
            and APIs) to their URIs.
        diagnosticOutputUri:
          type: string
          description: >-
            Output only. A URI pointing to the location of the diagnostics
            tarball.
          readOnly: true
        approximateUsage:
          readOnly: true
          description: >-
            Output only. Approximate workload resource usage, calculated when
            the workload completes (see Dataproc Serverless pricing
            (https://cloud.google.com/dataproc-serverless/pricing)).Note: This
            metric calculation may change in the future, for example, to capture
            cumulative workload resource consumption during workload execution
            (see the Dataproc Serverless release notes
            (https://cloud.google.com/dataproc-serverless/docs/release-notes)
            for announcements, changes, fixes and other Dataproc developments).
          $ref: '#/components/schemas/UsageMetrics'
        propertiesInfo:
          $ref: '#/components/schemas/PropertiesInfo'
          description: Optional. Properties of the workload organized by origin.
        outputUri:
          readOnly: true
          description: >-
            Output only. A URI pointing to the location of the stdout and stderr
            of the workload.
          type: string
    ShieldedInstanceConfig:
      type: object
      properties:
        enableVtpm:
          description: Optional. Defines whether instances have the vTPM enabled.
          type: boolean
        enableSecureBoot:
          type: boolean
          description: Optional. Defines whether instances have Secure Boot enabled.
        enableIntegrityMonitoring:
          description: >-
            Optional. Defines whether instances have integrity monitoring
            enabled.
          type: boolean
      id: ShieldedInstanceConfig
      description: >-
        Shielded Instance Config for clusters using Compute Engine Shielded VMs
        (https://cloud.google.com/security/shielded-cloud/shielded-vm).
    InstanceSelection:
      id: InstanceSelection
      type: object
      properties:
        rank:
          format: int32
          type: integer
          description: >-
            Optional. Preference of this instance selection. Lower number means
            higher preference. Dataproc will first try to create a VM based on
            the machine-type with priority rank and fallback to next rank based
            on availability. Machine types and instance selections with the same
            priority have the same preference.
        machineTypes:
          type: array
          items:
            type: string
          description: Optional. Full machine-type names, e.g. "n1-standard-16".
      description: Defines machines types and a rank to which the machines types belong.
    EnvironmentConfig:
      type: object
      id: EnvironmentConfig
      properties:
        executionConfig:
          $ref: '#/components/schemas/ExecutionConfig'
          description: Optional. Execution configuration for a workload.
        peripheralsConfig:
          $ref: '#/components/schemas/PeripheralsConfig'
          description: Optional. Peripherals configuration that workload has access to.
      description: Environment configuration for a workload.
    TestIamPermissionsRequest:
      properties:
        permissions:
          items:
            type: string
          description: >-
            The set of permissions to check for the resource. Permissions with
            wildcards (such as * or storage.*) are not allowed. For more
            information see IAM Overview
            (https://cloud.google.com/iam/docs/overview#permissions).
          type: array
      description: Request message for TestIamPermissions method.
      id: TestIamPermissionsRequest
      type: object
    StartupConfig:
      properties:
        requiredRegistrationFraction:
          format: double
          description: >-
            Optional. The config setting to enable cluster creation/ updation to
            be successful only after required_registration_fraction of instances
            are up and running. This configuration is applicable to only
            secondary workers for now. The cluster will fail if
            required_registration_fraction of instances are not available. This
            will include instance creation, agent registration, and service
            registration (if enabled).
          type: number
      id: StartupConfig
      type: object
      description: >-
        Configuration to handle the startup of instances during cluster create
        and update process.
    SearchSparkApplicationStageAttemptTasksResponse:
      id: SearchSparkApplicationStageAttemptTasksResponse
      properties:
        nextPageToken:
          description: >-
            This token is included in the response if there are more results to
            fetch. To fetch additional results, provide this value as the
            page_token in a subsequent
            ListSparkApplicationStageAttemptTasksRequest.
          type: string
        sparkApplicationStageAttemptTasks:
          readOnly: true
          type: array
          description: Output only. Data corresponding to tasks created by spark.
          items:
            $ref: '#/components/schemas/TaskData'
      type: object
      description: List of tasks for a stage of a Spark Application
    SearchSparkApplicationStageAttemptsResponse:
      id: SearchSparkApplicationStageAttemptsResponse
      description: A list of Stage Attempts for a Stage of a Spark Application.
      properties:
        sparkApplicationStageAttempts:
          items:
            $ref: '#/components/schemas/StageData'
          description: Output only. Data corresponding to a stage attempts
          readOnly: true
          type: array
        nextPageToken:
          description: >-
            This token is included in the response if there are more results to
            fetch. To fetch additional results, provide this value as the
            page_token in a subsequent ListSparkApplicationStageAttemptsRequest.
          type: string
      type: object
    WriteSparkApplicationContextRequest:
      properties:
        parent:
          type: string
          description: Required. Parent (Batch) resource reference.
        sparkWrapperObjects:
          type: array
          items:
            $ref: '#/components/schemas/SparkWrapperObject'
      id: WriteSparkApplicationContextRequest
      description: Write Spark Application data to internal storage systems
      type: object
    SearchSessionSparkApplicationSqlQueriesResponse:
      type: object
      description: List of all queries for a Spark Application.
      properties:
        sparkApplicationSqlQueries:
          type: array
          readOnly: true
          items:
            $ref: '#/components/schemas/SqlExecutionUiData'
          description: Output only. SQL Execution Data
        nextPageToken:
          type: string
          description: >-
            This token is included in the response if there are more results to
            fetch. To fetch additional results, provide this value as the
            page_token in a subsequent
            SearchSessionSparkApplicationSqlQueriesRequest.
      id: SearchSessionSparkApplicationSqlQueriesResponse
    WriteSessionSparkApplicationContextRequest:
      properties:
        sparkWrapperObjects:
          description: >-
            Required. The batch of spark application context objects sent for
            ingestion.
          items:
            $ref: '#/components/schemas/SparkWrapperObject'
          type: array
        parent:
          type: string
          description: Required. Parent (Batch) resource reference.
      description: Write Spark Application data to internal storage systems
      id: WriteSessionSparkApplicationContextRequest
      type: object
    StreamingQueryData:
      properties:
        name:
          type: string
        streamingQueryId:
          type: string
        runId:
          type: string
        startTimestamp:
          format: int64
          type: string
        endTimestamp:
          type: string
          format: int64
        isActive:
          type: boolean
        exception:
          type: string
      id: StreamingQueryData
      type: object
      description: Streaming
    SparkPlanGraphCluster:
      type: object
      description: Represents a tree of spark plan.
      properties:
        sparkPlanGraphClusterId:
          type: string
          format: int64
        desc:
          type: string
        name:
          type: string
        nodes:
          type: array
          items:
            $ref: '#/components/schemas/SparkPlanGraphNodeWrapper'
        metrics:
          type: array
          items:
            $ref: '#/components/schemas/SqlPlanMetric'
      id: SparkPlanGraphCluster
    Expr:
      properties:
        title:
          description: >-
            Optional. Title for the expression, i.e. a short string describing
            its purpose. This can be used e.g. in UIs which allow to enter the
            expression.
          type: string
        location:
          type: string
          description: >-
            Optional. String indicating the location of the expression for error
            reporting, e.g. a file name and a position in the file.
        expression:
          type: string
          description: >-
            Textual representation of an expression in Common Expression
            Language syntax.
        description:
          type: string
          description: >-
            Optional. Description of the expression. This is a longer text which
            describes the expression, e.g. when hovered over it in a UI.
      id: Expr
      type: object
      description: >-
        Represents a textual expression in the Common Expression Language (CEL)
        syntax. CEL is a C-like expression language. The syntax and semantics of
        CEL are documented at https://github.com/google/cel-spec.Example
        (Comparison): title: "Summary size limit" description: "Determines if a
        summary is less than 100 chars" expression: "document.summary.size() <
        100" Example (Equality): title: "Requestor is owner" description:
        "Determines if requestor is the document owner" expression:
        "document.owner == request.auth.claims.email" Example (Logic): title:
        "Public documents" description: "Determine whether the document should
        be publicly visible" expression: "document.type != 'private' &&
        document.type != 'internal'" Example (Data Manipulation): title:
        "Notification string" description: "Create a notification string with a
        timestamp." expression: "'New message received at ' +
        string(document.create_time)" The exact variables and functions that may
        be referenced within an expression are determined by the service that
        evaluates it. See the service documentation for additional information.
    MetastoreConfig:
      id: MetastoreConfig
      type: object
      description: Specifies a Metastore configuration.
      properties:
        dataprocMetastoreService:
          type: string
          description: >-
            Required. Resource name of an existing Dataproc Metastore
            service.Example:
            projects/[project_id]/locations/[dataproc_region]/services/[service-name]
    SparkWrapperObject:
      type: object
      id: SparkWrapperObject
      description: >-
        Outer message that contains the data obtained from spark listener,
        packaged with information that is required to process it.
      properties:
        eventTimestamp:
          type: string
          description: VM Timestamp associated with the data object.
          format: google-datetime
        applicationId:
          description: Application Id created by Spark.
          type: string
        taskData:
          $ref: '#/components/schemas/TaskData'
        appSummary:
          $ref: '#/components/schemas/AppSummary'
        executorSummary:
          $ref: '#/components/schemas/ExecutorSummary'
        processSummary:
          $ref: '#/components/schemas/ProcessSummary'
        nativeSqlExecutionUiData:
          description: Native SQL Execution Info
          $ref: '#/components/schemas/NativeSqlExecutionUiData'
        applicationEnvironmentInfo:
          $ref: '#/components/schemas/ApplicationEnvironmentInfo'
        streamingQueryProgress:
          $ref: '#/components/schemas/StreamingQueryProgress'
        resourceProfileInfo:
          $ref: '#/components/schemas/ResourceProfileInfo'
        speculationStageSummary:
          $ref: '#/components/schemas/SpeculationStageSummary'
        executorStageSummary:
          $ref: '#/components/schemas/ExecutorStageSummary'
        stageData:
          $ref: '#/components/schemas/StageData'
        applicationInfo:
          $ref: '#/components/schemas/ApplicationInfo'
        jobData:
          $ref: '#/components/schemas/JobData'
        streamBlockData:
          $ref: '#/components/schemas/StreamBlockData'
        sqlExecutionUiData:
          $ref: '#/components/schemas/SqlExecutionUiData'
        poolData:
          $ref: '#/components/schemas/PoolData'
        nativeBuildInfoUiData:
          $ref: '#/components/schemas/NativeBuildInfoUiData'
          description: Native Build Info
        rddStorageInfo:
          $ref: '#/components/schemas/RddStorageInfo'
        rddOperationGraph:
          $ref: '#/components/schemas/RddOperationGraph'
        streamingQueryData:
          $ref: '#/components/schemas/StreamingQueryData'
        sparkPlanGraph:
          $ref: '#/components/schemas/SparkPlanGraph'
    TaskMetrics:
      type: object
      description: Executor Task Metrics
      properties:
        diskBytesSpilled:
          format: int64
          type: string
        peakExecutionMemoryBytes:
          type: string
          format: int64
        memoryBytesSpilled:
          type: string
          format: int64
        inputMetrics:
          $ref: '#/components/schemas/InputMetrics'
        resultSize:
          type: string
          format: int64
        shuffleReadMetrics:
          $ref: '#/components/schemas/ShuffleReadMetrics'
        resultSerializationTimeMillis:
          type: string
          format: int64
        outputMetrics:
          $ref: '#/components/schemas/OutputMetrics'
        shuffleWriteMetrics:
          $ref: '#/components/schemas/ShuffleWriteMetrics'
        executorDeserializeCpuTimeNanos:
          type: string
          format: int64
        jvmGcTimeMillis:
          format: int64
          type: string
        executorRunTimeMillis:
          format: int64
          type: string
        executorCpuTimeNanos:
          type: string
          format: int64
        executorDeserializeTimeMillis:
          type: string
          format: int64
      id: TaskMetrics
    ApplicationEnvironmentInfo:
      properties:
        metricsProperties:
          type: object
          additionalProperties:
            type: string
        systemProperties:
          type: object
          additionalProperties:
            type: string
        sparkProperties:
          additionalProperties:
            type: string
          type: object
        classpathEntries:
          additionalProperties:
            type: string
          type: object
        runtime:
          $ref: '#/components/schemas/SparkRuntimeInfo'
        resourceProfiles:
          type: array
          items:
            $ref: '#/components/schemas/ResourceProfileInfo'
        hadoopProperties:
          type: object
          additionalProperties:
            type: string
      type: object
      id: ApplicationEnvironmentInfo
      description: Details about the Environment that the application is running in.
    AppSummary:
      id: AppSummary
      properties:
        numCompletedStages:
          format: int32
          type: integer
        numCompletedJobs:
          format: int32
          type: integer
      type: object
    ClusterStatus:
      id: ClusterStatus
      properties:
        stateStartTime:
          format: google-datetime
          type: string
          description: >-
            Output only. Time when this state was entered (see JSON
            representation of Timestamp
            (https://developers.google.com/protocol-buffers/docs/proto3#json)).
          readOnly: true
        substate:
          readOnly: true
          enumDescriptions:
            - The cluster substate is unknown.
            - >-
              The cluster is known to be in an unhealthy state (for example,
              critical daemons are not running or HDFS capacity is
              exhausted).Applies to RUNNING state.
            - >-
              The agent-reported status is out of date (may occur if Dataproc
              loses communication with Agent).Applies to RUNNING state.
          type: string
          enum:
            - UNSPECIFIED
            - UNHEALTHY
            - STALE_STATUS
          description: >-
            Output only. Additional state information that includes status
            reported by the agent.
        state:
          description: Output only. The cluster's state.
          readOnly: true
          enum:
            - UNKNOWN
            - CREATING
            - RUNNING
            - ERROR
            - ERROR_DUE_TO_UPDATE
            - DELETING
            - UPDATING
            - STOPPING
            - STOPPED
            - STARTING
            - REPAIRING
            - SCHEDULED
          enumDescriptions:
            - The cluster state is unknown.
            - The cluster is being created and set up. It is not ready for use.
            - >-
              The cluster is currently running and healthy. It is ready for
              use.Note: The cluster state changes from "creating" to "running"
              status after the master node(s), first two primary worker nodes
              (and the last primary worker node if primary workers > 2) are
              running.
            - The cluster encountered an error. It is not ready for use.
            - >-
              The cluster has encountered an error while being updated. Jobs can
              be submitted to the cluster, but the cluster cannot be updated.
            - The cluster is being deleted. It cannot be used.
            - >-
              The cluster is being updated. It continues to accept and process
              jobs.
            - The cluster is being stopped. It cannot be used.
            - The cluster is currently stopped. It is not ready for use.
            - The cluster is being started. It is not ready for use.
            - The cluster is being repaired. It is not ready for use.
            - >-
              Cluster creation is currently waiting for resources to be
              available. Once all resources are available, it will transition to
              CREATING and then RUNNING.
          type: string
        detail:
          type: string
          readOnly: true
          description: Optional. Output only. Details of cluster's state.
      description: The status of a cluster and its instances.
      type: object
    BatchOperationMetadata:
      properties:
        description:
          type: string
          description: Short description of the operation.
        batch:
          description: Name of the batch for the operation.
          type: string
        batchUuid:
          type: string
          description: Batch UUID for the operation.
        doneTime:
          format: google-datetime
          type: string
          description: The time when the operation finished.
        labels:
          additionalProperties:
            type: string
          description: Labels associated with the operation.
          type: object
        createTime:
          description: The time when the operation was created.
          format: google-datetime
          type: string
        warnings:
          description: Warnings encountered during operation execution.
          items:
            type: string
          type: array
        operationType:
          description: The operation type.
          enum:
            - BATCH_OPERATION_TYPE_UNSPECIFIED
            - BATCH
          enumDescriptions:
            - Batch operation type is unknown.
            - Batch operation type.
          type: string
      type: object
      description: Metadata describing the Batch operation.
      id: BatchOperationMetadata
    InputMetrics:
      properties:
        bytesRead:
          type: string
          format: int64
        recordsRead:
          format: int64
          type: string
      description: Metrics about the input data read by the task.
      type: object
      id: InputMetrics
    EncryptionConfig:
      description: Encryption settings for the cluster.
      type: object
      id: EncryptionConfig
      properties:
        gcePdKmsKeyName:
          type: string
          description: >-
            Optional. The Cloud KMS key resource name to use for persistent disk
            encryption for all instances in the cluster. See Use CMEK with
            cluster data
            (https://cloud.google.com//dataproc/docs/concepts/configuring-clusters/customer-managed-encryption#use_cmek_with_cluster_data)
            for more information.
        kmsKey:
          description: >-
            Optional. The Cloud KMS key resource name to use for cluster
            persistent disk and job argument encryption. See Use CMEK with
            cluster data
            (https://cloud.google.com//dataproc/docs/concepts/configuring-clusters/customer-managed-encryption#use_cmek_with_cluster_data)
            for more information.When this key resource name is provided, the
            following job arguments of the following job types submitted to the
            cluster are encrypted using CMEK: FlinkJob args
            (https://cloud.google.com/dataproc/docs/reference/rest/v1/FlinkJob)
            HadoopJob args
            (https://cloud.google.com/dataproc/docs/reference/rest/v1/HadoopJob)
            SparkJob args
            (https://cloud.google.com/dataproc/docs/reference/rest/v1/SparkJob)
            SparkRJob args
            (https://cloud.google.com/dataproc/docs/reference/rest/v1/SparkRJob)
            PySparkJob args
            (https://cloud.google.com/dataproc/docs/reference/rest/v1/PySparkJob)
            SparkSqlJob
            (https://cloud.google.com/dataproc/docs/reference/rest/v1/SparkSqlJob)
            scriptVariables and queryList.queries HiveJob
            (https://cloud.google.com/dataproc/docs/reference/rest/v1/HiveJob)
            scriptVariables and queryList.queries PigJob
            (https://cloud.google.com/dataproc/docs/reference/rest/v1/PigJob)
            scriptVariables and queryList.queries PrestoJob
            (https://cloud.google.com/dataproc/docs/reference/rest/v1/PrestoJob)
            scriptVariables and queryList.queries
          type: string
    StageMetrics:
      id: StageMetrics
      properties:
        resultSerializationTimeMillis:
          type: string
          format: int64
        peakExecutionMemoryBytes:
          format: int64
          type: string
        executorRunTimeMillis:
          format: int64
          type: string
        executorDeserializeTimeMillis:
          type: string
          format: int64
        memoryBytesSpilled:
          format: int64
          type: string
        diskBytesSpilled:
          format: int64
          type: string
        resultSize:
          format: int64
          type: string
        executorCpuTimeNanos:
          type: string
          format: int64
        executorDeserializeCpuTimeNanos:
          type: string
          format: int64
        stageOutputMetrics:
          $ref: '#/components/schemas/StageOutputMetrics'
        jvmGcTimeMillis:
          type: string
          format: int64
        stageShuffleReadMetrics:
          $ref: '#/components/schemas/StageShuffleReadMetrics'
        stageInputMetrics:
          $ref: '#/components/schemas/StageInputMetrics'
        stageShuffleWriteMetrics:
          $ref: '#/components/schemas/StageShuffleWriteMetrics'
      description: Stage Level Aggregated Metrics
      type: object
    RddOperationGraph:
      description: >-
        Graph representing RDD dependencies. Consists of edges and a root
        cluster.
      id: RddOperationGraph
      type: object
      properties:
        incomingEdges:
          type: array
          items:
            $ref: '#/components/schemas/RddOperationEdge'
        rootCluster:
          $ref: '#/components/schemas/RddOperationCluster'
        stageId:
          type: string
          format: int64
        outgoingEdges:
          type: array
          items:
            $ref: '#/components/schemas/RddOperationEdge'
        edges:
          items:
            $ref: '#/components/schemas/RddOperationEdge'
          type: array
    SqlExecutionUiData:
      description: SQL Execution Data
      properties:
        submissionTime:
          format: google-datetime
          type: string
        details:
          type: string
        description:
          type: string
        executionId:
          type: string
          format: int64
        completionTime:
          format: google-datetime
          type: string
        jobs:
          additionalProperties:
            type: string
            enum:
              - JOB_EXECUTION_STATUS_UNSPECIFIED
              - JOB_EXECUTION_STATUS_RUNNING
              - JOB_EXECUTION_STATUS_SUCCEEDED
              - JOB_EXECUTION_STATUS_FAILED
              - JOB_EXECUTION_STATUS_UNKNOWN
            enumDescriptions:
              - ''
              - ''
              - ''
              - ''
              - ''
          type: object
        modifiedConfigs:
          additionalProperties:
            type: string
          type: object
        errorMessage:
          type: string
        rootExecutionId:
          type: string
          format: int64
        metricValues:
          type: object
          additionalProperties:
            type: string
        metricValuesIsNull:
          type: boolean
        stages:
          type: array
          items:
            format: int64
            type: string
        physicalPlanDescription:
          type: string
        metrics:
          items:
            $ref: '#/components/schemas/SqlPlanMetric'
          type: array
      type: object
      id: SqlExecutionUiData
    AccessSessionSparkApplicationSqlSparkPlanGraphResponse:
      description: >-
        SparkPlanGraph for a Spark Application execution limited to maximum
        10000 clusters.
      type: object
      properties:
        sparkPlanGraph:
          description: SparkPlanGraph for a Spark Application execution.
          $ref: '#/components/schemas/SparkPlanGraph'
      id: AccessSessionSparkApplicationSqlSparkPlanGraphResponse
    ShuffleReadQuantileMetrics:
      properties:
        remoteBytesReadToDisk:
          $ref: '#/components/schemas/Quantiles'
        shufflePushReadMetrics:
          $ref: '#/components/schemas/ShufflePushReadQuantileMetrics'
        readBytes:
          $ref: '#/components/schemas/Quantiles'
        fetchWaitTimeMillis:
          $ref: '#/components/schemas/Quantiles'
        remoteBlocksFetched:
          $ref: '#/components/schemas/Quantiles'
        localBlocksFetched:
          $ref: '#/components/schemas/Quantiles'
        remoteBytesRead:
          $ref: '#/components/schemas/Quantiles'
        readRecords:
          $ref: '#/components/schemas/Quantiles'
        remoteReqsDuration:
          $ref: '#/components/schemas/Quantiles'
        totalBlocksFetched:
          $ref: '#/components/schemas/Quantiles'
      id: ShuffleReadQuantileMetrics
      type: object
    InstanceReference:
      properties:
        publicKey:
          description: The public RSA key used for sharing data with this instance.
          type: string
        instanceId:
          type: string
          description: The unique identifier of the Compute Engine instance.
        instanceName:
          description: The user-friendly name of the Compute Engine instance.
          type: string
        publicEciesKey:
          description: The public ECIES key used for sharing data with this instance.
          type: string
      description: A reference to a Compute Engine instance.
      id: InstanceReference
      type: object
    SparkRBatch:
      id: SparkRBatch
      type: object
      description: >-
        A configuration for running an Apache SparkR
        (https://spark.apache.org/docs/latest/sparkr.html) batch workload.
      properties:
        fileUris:
          items:
            type: string
          type: array
          description: >-
            Optional. HCFS URIs of files to be placed in the working directory
            of each executor.
        mainRFileUri:
          type: string
          description: >-
            Required. The HCFS URI of the main R file to use as the driver. Must
            be a .R or .r file.
        args:
          items:
            type: string
          type: array
          description: >-
            Optional. The arguments to pass to the Spark driver. Do not include
            arguments that can be set as batch properties, such as --conf, since
            a collision can occur that causes an incorrect batch submission.
        archiveUris:
          description: >-
            Optional. HCFS URIs of archives to be extracted into the working
            directory of each executor. Supported file types: .jar, .tar,
            .tar.gz, .tgz, and .zip.
          items:
            type: string
          type: array
    StageAttemptTasksSummary:
      type: object
      description: Data related to tasks summary for a Spark Stage Attempt
      id: StageAttemptTasksSummary
      properties:
        numTasks:
          type: integer
          format: int32
        stageId:
          format: int64
          type: string
        numPendingTasks:
          format: int32
          type: integer
        numSuccessTasks:
          format: int32
          type: integer
        numFailedTasks:
          format: int32
          type: integer
        numRunningTasks:
          type: integer
          format: int32
        numKilledTasks:
          format: int32
          type: integer
        applicationId:
          type: string
        stageAttemptId:
          format: int32
          type: integer
    ClusterConfig:
      properties:
        configBucket:
          description: >-
            Optional. A Cloud Storage bucket used to stage job dependencies,
            config files, and job driver console output. If you do not specify a
            staging bucket, Cloud Dataproc will determine a Cloud Storage
            location (US, ASIA, or EU) for your cluster's staging bucket
            according to the Compute Engine zone where your cluster is deployed,
            and then create and manage this project-level, per-location bucket
            (see Dataproc staging and temp buckets
            (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)).
            This field requires a Cloud Storage bucket name, not a gs://... URI
            to a Cloud Storage bucket.
          type: string
        clusterTier:
          description: Optional. The cluster tier.
          type: string
          enum:
            - CLUSTER_TIER_UNSPECIFIED
            - CLUSTER_TIER_STANDARD
            - CLUSTER_TIER_PREMIUM
          enumDescriptions:
            - Not set. Works the same as CLUSTER_TIER_STANDARD.
            - Standard Dataproc cluster.
            - Premium Dataproc cluster.
        initializationActions:
          description: >-
            Optional. Commands to execute on each node after config is
            completed. By default, executables are run on master and all worker
            nodes. You can test a node's role metadata to run an executable on a
            master or worker node, as shown below using curl (you can also use
            wget): ROLE=$(curl -H Metadata-Flavor:Google
            http://metadata/computeMetadata/v1/instance/attributes/dataproc-role)
            if [[ "${ROLE}" == 'Master' ]]; then ... master specific actions ...
            else ... worker specific actions ... fi 
          items:
            $ref: '#/components/schemas/NodeInitializationAction'
          type: array
        gceClusterConfig:
          $ref: '#/components/schemas/GceClusterConfig'
          description: >-
            Optional. The shared Compute Engine config settings for all
            instances in a cluster.
        metastoreConfig:
          description: Optional. Metastore configuration.
          $ref: '#/components/schemas/MetastoreConfig'
        lifecycleConfig:
          $ref: '#/components/schemas/LifecycleConfig'
          description: Optional. Lifecycle setting for the cluster.
        softwareConfig:
          description: Optional. The config settings for cluster software.
          $ref: '#/components/schemas/SoftwareConfig'
        dataprocMetricConfig:
          description: Optional. The config for Dataproc metrics.
          $ref: '#/components/schemas/DataprocMetricConfig'
        auxiliaryNodeGroups:
          items:
            $ref: '#/components/schemas/AuxiliaryNodeGroup'
          type: array
          description: Optional. The node group settings.
        gkeClusterConfig:
          description: >-
            Optional. BETA. The Kubernetes Engine config for Dataproc clusters
            deployed to The Kubernetes Engine config for Dataproc clusters
            deployed to Kubernetes. These config settings are mutually exclusive
            with Compute Engine-based options, such as gce_cluster_config,
            master_config, worker_config, secondary_worker_config, and
            autoscaling_config.
          $ref: '#/components/schemas/GkeClusterConfig'
          deprecated: true
        securityConfig:
          $ref: '#/components/schemas/SecurityConfig'
          description: Optional. Security settings for the cluster.
        workerConfig:
          description: >-
            Optional. The Compute Engine config settings for the cluster's
            worker instances.
          $ref: '#/components/schemas/InstanceGroupConfig'
        clusterType:
          type: string
          enum:
            - CLUSTER_TYPE_UNSPECIFIED
            - STANDARD
            - SINGLE_NODE
            - ZERO_SCALE
          description: Optional. The type of the cluster.
          enumDescriptions:
            - Not set.
            - Standard dataproc cluster with a minimum of two primary workers.
            - >-
              https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/single-node-clusters
            - >-
              Clusters that can use only secondary workers and be scaled down to
              zero secondary worker nodes.
        endpointConfig:
          description: Optional. Port/endpoint configuration for this cluster
          $ref: '#/components/schemas/EndpointConfig'
        secondaryWorkerConfig:
          description: >-
            Optional. The Compute Engine config settings for a cluster's
            secondary worker instances
          $ref: '#/components/schemas/InstanceGroupConfig'
        autoscalingConfig:
          $ref: '#/components/schemas/AutoscalingConfig'
          description: >-
            Optional. Autoscaling config for the policy associated with the
            cluster. Cluster does not autoscale if this field is unset.
        masterConfig:
          $ref: '#/components/schemas/InstanceGroupConfig'
          description: >-
            Optional. The Compute Engine config settings for the cluster's
            master instance.
        encryptionConfig:
          description: Optional. Encryption settings for the cluster.
          $ref: '#/components/schemas/EncryptionConfig'
        tempBucket:
          description: >-
            Optional. A Cloud Storage bucket used to store ephemeral cluster and
            jobs data, such as Spark and MapReduce history files. If you do not
            specify a temp bucket, Dataproc will determine a Cloud Storage
            location (US, ASIA, or EU) for your cluster's temp bucket according
            to the Compute Engine zone where your cluster is deployed, and then
            create and manage this project-level, per-location bucket. The
            default bucket has a TTL of 90 days, but you can use any TTL (or
            none) if you specify a bucket (see Dataproc staging and temp buckets
            (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)).
            This field requires a Cloud Storage bucket name, not a gs://... URI
            to a Cloud Storage bucket.
          type: string
        diagnosticBucket:
          type: string
          description: >-
            Optional. A Cloud Storage bucket used to collect checkpoint
            diagnostic data
            (https://cloud.google.com/dataproc/docs/support/diagnose-clusters#checkpoint_diagnostic_data).
            If you do not specify a diagnostic bucket, Cloud Dataproc will use
            the Dataproc temp bucket to collect the checkpoint diagnostic data.
            This field requires a Cloud Storage bucket name, not a gs://... URI
            to a Cloud Storage bucket.
      type: object
      description: The cluster config.
      id: ClusterConfig
    SearchSessionSparkApplicationStageAttemptTasksResponse:
      properties:
        sparkApplicationStageAttemptTasks:
          type: array
          items:
            $ref: '#/components/schemas/TaskData'
          readOnly: true
          description: Output only. Data corresponding to tasks created by spark.
        nextPageToken:
          type: string
          description: >-
            This token is included in the response if there are more results to
            fetch. To fetch additional results, provide this value as the
            page_token in a subsequent
            SearchSessionSparkApplicationStageAttemptTasksRequest.
      id: SearchSessionSparkApplicationStageAttemptTasksResponse
      description: List of tasks for a stage of a Spark Application
      type: object
    UsageMetrics:
      type: object
      description: >-
        Usage metrics represent approximate total resources consumed by a
        workload.
      properties:
        shuffleStorageGbSeconds:
          format: int64
          description: >-
            Optional. Shuffle storage usage in (GB x seconds) (see Dataproc
            Serverless pricing
            (https://cloud.google.com/dataproc-serverless/pricing)).
          type: string
        milliDcuSeconds:
          format: int64
          description: >-
            Optional. DCU (Dataproc Compute Units) usage in (milliDCU x seconds)
            (see Dataproc Serverless pricing
            (https://cloud.google.com/dataproc-serverless/pricing)).
          type: string
        milliAcceleratorSeconds:
          type: string
          format: int64
          description: >-
            Optional. DEPRECATED Accelerator usage in (milliAccelerator x
            seconds) (see Dataproc Serverless pricing
            (https://cloud.google.com/dataproc-serverless/pricing)).
        acceleratorType:
          description: Optional. DEPRECATED Accelerator type being used, if any
          type: string
        updateTime:
          format: google-datetime
          description: Optional. The timestamp of the usage metrics.
          type: string
      id: UsageMetrics
    AccessSparkApplicationEnvironmentInfoResponse:
      id: AccessSparkApplicationEnvironmentInfoResponse
      type: object
      properties:
        applicationEnvironmentInfo:
          $ref: '#/components/schemas/ApplicationEnvironmentInfo'
          description: Details about the Environment that the application is running in.
      description: Environment details of a Saprk Application.
    JobData:
      properties:
        jobGroup:
          type: string
        numActiveStages:
          type: integer
          format: int32
        killTasksSummary:
          additionalProperties:
            type: integer
            format: int32
          type: object
        numCompletedIndices:
          type: integer
          format: int32
        numCompletedStages:
          format: int32
          type: integer
        numKilledTasks:
          type: integer
          format: int32
        numFailedTasks:
          type: integer
          format: int32
        submissionTime:
          format: google-datetime
          type: string
        sqlExecutionId:
          type: string
          format: int64
        numFailedStages:
          type: integer
          format: int32
        numActiveTasks:
          format: int32
          type: integer
        skippedStages:
          items:
            type: integer
            format: int32
          type: array
        completionTime:
          format: google-datetime
          type: string
        name:
          type: string
        numTasks:
          type: integer
          format: int32
        description:
          type: string
        numSkippedTasks:
          format: int32
          type: integer
        numSkippedStages:
          type: integer
          format: int32
        numCompletedTasks:
          type: integer
          format: int32
        jobId:
          type: string
          format: int64
        stageIds:
          items:
            format: int64
            type: string
          type: array
        status:
          enum:
            - JOB_EXECUTION_STATUS_UNSPECIFIED
            - JOB_EXECUTION_STATUS_RUNNING
            - JOB_EXECUTION_STATUS_SUCCEEDED
            - JOB_EXECUTION_STATUS_FAILED
            - JOB_EXECUTION_STATUS_UNKNOWN
          type: string
          enumDescriptions:
            - ''
            - ''
            - ''
            - ''
            - ''
      id: JobData
      description: Data corresponding to a spark job.
      type: object
    WorkflowMetadata:
      type: object
      id: WorkflowMetadata
      description: A Dataproc workflow template resource.
      properties:
        dagStartTime:
          description: >-
            Output only. DAG start time, only set for workflows with dag_timeout
            when DAG begins.
          readOnly: true
          type: string
          format: google-datetime
        version:
          type: integer
          readOnly: true
          format: int32
          description: >-
            Output only. The version of template at the time of workflow
            instantiation.
        createCluster:
          $ref: '#/components/schemas/ClusterOperation'
          description: Output only. The create cluster operation metadata.
          readOnly: true
        state:
          description: Output only. The workflow state.
          readOnly: true
          enum:
            - UNKNOWN
            - PENDING
            - RUNNING
            - DONE
          enumDescriptions:
            - Unused.
            - The operation has been created.
            - The operation is running.
            - The operation is done; either cancelled or completed.
          type: string
        template:
          type: string
          readOnly: true
          description: >-
            Output only. The resource name of the workflow template as described
            in https://cloud.google.com/apis/design/resource_names. For
            projects.regions.workflowTemplates, the resource name of the
            template has the following format:
            projects/{project_id}/regions/{region}/workflowTemplates/{template_id}
            For projects.locations.workflowTemplates, the resource name of the
            template has the following format:
            projects/{project_id}/locations/{location}/workflowTemplates/{template_id}
        dagTimeout:
          type: string
          readOnly: true
          description: >-
            Output only. The timeout duration for the DAG of jobs, expressed in
            seconds (see JSON representation of duration
            (https://developers.google.com/protocol-buffers/docs/proto3#json)).
          format: google-duration
        endTime:
          type: string
          description: Output only. Workflow end time.
          readOnly: true
          format: google-datetime
        clusterUuid:
          type: string
          description: Output only. The UUID of target cluster.
          readOnly: true
        parameters:
          description: >-
            Map from parameter names to values that were used for those
            parameters.
          type: object
          additionalProperties:
            type: string
        dagEndTime:
          format: google-datetime
          type: string
          description: >-
            Output only. DAG end time, only set for workflows with dag_timeout
            when DAG ends.
          readOnly: true
        clusterName:
          type: string
          description: Output only. The name of the target cluster.
          readOnly: true
        startTime:
          description: Output only. Workflow start time.
          type: string
          readOnly: true
          format: google-datetime
        deleteCluster:
          $ref: '#/components/schemas/ClusterOperation'
          description: Output only. The delete cluster operation metadata.
          readOnly: true
        graph:
          $ref: '#/components/schemas/WorkflowGraph'
          description: Output only. The workflow graph.
          readOnly: true
    StateOperatorProgress:
      type: object
      properties:
        memoryUsedBytes:
          type: string
          format: int64
        numRowsUpdated:
          format: int64
          type: string
        numShufflePartitions:
          type: string
          format: int64
        operatorName:
          type: string
        customMetrics:
          type: object
          additionalProperties:
            type: string
            format: int64
        numRowsDroppedByWatermark:
          type: string
          format: int64
        allRemovalsTimeMs:
          format: int64
          type: string
        numStateStoreInstances:
          format: int64
          type: string
        commitTimeMs:
          type: string
          format: int64
        numRowsTotal:
          format: int64
          type: string
        numRowsRemoved:
          format: int64
          type: string
        allUpdatesTimeMs:
          type: string
          format: int64
      id: StateOperatorProgress
    ExecutorPeakMetricsDistributions:
      type: object
      id: ExecutorPeakMetricsDistributions
      properties:
        quantiles:
          type: array
          items:
            type: number
            format: double
        executorMetrics:
          type: array
          items:
            $ref: '#/components/schemas/ExecutorMetrics'
    ClusterMetrics:
      id: ClusterMetrics
      properties:
        yarnMetrics:
          description: YARN metrics.
          type: object
          additionalProperties:
            type: string
            format: int64
        hdfsMetrics:
          additionalProperties:
            type: string
            format: int64
          description: The HDFS metrics.
          type: object
      type: object
      description: >-
        Contains cluster daemon metrics, such as HDFS and YARN stats.Beta
        Feature: This report is available for testing purposes only. It may be
        changed before final release.
    SummarizeSessionSparkApplicationJobsResponse:
      type: object
      properties:
        jobsSummary:
          description: Summary of a Spark Application Jobs
          $ref: '#/components/schemas/JobsSummary'
      description: Summary of a Spark Application jobs.
      id: SummarizeSessionSparkApplicationJobsResponse
    ShuffleWriteMetrics:
      description: Shuffle data written by task.
      properties:
        recordsWritten:
          format: int64
          type: string
        bytesWritten:
          format: int64
          type: string
        writeTimeNanos:
          type: string
          format: int64
      id: ShuffleWriteMetrics
      type: object
    Batch:
      type: object
      description: A representation of a batch workload in the service.
      id: Batch
      properties:
        creator:
          type: string
          description: Output only. The email address of the user who created the batch.
          readOnly: true
        stateTime:
          type: string
          readOnly: true
          description: Output only. The time when the batch entered a current state.
          format: google-datetime
        labels:
          type: object
          additionalProperties:
            type: string
          description: >-
            Optional. The labels to associate with this batch. Label keys must
            contain 1 to 63 characters, and must conform to RFC 1035
            (https://www.ietf.org/rfc/rfc1035.txt). Label values may be empty,
            but, if present, must contain 1 to 63 characters, and must conform
            to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). No more than 32
            labels can be associated with a batch.
        runtimeInfo:
          $ref: '#/components/schemas/RuntimeInfo'
          readOnly: true
          description: Output only. Runtime information about batch execution.
        name:
          type: string
          readOnly: true
          description: Output only. The resource name of the batch.
        stateMessage:
          type: string
          readOnly: true
          description: >-
            Output only. Batch state details, such as a failure description if
            the state is FAILED.
        sparkSqlBatch:
          $ref: '#/components/schemas/SparkSqlBatch'
          description: Optional. SparkSql batch config.
        state:
          readOnly: true
          enum:
            - STATE_UNSPECIFIED
            - PENDING
            - RUNNING
            - CANCELLING
            - CANCELLED
            - SUCCEEDED
            - FAILED
          type: string
          description: Output only. The state of the batch.
          enumDescriptions:
            - The batch state is unknown.
            - The batch is created before running.
            - The batch is running.
            - The batch is cancelling.
            - The batch cancellation was successful.
            - The batch completed successfully.
            - The batch is no longer running due to an error.
        uuid:
          type: string
          readOnly: true
          description: >-
            Output only. A batch UUID (Unique Universal Identifier). The service
            generates this value when it creates the batch.
        operation:
          description: >-
            Output only. The resource name of the operation associated with this
            batch.
          readOnly: true
          type: string
        stateHistory:
          readOnly: true
          description: Output only. Historical state information for the batch.
          type: array
          items:
            $ref: '#/components/schemas/StateHistory'
        createTime:
          description: Output only. The time when the batch was created.
          type: string
          readOnly: true
          format: google-datetime
        pysparkBatch:
          description: Optional. PySpark batch config.
          $ref: '#/components/schemas/PySparkBatch'
        sparkRBatch:
          description: Optional. SparkR batch config.
          $ref: '#/components/schemas/SparkRBatch'
        sparkBatch:
          description: Optional. Spark batch config.
          $ref: '#/components/schemas/SparkBatch'
        runtimeConfig:
          description: Optional. Runtime configuration for the batch execution.
          $ref: '#/components/schemas/RuntimeConfig'
        environmentConfig:
          $ref: '#/components/schemas/EnvironmentConfig'
          description: Optional. Environment configuration for the batch execution.
    QueryList:
      description: A list of queries to run on a cluster.
      type: object
      id: QueryList
      properties:
        queries:
          description: >-
            Required. The queries to execute. You do not need to end a query
            expression with a semicolon. Multiple queries can be specified in
            one string by separating each with a semicolon. Here is an example
            of a Dataproc API snippet that uses a QueryList to specify a
            HiveJob: "hiveJob": { "queryList": { "queries": [ "query1",
            "query2", "query3;query4", ] } } 
          items:
            type: string
          type: array
    TestIamPermissionsResponse:
      properties:
        permissions:
          items:
            type: string
          type: array
          description: >-
            A subset of TestPermissionsRequest.permissions that the caller is
            allowed.
      type: object
      description: Response message for TestIamPermissions method.
      id: TestIamPermissionsResponse
    ExecutorResourceRequest:
      properties:
        resourceName:
          type: string
        discoveryScript:
          type: string
        vendor:
          type: string
        amount:
          type: string
          format: int64
      type: object
      description: Resources used per executor used by the application.
      id: ExecutorResourceRequest
    SearchSessionSparkApplicationExecutorStageSummaryResponse:
      id: SearchSessionSparkApplicationExecutorStageSummaryResponse
      properties:
        sparkApplicationStageExecutors:
          items:
            $ref: '#/components/schemas/ExecutorStageSummary'
          description: Details about executors used by the application stage.
          type: array
        nextPageToken:
          type: string
          description: >-
            This token is included in the response if there are more results to
            fetch. To fetch additional results, provide this value as the
            page_token in a subsequent
            SearchSessionSparkApplicationExecutorStageSummaryRequest.
      type: object
      description: List of Executors associated with a Spark Application Stage.
    DiagnoseClusterResults:
      type: object
      description: The location of diagnostic output.
      properties:
        outputUri:
          type: string
          description: >-
            Output only. The Cloud Storage URI of the diagnostic output. The
            output report is a plain text file with a summary of collected
            diagnostics.
          readOnly: true
      id: DiagnoseClusterResults
    AutotuningConfig:
      description: Autotuning configuration of the workload.
      properties:
        scenarios:
          items:
            enum:
              - SCENARIO_UNSPECIFIED
              - SCALING
              - BROADCAST_HASH_JOIN
              - MEMORY
              - NONE
              - AUTO
            enumDescriptions:
              - Default value.
              - Scaling recommendations such as initialExecutors.
              - Adding hints for potential relation broadcasts.
              - Memory management for workloads.
              - No autotuning.
              - Automatic selection of scenarios.
            type: string
          description: Optional. Scenarios for which tunings are applied.
          type: array
      type: object
      id: AutotuningConfig
    ClusterToRepair:
      type: object
      description: Cluster to be repaired
      id: ClusterToRepair
      properties:
        clusterRepairAction:
          enumDescriptions:
            - No action will be taken by default.
            - Repair cluster in ERROR_DUE_TO_UPDATE states.
          enum:
            - CLUSTER_REPAIR_ACTION_UNSPECIFIED
            - REPAIR_ERROR_DUE_TO_UPDATE_CLUSTER
          description: Required. Repair action to take on the cluster resource.
          type: string
    InstanceFlexibilityPolicy:
      id: InstanceFlexibilityPolicy
      description: >-
        Instance flexibility Policy allowing a mixture of VM shapes and
        provisioning models.
      properties:
        provisioningModelMix:
          description: >-
            Optional. Defines how the Group selects the provisioning model to
            ensure required reliability.
          $ref: '#/components/schemas/ProvisioningModelMix'
        instanceSelectionList:
          type: array
          items:
            $ref: '#/components/schemas/InstanceSelection'
          description: >-
            Optional. List of instance selection options that the group will use
            when creating new VMs.
        instanceSelectionResults:
          readOnly: true
          type: array
          description: Output only. A list of instance selection results in the group.
          items:
            $ref: '#/components/schemas/InstanceSelectionResult'
      type: object
    SparkJob:
      description: >-
        A Dataproc job for running Apache Spark (https://spark.apache.org/)
        applications on YARN.
      type: object
      properties:
        fileUris:
          description: >-
            Optional. HCFS URIs of files to be placed in the working directory
            of each executor. Useful for naively parallel tasks.
          type: array
          items:
            type: string
        archiveUris:
          type: array
          description: >-
            Optional. HCFS URIs of archives to be extracted into the working
            directory of each executor. Supported file types: .jar, .tar,
            .tar.gz, .tgz, and .zip.
          items:
            type: string
        jarFileUris:
          description: >-
            Optional. HCFS URIs of jar files to add to the CLASSPATHs of the
            Spark driver and tasks.
          type: array
          items:
            type: string
        properties:
          type: object
          additionalProperties:
            type: string
          description: >-
            Optional. A mapping of property names to values, used to configure
            Spark. Properties that conflict with values set by the Dataproc API
            might be overwritten. Can include properties set in
            /etc/spark/conf/spark-defaults.conf and classes in user code.
        mainJarFileUri:
          type: string
          description: The HCFS URI of the jar file that contains the main class.
        mainClass:
          description: >-
            The name of the driver's main class. The jar file that contains the
            class must be in the default CLASSPATH or specified in
            SparkJob.jar_file_uris.
          type: string
        args:
          type: array
          items:
            type: string
          description: >-
            Optional. The arguments to pass to the driver. Do not include
            arguments, such as --conf, that can be set as job properties, since
            a collision may occur that causes an incorrect job submission.
        loggingConfig:
          $ref: '#/components/schemas/LoggingConfig'
          description: Optional. The runtime log config for job execution.
      id: SparkJob
    AuxiliaryServicesConfig:
      type: object
      description: Auxiliary services configuration for a Cluster.
      properties:
        sparkHistoryServerConfig:
          $ref: '#/components/schemas/SparkHistoryServerConfig'
          description: Optional. The Spark History Server configuration for the workload.
        metastoreConfig:
          description: Optional. The Hive Metastore configuration for this workload.
          $ref: '#/components/schemas/MetastoreConfig'
      id: AuxiliaryServicesConfig
    GkeNodePoolAutoscalingConfig:
      properties:
        minNodeCount:
          description: >-
            The minimum number of nodes in the node pool. Must be >= 0 and <=
            max_node_count.
          format: int32
          type: integer
        maxNodeCount:
          type: integer
          format: int32
          description: >-
            The maximum number of nodes in the node pool. Must be >=
            min_node_count, and must be > 0. Note: Quota must be sufficient to
            scale up the cluster.
      id: GkeNodePoolAutoscalingConfig
      type: object
      description: >-
        GkeNodePoolAutoscaling contains information the cluster autoscaler needs
        to adjust the size of the node pool to the current cluster usage.
    AccessSparkApplicationResponse:
      description: A summary of Spark Application
      type: object
      id: AccessSparkApplicationResponse
      properties:
        application:
          $ref: '#/components/schemas/ApplicationInfo'
          readOnly: true
          description: Output only. High level information corresponding to an application.
    AccessSparkApplicationStageRddOperationGraphResponse:
      properties:
        rddOperationGraph:
          description: RDD operation graph for a Spark Application Stage.
          $ref: '#/components/schemas/RddOperationGraph'
      id: AccessSparkApplicationStageRddOperationGraphResponse
      type: object
      description: >-
        RDD operation graph for a Spark Application Stage limited to maximum
        10000 clusters.
    GkeNodePoolTarget:
      properties:
        roles:
          type: array
          description: Required. The roles associated with the GKE node pool.
          items:
            enumDescriptions:
              - Role is unspecified.
              - >-
                At least one node pool must have the DEFAULT role. Work assigned
                to a role that is not associated with a node pool is assigned to
                the node pool with the DEFAULT role. For example, work assigned
                to the CONTROLLER role will be assigned to the node pool with
                the DEFAULT role if no node pool has the CONTROLLER role.
              - >-
                Run work associated with the Dataproc control plane (for
                example, controllers and webhooks). Very low resource
                requirements.
              - Run work associated with a Spark driver of a job.
              - Run work associated with a Spark executor of a job.
            type: string
            enum:
              - ROLE_UNSPECIFIED
              - DEFAULT
              - CONTROLLER
              - SPARK_DRIVER
              - SPARK_EXECUTOR
        nodePoolConfig:
          description: >-
            Input only. The configuration for the GKE node pool.If specified,
            Dataproc attempts to create a node pool with the specified shape. If
            one with the same name already exists, it is verified against all
            specified fields. If a field differs, the virtual cluster creation
            will fail.If omitted, any node pool with the specified name is used.
            If a node pool with the specified name does not exist, Dataproc
            create a node pool with default values.This is an input only field.
            It will not be returned by the API.
          $ref: '#/components/schemas/GkeNodePoolConfig'
        nodePool:
          type: string
          description: >-
            Required. The target GKE node pool. Format:
            'projects/{project}/locations/{location}/clusters/{cluster}/nodePools/{node_pool}'
      id: GkeNodePoolTarget
      type: object
      description: GKE node pools that Dataproc workloads run on.
    RddOperationNode:
      id: RddOperationNode
      properties:
        cached:
          type: boolean
        nodeId:
          type: integer
          format: int32
        barrier:
          type: boolean
        name:
          type: string
        outputDeterministicLevel:
          enum:
            - DETERMINISTIC_LEVEL_UNSPECIFIED
            - DETERMINISTIC_LEVEL_DETERMINATE
            - DETERMINISTIC_LEVEL_UNORDERED
            - DETERMINISTIC_LEVEL_INDETERMINATE
          enumDescriptions:
            - ''
            - ''
            - ''
            - ''
          type: string
        callsite:
          type: string
      type: object
      description: A node in the RDD operation graph. Corresponds to a single RDD.
    PoolData:
      properties:
        stageIds:
          items:
            type: string
            format: int64
          type: array
        name:
          type: string
      id: PoolData
      description: Pool Data
      type: object
    SpeculationStageSummary:
      properties:
        stageAttemptId:
          type: integer
          format: int32
        stageId:
          format: int64
          type: string
        numActiveTasks:
          type: integer
          format: int32
        numKilledTasks:
          type: integer
          format: int32
        numTasks:
          type: integer
          format: int32
        numCompletedTasks:
          type: integer
          format: int32
        numFailedTasks:
          format: int32
          type: integer
      description: Details of the speculation task when speculative execution is enabled.
      type: object
      id: SpeculationStageSummary
    JobReference:
      id: JobReference
      type: object
      description: Encapsulates the full scoping used to reference a job.
      properties:
        projectId:
          description: >-
            Optional. The ID of the Google Cloud Platform project that the job
            belongs to. If specified, must match the request project ID.
          type: string
        jobId:
          type: string
          description: >-
            Optional. The job ID, which must be unique within the project.The ID
            must contain only letters (a-z, A-Z), numbers (0-9), underscores
            (_), or hyphens (-). The maximum length is 100 characters.If not
            specified by the caller, the job ID will be provided by the server.
    GetIamPolicyRequest:
      description: Request message for GetIamPolicy method.
      type: object
      properties:
        options:
          $ref: '#/components/schemas/GetPolicyOptions'
          description: >-
            OPTIONAL: A GetPolicyOptions object for specifying options to
            GetIamPolicy.
      id: GetIamPolicyRequest
    SqlPlanMetric:
      description: Metrics related to SQL execution.
      properties:
        accumulatorId:
          format: int64
          type: string
        metricType:
          type: string
        name:
          type: string
      id: SqlPlanMetric
      type: object
    SinkProgress:
      id: SinkProgress
      properties:
        numOutputRows:
          type: string
          format: int64
        description:
          type: string
        metrics:
          type: object
          additionalProperties:
            type: string
      type: object
    YarnApplication:
      description: >-
        A YARN application created by a job. Application information is a subset
        of org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Beta
        Feature: This report is available for testing purposes only. It may be
        changed before final release.
      properties:
        memoryMbSeconds:
          description: >-
            Optional. The cumulative memory usage of the application for a job,
            measured in mb-seconds.
          format: int64
          type: string
        state:
          enumDescriptions:
            - Status is unspecified.
            - Status is NEW.
            - Status is NEW_SAVING.
            - Status is SUBMITTED.
            - Status is ACCEPTED.
            - Status is RUNNING.
            - Status is FINISHED.
            - Status is FAILED.
            - Status is KILLED.
          enum:
            - STATE_UNSPECIFIED
            - NEW
            - NEW_SAVING
            - SUBMITTED
            - ACCEPTED
            - RUNNING
            - FINISHED
            - FAILED
            - KILLED
          description: Required. The application state.
          type: string
        vcoreSeconds:
          format: int64
          description: >-
            Optional. The cumulative CPU time consumed by the application for a
            job, measured in vcore-seconds.
          type: string
        progress:
          description: Required. The numerical progress of the application, from 1 to 100.
          format: float
          type: number
        trackingUrl:
          type: string
          description: >-
            Optional. The HTTP URL of the ApplicationMaster, HistoryServer, or
            TimelineServer that provides application-specific information. The
            URL uses the internal hostname, and requires a proxy server for
            resolution and, possibly, access.
        name:
          description: Required. The application name.
          type: string
      type: object
      id: YarnApplication
    GceClusterConfig:
      properties:
        nodeGroupAffinity:
          description: Optional. Node Group Affinity for sole-tenant clusters.
          $ref: '#/components/schemas/NodeGroupAffinity'
        privateIpv6GoogleAccess:
          enumDescriptions:
            - >-
              If unspecified, Compute Engine default behavior will apply, which
              is the same as INHERIT_FROM_SUBNETWORK.
            - >-
              Private access to and from Google Services configuration inherited
              from the subnetwork configuration. This is the default Compute
              Engine behavior.
            - >-
              Enables outbound private IPv6 access to Google Services from the
              Dataproc cluster.
            - >-
              Enables bidirectional private IPv6 access between Google Services
              and the Dataproc cluster.
          enum:
            - PRIVATE_IPV6_GOOGLE_ACCESS_UNSPECIFIED
            - INHERIT_FROM_SUBNETWORK
            - OUTBOUND
            - BIDIRECTIONAL
          type: string
          description: Optional. The type of IPv6 access for a cluster.
        networkUri:
          description: >-
            Optional. The Compute Engine network to be used for machine
            communications. Cannot be specified with subnetwork_uri. If neither
            network_uri nor subnetwork_uri is specified, the "default" network
            of the project is used, if it exists. Cannot be a "Custom Subnet
            Network" (see Using Subnetworks
            (https://cloud.google.com/compute/docs/subnetworks) for more
            information).A full URL, partial URI, or short name are valid.
            Examples:
            https://www.googleapis.com/compute/v1/projects/[project_id]/global/networks/default
            projects/[project_id]/global/networks/default default
          type: string
        reservationAffinity:
          $ref: '#/components/schemas/ReservationAffinity'
          description: Optional. Reservation Affinity for consuming Zonal reservation.
        resourceManagerTags:
          type: object
          description: >-
            Optional. Resource manager tags
            (https://cloud.google.com/resource-manager/docs/tags/tags-creating-and-managing)
            to add to all instances (see Use secure tags in Dataproc
            (https://cloud.google.com/dataproc/docs/guides/use-secure-tags)).
          additionalProperties:
            type: string
        serviceAccount:
          type: string
          description: >-
            Optional. The Dataproc service account
            (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#service_accounts_in_dataproc)
            (also see VM Data Plane identity
            (https://cloud.google.com/dataproc/docs/concepts/iam/dataproc-principals#vm_service_account_data_plane_identity))
            used by Dataproc cluster VM instances to access Google Cloud
            Platform services.If not specified, the Compute Engine default
            service account
            (https://cloud.google.com/compute/docs/access/service-accounts#default_service_account)
            is used.
        serviceAccountScopes:
          type: array
          description: >-
            Optional. The URIs of service account scopes to be included in
            Compute Engine instances. The following base set of scopes is always
            included:
            https://www.googleapis.com/auth/cloud.useraccounts.readonly
            https://www.googleapis.com/auth/devstorage.read_write
            https://www.googleapis.com/auth/logging.writeIf no scopes are
            specified, the following defaults are also provided:
            https://www.googleapis.com/auth/bigquery
            https://www.googleapis.com/auth/bigtable.admin.table
            https://www.googleapis.com/auth/bigtable.data
            https://www.googleapis.com/auth/devstorage.full_control
          items:
            type: string
        internalIpOnly:
          description: >-
            Optional. This setting applies to subnetwork-enabled networks. It is
            set to true by default in clusters created with image versions
            2.2.x.When set to true: All cluster VMs have internal IP addresses.
            Google Private Access
            (https://cloud.google.com/vpc/docs/private-google-access) must be
            enabled to access Dataproc and other Google Cloud APIs. Off-cluster
            dependencies must be configured to be accessible without external IP
            addresses.When set to false: Cluster VMs are not restricted to
            internal IP addresses. Ephemeral external IP addresses are assigned
            to each cluster VM.
          type: boolean
        shieldedInstanceConfig:
          description: >-
            Optional. Shielded Instance Config for clusters using Compute Engine
            Shielded VMs
            (https://cloud.google.com/security/shielded-cloud/shielded-vm).
          $ref: '#/components/schemas/ShieldedInstanceConfig'
        confidentialInstanceConfig:
          $ref: '#/components/schemas/ConfidentialInstanceConfig'
          description: >-
            Optional. Confidential Instance Config for clusters using
            Confidential VMs
            (https://cloud.google.com/compute/confidential-vm/docs).
        tags:
          items:
            type: string
          description: >-
            The Compute Engine network tags to add to all instances (see Tagging
            instances
            (https://cloud.google.com/vpc/docs/add-remove-network-tags)).
          type: array
        metadata:
          additionalProperties:
            type: string
          type: object
          description: >-
            Optional. The Compute Engine metadata entries to add to all
            instances (see Project and instance metadata
            (https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
        subnetworkUri:
          description: >-
            Optional. The Compute Engine subnetwork to be used for machine
            communications. Cannot be specified with network_uri.A full URL,
            partial URI, or short name are valid. Examples:
            https://www.googleapis.com/compute/v1/projects/[project_id]/regions/[region]/subnetworks/sub0
            projects/[project_id]/regions/[region]/subnetworks/sub0 sub0
          type: string
        zoneUri:
          description: >-
            Optional. The Compute Engine zone where the Dataproc cluster will be
            located. If omitted, the service will pick a zone in the cluster's
            Compute Engine region. On a get request, zone will always be
            present.A full URL, partial URI, or short name are valid. Examples:
            https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]
            projects/[project_id]/zones/[zone] [zone]
          type: string
      type: object
      description: >-
        Common config settings for resources of Compute Engine cluster
        instances, applicable to all instances in the cluster.
      id: GceClusterConfig
    AnalyzeOperationMetadata:
      type: object
      description: Metadata describing the Analyze operation.
      properties:
        analyzedWorkloadName:
          description: Output only. name of the workload being analyzed.
          type: string
          readOnly: true
        createTime:
          type: string
          format: google-datetime
          description: Output only. The time when the operation was created.
          readOnly: true
        labels:
          description: Output only. Labels associated with the operation.
          type: object
          additionalProperties:
            type: string
          readOnly: true
        description:
          readOnly: true
          description: Output only. Short description of the operation.
          type: string
        analyzedWorkloadType:
          enum:
            - WORKLOAD_TYPE_UNSPECIFIED
            - BATCH
          type: string
          description: Output only. Type of the workload being analyzed.
          enumDescriptions:
            - Undefined option
            - Serverless batch job
          readOnly: true
        doneTime:
          type: string
          readOnly: true
          description: Output only. The time when the operation finished.
          format: google-datetime
        analyzedWorkloadUuid:
          type: string
          readOnly: true
          description: >-
            Output only. unique identifier of the workload typically generated
            by control plane. E.g. batch uuid.
        warnings:
          type: array
          items:
            type: string
          description: Output only. Warnings encountered during operation execution.
          readOnly: true
      id: AnalyzeOperationMetadata
    ListAutoscalingPoliciesResponse:
      description: A response to a request to list autoscaling policies in a project.
      properties:
        policies:
          type: array
          description: Output only. Autoscaling policies list.
          readOnly: true
          items:
            $ref: '#/components/schemas/AutoscalingPolicy'
        nextPageToken:
          readOnly: true
          description: >-
            Output only. This token is included in the response if there are
            more results to fetch.
          type: string
      type: object
      id: ListAutoscalingPoliciesResponse
    EndpointConfig:
      id: EndpointConfig
      type: object
      description: Endpoint config for this cluster
      properties:
        enableHttpPortAccess:
          description: >-
            Optional. If true, enable http access to specific ports on the
            cluster from external sources. Defaults to false.
          type: boolean
        httpPorts:
          readOnly: true
          type: object
          additionalProperties:
            type: string
          description: >-
            Output only. The map of port descriptions to URLs. Will only be
            populated if enable_http_port_access is true.
    SparkSqlJob:
      properties:
        properties:
          description: >-
            Optional. A mapping of property names to values, used to configure
            Spark SQL's SparkConf. Properties that conflict with values set by
            the Dataproc API might be overwritten.
          type: object
          additionalProperties:
            type: string
        loggingConfig:
          $ref: '#/components/schemas/LoggingConfig'
          description: Optional. The runtime log config for job execution.
        scriptVariables:
          additionalProperties:
            type: string
          type: object
          description: >-
            Optional. Mapping of query variable names to values (equivalent to
            the Spark SQL command: SET name="value";).
        queryFileUri:
          type: string
          description: The HCFS URI of the script that contains SQL queries.
        jarFileUris:
          description: Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
          type: array
          items:
            type: string
        queryList:
          description: A list of queries.
          $ref: '#/components/schemas/QueryList'
      description: >-
        A Dataproc job for running Apache Spark SQL
        (https://spark.apache.org/sql/) queries.
      id: SparkSqlJob
      type: object
    PySparkJob:
      description: >-
        A Dataproc job for running Apache PySpark
        (https://spark.apache.org/docs/latest/api/python/index.html#pyspark-overview)
        applications on YARN.
      properties:
        pythonFileUris:
          type: array
          items:
            type: string
          description: >-
            Optional. HCFS file URIs of Python files to pass to the PySpark
            framework. Supported file types: .py, .egg, and .zip.
        jarFileUris:
          type: array
          description: >-
            Optional. HCFS URIs of jar files to add to the CLASSPATHs of the
            Python driver and tasks.
          items:
            type: string
        mainPythonFileUri:
          type: string
          description: >-
            Required. The HCFS URI of the main Python file to use as the driver.
            Must be a .py file.
        args:
          items:
            type: string
          type: array
          description: >-
            Optional. The arguments to pass to the driver. Do not include
            arguments, such as --conf, that can be set as job properties, since
            a collision may occur that causes an incorrect job submission.
        fileUris:
          description: >-
            Optional. HCFS URIs of files to be placed in the working directory
            of each executor. Useful for naively parallel tasks.
          items:
            type: string
          type: array
        archiveUris:
          description: >-
            Optional. HCFS URIs of archives to be extracted into the working
            directory of each executor. Supported file types: .jar, .tar,
            .tar.gz, .tgz, and .zip.Note: Spark applications must be deployed in
            cluster mode
            (https://spark.apache.org/docs/latest/cluster-overview.html) for
            correct environment propagation.
          type: array
          items:
            type: string
        loggingConfig:
          description: Optional. The runtime log config for job execution.
          $ref: '#/components/schemas/LoggingConfig'
        properties:
          description: >-
            Optional. A mapping of property names to values, used to configure
            PySpark. Properties that conflict with values set by the Dataproc
            API might be overwritten. Can include properties set in
            /etc/spark/conf/spark-defaults.conf and classes in user code.
          additionalProperties:
            type: string
          type: object
      type: object
      id: PySparkJob
    ExecutorStageSummary:
      type: object
      description: Executor resources consumed by a stage.
      properties:
        peakMemoryMetrics:
          $ref: '#/components/schemas/ExecutorMetrics'
        failedTasks:
          type: integer
          format: int32
        inputRecords:
          format: int64
          type: string
        shuffleReadRecords:
          format: int64
          type: string
        outputRecords:
          type: string
          format: int64
        shuffleWriteRecords:
          type: string
          format: int64
        stageAttemptId:
          format: int32
          type: integer
        executorId:
          type: string
        shuffleRead:
          type: string
          format: int64
        taskTimeMillis:
          type: string
          format: int64
        outputBytes:
          type: string
          format: int64
        memoryBytesSpilled:
          format: int64
          type: string
        isExcludedForStage:
          type: boolean
        stageId:
          type: string
          format: int64
        succeededTasks:
          type: integer
          format: int32
        shuffleWrite:
          type: string
          format: int64
        diskBytesSpilled:
          type: string
          format: int64
        inputBytes:
          type: string
          format: int64
        killedTasks:
          type: integer
          format: int32
      id: ExecutorStageSummary
    SearchSparkApplicationsResponse:
      type: object
      properties:
        sparkApplications:
          readOnly: true
          description: Output only. High level information corresponding to an application.
          items:
            $ref: '#/components/schemas/SparkApplication'
          type: array
        nextPageToken:
          description: >-
            This token is included in the response if there are more results to
            fetch. To fetch additional results, provide this value as the
            page_token in a subsequent SearchSparkApplicationsRequest.
          type: string
      description: A list of summary of Spark Applications
      id: SearchSparkApplicationsResponse
    ExecutionConfig:
      id: ExecutionConfig
      type: object
      properties:
        authenticationConfig:
          description: >-
            Optional. Authentication configuration used to set the default
            identity for the workload execution. The config specifies the type
            of identity (service account or user) that will be used by workloads
            to access resources on the project(s).
          $ref: '#/components/schemas/AuthenticationConfig'
        ttl:
          type: string
          format: google-duration
          description: >-
            Optional. The duration after which the workload will be terminated,
            specified as the JSON representation for Duration
            (https://protobuf.dev/programming-guides/proto3/#json). When the
            workload exceeds this duration, it will be unconditionally
            terminated without waiting for ongoing work to finish. If ttl is not
            specified for a batch workload, the workload will be allowed to run
            until it exits naturally (or run forever without exiting). If ttl is
            not specified for an interactive session, it defaults to 24 hours.
            If ttl is not specified for a batch that uses 2.1+ runtime version,
            it defaults to 4 hours. Minimum value is 10 minutes; maximum value
            is 14 days. If both ttl and idle_ttl are specified (for an
            interactive session), the conditions are treated as OR conditions:
            the workload will be terminated when it has been idle for idle_ttl
            or when ttl has been exceeded, whichever occurs first.
        idleTtl:
          description: >-
            Optional. Applies to sessions only. The duration to keep the session
            alive while it's idling. Exceeding this threshold causes the session
            to terminate. This field cannot be set on a batch workload. Minimum
            value is 10 minutes; maximum value is 14 days (see JSON
            representation of Duration
            (https://developers.google.com/protocol-buffers/docs/proto3#json)).
            Defaults to 1 hour if not set. If both ttl and idle_ttl are
            specified for an interactive session, the conditions are treated as
            OR conditions: the workload will be terminated when it has been idle
            for idle_ttl or when ttl has been exceeded, whichever occurs first.
          type: string
          format: google-duration
        stagingBucket:
          type: string
          description: >-
            Optional. A Cloud Storage bucket used to stage workload
            dependencies, config files, and store workload output and other
            ephemeral data, such as Spark history files. If you do not specify a
            staging bucket, Cloud Dataproc will determine a Cloud Storage
            location according to the region where your workload is running, and
            then create and manage project-level, per-location staging and
            temporary buckets. This field requires a Cloud Storage bucket name,
            not a gs://... URI to a Cloud Storage bucket.
        serviceAccount:
          type: string
          description: Optional. Service account that used to execute workload.
        networkTags:
          items:
            type: string
          description: Optional. Tags used for network traffic control.
          type: array
        subnetworkUri:
          type: string
          description: Optional. Subnetwork URI to connect workload to.
        kmsKey:
          description: Optional. The Cloud KMS key to use for encryption.
          type: string
        networkUri:
          description: Optional. Network URI to connect workload to.
          type: string
      description: Execution configuration for a workload.
    ListOperationsResponse:
      description: The response message for Operations.ListOperations.
      type: object
      properties:
        nextPageToken:
          description: The standard List next-page token.
          type: string
        unreachable:
          items:
            type: string
          type: array
          description: >-
            Unordered list. Unreachable resources. Populated when the request
            sets ListOperationsRequest.return_partial_success and reads across
            collections e.g. when attempting to list all resources across all
            supported locations.
        operations:
          items:
            $ref: '#/components/schemas/Operation'
          description: >-
            A list of operations that matches the specified filter in the
            request.
          type: array
      id: ListOperationsResponse
    GkeNodePoolAcceleratorConfig:
      description: >-
        A GkeNodeConfigAcceleratorConfig represents a Hardware Accelerator
        request for a node pool.
      properties:
        acceleratorCount:
          format: int64
          description: The number of accelerator cards exposed to an instance.
          type: string
        gpuPartitionSize:
          description: >-
            Size of partitions to create on the GPU. Valid values are described
            in the NVIDIA mig user guide
            (https://docs.nvidia.com/datacenter/tesla/mig-user-guide/#partitioning).
          type: string
        acceleratorType:
          type: string
          description: The accelerator type resource namename (see GPUs on Compute Engine).
      id: GkeNodePoolAcceleratorConfig
      type: object
    SecurityConfig:
      type: object
      description: Security related configuration, including encryption, Kerberos, etc.
      properties:
        identityConfig:
          description: >-
            Optional. Identity related configuration, including service account
            based secure multi-tenancy user mappings.
          $ref: '#/components/schemas/IdentityConfig'
        kerberosConfig:
          description: Optional. Kerberos related configuration.
          $ref: '#/components/schemas/KerberosConfig'
      id: SecurityConfig
    DataprocMetricConfig:
      description: Dataproc metric config.
      id: DataprocMetricConfig
      type: object
      properties:
        metrics:
          items:
            $ref: '#/components/schemas/Metric'
          description: Required. Metrics sources to enable.
          type: array
    SearchSparkApplicationSqlQueriesResponse:
      properties:
        sparkApplicationSqlQueries:
          readOnly: true
          type: array
          description: Output only. SQL Execution Data
          items:
            $ref: '#/components/schemas/SqlExecutionUiData'
        nextPageToken:
          type: string
          description: >-
            This token is included in the response if there are more results to
            fetch. To fetch additional results, provide this value as the
            page_token in a subsequent SearchSparkApplicationSqlQueriesRequest.
      type: object
      description: List of all queries for a Spark Application.
      id: SearchSparkApplicationSqlQueriesResponse
    PigJob:
      properties:
        properties:
          type: object
          additionalProperties:
            type: string
          description: >-
            Optional. A mapping of property names to values, used to configure
            Pig. Properties that conflict with values set by the Dataproc API
            might be overwritten. Can include properties set in
            /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and
            classes in user code.
        jarFileUris:
          type: array
          items:
            type: string
          description: >-
            Optional. HCFS URIs of jar files to add to the CLASSPATH of the Pig
            Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
        continueOnFailure:
          type: boolean
          description: >-
            Optional. Whether to continue executing queries if a query fails.
            The default value is false. Setting to true can be useful when
            executing independent parallel queries.
        queryList:
          description: A list of queries.
          $ref: '#/components/schemas/QueryList'
        loggingConfig:
          description: Optional. The runtime log config for job execution.
          $ref: '#/components/schemas/LoggingConfig'
        queryFileUri:
          description: The HCFS URI of the script that contains the Pig queries.
          type: string
        scriptVariables:
          type: object
          description: >-
            Optional. Mapping of query variable names to values (equivalent to
            the Pig command: name=[value]).
          additionalProperties:
            type: string
      description: >-
        A Dataproc job for running Apache Pig (https://pig.apache.org/) queries
        on YARN.
      type: object
      id: PigJob
    PrestoJob:
      properties:
        properties:
          additionalProperties:
            type: string
          type: object
          description: >-
            Optional. A mapping of property names to values. Used to set Presto
            session properties
            (https://prestodb.io/docs/current/sql/set-session.html) Equivalent
            to using the --session flag in the Presto CLI
        outputFormat:
          type: string
          description: >-
            Optional. The format in which query output will be displayed. See
            the Presto documentation for supported output formats
        queryFileUri:
          description: The HCFS URI of the script that contains SQL queries.
          type: string
        loggingConfig:
          description: Optional. The runtime log config for job execution.
          $ref: '#/components/schemas/LoggingConfig'
        clientTags:
          description: Optional. Presto client tags to attach to this query
          items:
            type: string
          type: array
        continueOnFailure:
          type: boolean
          description: >-
            Optional. Whether to continue executing queries if a query fails.
            The default value is false. Setting to true can be useful when
            executing independent parallel queries.
        queryList:
          description: A list of queries.
          $ref: '#/components/schemas/QueryList'
      description: >-
        A Dataproc job for running Presto (https://prestosql.io/) queries.
        IMPORTANT: The Dataproc Presto Optional Component
        (https://cloud.google.com/dataproc/docs/concepts/components/presto) must
        be enabled when the cluster is created to submit a Presto job to the
        cluster.
      type: object
      id: PrestoJob
    KubernetesSoftwareConfig:
      description: >-
        The software configuration for this Dataproc cluster running on
        Kubernetes.
      properties:
        componentVersion:
          additionalProperties:
            type: string
          description: >-
            The components that should be installed in this Dataproc cluster.
            The key must be a string from the KubernetesComponent enumeration.
            The value is the version of the software to be installed. At least
            one entry must be specified.
          type: object
        properties:
          description: >-
            The properties to set on daemon config files.Property keys are
            specified in prefix:property format, for example
            spark:spark.kubernetes.container.image. The following are supported
            prefixes and their mappings: spark: spark-defaults.confFor more
            information, see Cluster properties
            (https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
          type: object
          additionalProperties:
            type: string
      id: KubernetesSoftwareConfig
      type: object
    SummarizeSessionSparkApplicationExecutorsResponse:
      type: object
      properties:
        applicationId:
          description: Spark Application Id
          type: string
        deadExecutorSummary:
          $ref: '#/components/schemas/ConsolidatedExecutorSummary'
          description: Consolidated summary for dead executors.
        activeExecutorSummary:
          $ref: '#/components/schemas/ConsolidatedExecutorSummary'
          description: Consolidated summary for active executors.
        totalExecutorSummary:
          description: Overall consolidated summary for all executors.
          $ref: '#/components/schemas/ConsolidatedExecutorSummary'
      description: Consolidated summary of executors for a Spark Application.
      id: SummarizeSessionSparkApplicationExecutorsResponse
    JobScheduling:
      properties:
        maxFailuresPerHour:
          description: >-
            Optional. Maximum number of times per hour a driver can be restarted
            as a result of driver exiting with non-zero code before job is
            reported failed.A job might be reported as thrashing if the driver
            exits with a non-zero code four times within a 10-minute
            window.Maximum value is 10.Note: This restartable job option is not
            supported in Dataproc workflow templates
            (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template).
          type: integer
          format: int32
        maxFailuresTotal:
          type: integer
          description: >-
            Optional. Maximum total number of times a driver can be restarted as
            a result of the driver exiting with a non-zero code. After the
            maximum number is reached, the job will be reported as
            failed.Maximum value is 240.Note: Currently, this restartable job
            option is not supported in Dataproc workflow templates
            (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template).
          format: int32
      type: object
      id: JobScheduling
      description: Job scheduling options.
    SummarizeSparkApplicationStagesResponse:
      description: Summary of a Spark Application stages.
      id: SummarizeSparkApplicationStagesResponse
      properties:
        stagesSummary:
          $ref: '#/components/schemas/StagesSummary'
          description: Summary of a Spark Application Stages
      type: object
    SummarizeSparkApplicationStageAttemptTasksResponse:
      properties:
        stageAttemptTasksSummary:
          description: Summary of tasks for a Spark Application Stage Attempt
          $ref: '#/components/schemas/StageAttemptTasksSummary'
      type: object
      description: Summary of tasks for a Spark Application stage attempt.
      id: SummarizeSparkApplicationStageAttemptTasksResponse
    Session:
      properties:
        runtimeInfo:
          readOnly: true
          description: Output only. Runtime information about session execution.
          $ref: '#/components/schemas/RuntimeInfo'
        sparkConnectSession:
          $ref: '#/components/schemas/SparkConnectConfig'
          description: Optional. Spark connect session config.
        labels:
          additionalProperties:
            type: string
          type: object
          description: >-
            Optional. The labels to associate with the session. Label keys must
            contain 1 to 63 characters, and must conform to RFC 1035
            (https://www.ietf.org/rfc/rfc1035.txt). Label values may be empty,
            but, if present, must contain 1 to 63 characters, and must conform
            to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). No more than 32
            labels can be associated with a session.
        uuid:
          description: >-
            Output only. A session UUID (Unique Universal Identifier). The
            service generates this value when it creates the session.
          type: string
          readOnly: true
        createTime:
          description: Output only. The time when the session was created.
          type: string
          format: google-datetime
          readOnly: true
        stateTime:
          format: google-datetime
          type: string
          description: Output only. The time when the session entered the current state.
          readOnly: true
        user:
          type: string
          description: Optional. The email address of the user who owns the session.
        runtimeConfig:
          $ref: '#/components/schemas/RuntimeConfig'
          description: Optional. Runtime configuration for the session execution.
        jupyterSession:
          $ref: '#/components/schemas/JupyterConfig'
          description: Optional. Jupyter session config.
        sessionTemplate:
          type: string
          description: >-
            Optional. The session template used by the session.Only resource
            names, including project ID and location, are valid.Example: *
            https://www.googleapis.com/compute/v1/projects/[project_id]/locations/[dataproc_region]/sessionTemplates/[template_id]
            *
            projects/[project_id]/locations/[dataproc_region]/sessionTemplates/[template_id]The
            template must be in the same project and Dataproc region as the
            session.
        environmentConfig:
          description: Optional. Environment configuration for the session execution.
          $ref: '#/components/schemas/EnvironmentConfig'
        state:
          enumDescriptions:
            - The session state is unknown.
            - The session is created prior to running.
            - The session is running.
            - The session is terminating.
            - The session is terminated successfully.
            - The session is no longer running due to an error.
          description: Output only. A state of the session.
          type: string
          enum:
            - STATE_UNSPECIFIED
            - CREATING
            - ACTIVE
            - TERMINATING
            - TERMINATED
            - FAILED
          readOnly: true
        stateMessage:
          readOnly: true
          type: string
          description: >-
            Output only. Session state details, such as the failure description
            if the state is FAILED.
        name:
          type: string
          description: Identifier. The resource name of the session.
        creator:
          readOnly: true
          description: Output only. The email address of the user who created the session.
          type: string
        stateHistory:
          description: Output only. Historical state information for the session.
          readOnly: true
          type: array
          items:
            $ref: '#/components/schemas/SessionStateHistory'
      description: A representation of a session.
      id: Session
      type: object
    SummarizeSparkApplicationJobsResponse:
      properties:
        jobsSummary:
          description: Summary of a Spark Application Jobs
          $ref: '#/components/schemas/JobsSummary'
      id: SummarizeSparkApplicationJobsResponse
      description: Summary of a Spark Application jobs.
      type: object
    ManagedGroupConfig:
      type: object
      id: ManagedGroupConfig
      properties:
        instanceTemplateName:
          description: >-
            Output only. The name of the Instance Template used for the Managed
            Instance Group.
          readOnly: true
          type: string
        instanceGroupManagerName:
          readOnly: true
          description: Output only. The name of the Instance Group Manager for this group.
          type: string
        instanceGroupManagerUri:
          type: string
          description: >-
            Output only. The partial URI to the instance group manager for this
            group. E.g.
            projects/my-project/regions/us-central1/instanceGroupManagers/my-igm.
          readOnly: true
      description: Specifies the resources used to actively manage an instance group.
    ExecutorSummary:
      id: ExecutorSummary
      description: Details about executors used by the application.
      properties:
        executorLogs:
          type: object
          additionalProperties:
            type: string
        resources:
          additionalProperties:
            $ref: '#/components/schemas/ResourceInformation'
          type: object
        totalCores:
          type: integer
          format: int32
        totalInputBytes:
          type: string
          format: int64
        excludedInStages:
          type: array
          items:
            type: string
            format: int64
        attributes:
          additionalProperties:
            type: string
          type: object
        rddBlocks:
          type: integer
          format: int32
        maxTasks:
          format: int32
          type: integer
        isActive:
          type: boolean
        activeTasks:
          format: int32
          type: integer
        maxMemory:
          format: int64
          type: string
        isExcluded:
          type: boolean
        resourceProfileId:
          format: int32
          type: integer
        totalTasks:
          format: int32
          type: integer
        totalDurationMillis:
          format: int64
          type: string
        failedTasks:
          type: integer
          format: int32
        hostPort:
          type: string
        executorId:
          type: string
        totalShuffleRead:
          format: int64
          type: string
        diskUsed:
          type: string
          format: int64
        memoryUsed:
          format: int64
          type: string
        removeReason:
          type: string
        totalShuffleWrite:
          format: int64
          type: string
        completedTasks:
          type: integer
          format: int32
        removeTime:
          type: string
          format: google-datetime
        memoryMetrics:
          $ref: '#/components/schemas/MemoryMetrics'
        totalGcTimeMillis:
          type: string
          format: int64
        peakMemoryMetrics:
          $ref: '#/components/schemas/ExecutorMetrics'
        addTime:
          type: string
          format: google-datetime
      type: object
    SummarizeSparkApplicationExecutorsResponse:
      description: Consolidated summary of executors for a Spark Application.
      id: SummarizeSparkApplicationExecutorsResponse
      type: object
      properties:
        activeExecutorSummary:
          $ref: '#/components/schemas/ConsolidatedExecutorSummary'
          description: Consolidated summary for active executors.
        applicationId:
          description: Spark Application Id
          type: string
        totalExecutorSummary:
          description: Overall consolidated summary for all executors.
          $ref: '#/components/schemas/ConsolidatedExecutorSummary'
        deadExecutorSummary:
          description: Consolidated summary for dead executors.
          $ref: '#/components/schemas/ConsolidatedExecutorSummary'
    BasicAutoscalingAlgorithm:
      type: object
      id: BasicAutoscalingAlgorithm
      description: Basic algorithm for autoscaling.
      properties:
        yarnConfig:
          $ref: '#/components/schemas/BasicYarnAutoscalingConfig'
          description: Optional. YARN autoscaling configuration.
        sparkStandaloneConfig:
          description: Optional. Spark Standalone autoscaling configuration
          $ref: '#/components/schemas/SparkStandaloneAutoscalingConfig'
        cooldownPeriod:
          description: >-
            Optional. Duration between scaling events. A scaling period starts
            after the update operation from the previous event has
            completed.Bounds: 2m, 1d. Default: 2m.
          type: string
          format: google-duration
    JupyterConfig:
      description: Jupyter configuration for an interactive session.
      type: object
      properties:
        displayName:
          description: Optional. Display name, shown in the Jupyter kernelspec card.
          type: string
        kernel:
          enumDescriptions:
            - The kernel is unknown.
            - Python kernel.
            - Scala kernel.
          enum:
            - KERNEL_UNSPECIFIED
            - PYTHON
            - SCALA
          description: Optional. Kernel
          type: string
      id: JupyterConfig
    ListSessionTemplatesResponse:
      id: ListSessionTemplatesResponse
      description: A list of session templates.
      properties:
        sessionTemplates:
          type: array
          items:
            $ref: '#/components/schemas/SessionTemplate'
          readOnly: true
          description: Output only. Session template list
        nextPageToken:
          type: string
          description: >-
            A token, which can be sent as page_token to retrieve the next page.
            If this field is omitted, there are no subsequent pages.
      type: object
    UsageSnapshot:
      id: UsageSnapshot
      description: >-
        The usage snapshot represents the resources consumed by a workload at a
        specified time.
      type: object
      properties:
        milliAccelerator:
          format: int64
          type: string
          description: >-
            Optional. Milli (one-thousandth) accelerator. (see Dataproc
            Serverless pricing
            (https://cloud.google.com/dataproc-serverless/pricing))
        milliDcuPremium:
          format: int64
          type: string
          description: >-
            Optional. Milli (one-thousandth) Dataproc Compute Units (DCUs)
            charged at premium tier (see Dataproc Serverless pricing
            (https://cloud.google.com/dataproc-serverless/pricing)).
        shuffleStorageGb:
          type: string
          description: >-
            Optional. Shuffle Storage in gigabytes (GB). (see Dataproc
            Serverless pricing
            (https://cloud.google.com/dataproc-serverless/pricing))
          format: int64
        acceleratorType:
          description: Optional. Accelerator type being used, if any
          type: string
        snapshotTime:
          type: string
          description: Optional. The timestamp of the usage snapshot.
          format: google-datetime
        shuffleStorageGbPremium:
          format: int64
          type: string
          description: >-
            Optional. Shuffle Storage in gigabytes (GB) charged at premium tier.
            (see Dataproc Serverless pricing
            (https://cloud.google.com/dataproc-serverless/pricing))
        milliDcu:
          format: int64
          description: >-
            Optional. Milli (one-thousandth) Dataproc Compute Units (DCUs) (see
            Dataproc Serverless pricing
            (https://cloud.google.com/dataproc-serverless/pricing)).
          type: string
    ProvisioningModelMix:
      properties:
        standardCapacityBase:
          format: int32
          description: >-
            Optional. The base capacity that will always use Standard VMs to
            avoid risk of more preemption than the minimum capacity you need.
            Dataproc will create only standard VMs until it reaches
            standard_capacity_base, then it will start using
            standard_capacity_percent_above_base to mix Spot with Standard VMs.
            eg. If 15 instances are requested and standard_capacity_base is 5,
            Dataproc will create 5 standard VMs and then start mixing spot and
            standard VMs for remaining 10 instances.
          type: integer
        standardCapacityPercentAboveBase:
          format: int32
          type: integer
          description: >-
            Optional. The percentage of target capacity that should use Standard
            VM. The remaining percentage will use Spot VMs. The percentage
            applies only to the capacity above standard_capacity_base. eg. If 15
            instances are requested and standard_capacity_base is 5 and
            standard_capacity_percent_above_base is 30, Dataproc will create 5
            standard VMs and then start mixing spot and standard VMs for
            remaining 10 instances. The mix will be 30% standard and 70% spot.
      type: object
      id: ProvisioningModelMix
      description: >-
        Defines how Dataproc should create VMs with a mixture of provisioning
        models.
    AuxiliaryNodeGroup:
      id: AuxiliaryNodeGroup
      description: Node group identification and configuration information.
      properties:
        nodeGroupId:
          description: >-
            Optional. A node group ID. Generated if not specified.The ID must
            contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and
            hyphens (-). Cannot begin or end with underscore or hyphen. Must
            consist of from 3 to 33 characters.
          type: string
        nodeGroup:
          $ref: '#/components/schemas/NodeGroup'
          description: Required. Node group configuration.
      type: object
    NativeBuildInfoUiData:
      type: object
      id: NativeBuildInfoUiData
      properties:
        buildClass:
          type: string
          description: Optional. Build class of Native.
        buildInfo:
          items:
            $ref: '#/components/schemas/BuildInfo'
          type: array
          description: Optional. Build related details.
    ReservationAffinity:
      description: Reservation Affinity for consuming Zonal reservation.
      id: ReservationAffinity
      type: object
      properties:
        key:
          description: Optional. Corresponds to the label key of reservation resource.
          type: string
        values:
          type: array
          description: Optional. Corresponds to the label values of reservation resource.
          items:
            type: string
        consumeReservationType:
          enumDescriptions:
            - ''
            - Do not consume from any allocated capacity.
            - Consume any reservation available.
            - >-
              Must consume from a specific reservation. Must specify key value
              fields for specifying the reservations.
          type: string
          enum:
            - TYPE_UNSPECIFIED
            - NO_RESERVATION
            - ANY_RESERVATION
            - SPECIFIC_RESERVATION
          description: Optional. Type of reservation to consume
    StageShuffleReadMetrics:
      id: StageShuffleReadMetrics
      description: Shuffle data read for the stage.
      properties:
        remoteBytesRead:
          type: string
          format: int64
        bytesRead:
          type: string
          format: int64
        remoteBlocksFetched:
          type: string
          format: int64
        localBytesRead:
          format: int64
          type: string
        localBlocksFetched:
          format: int64
          type: string
        stageShufflePushReadMetrics:
          $ref: '#/components/schemas/StageShufflePushReadMetrics'
        remoteReqsDuration:
          type: string
          format: int64
        fetchWaitTimeMillis:
          format: int64
          type: string
        remoteBytesReadToDisk:
          type: string
          format: int64
        recordsRead:
          type: string
          format: int64
      type: object
    BuildInfo:
      type: object
      properties:
        buildKey:
          type: string
          description: Optional. Build key.
        buildValue:
          description: Optional. Build value.
          type: string
      id: BuildInfo
      description: Native Build Info
    InputQuantileMetrics:
      id: InputQuantileMetrics
      properties:
        bytesRead:
          $ref: '#/components/schemas/Quantiles'
        recordsRead:
          $ref: '#/components/schemas/Quantiles'
      type: object
    SparkConnectConfig:
      id: SparkConnectConfig
      properties: {}
      description: Spark connect configuration for an interactive session.
      type: object
    InstantiateWorkflowTemplateRequest:
      type: object
      id: InstantiateWorkflowTemplateRequest
      properties:
        version:
          type: integer
          format: int32
          description: >-
            Optional. The version of workflow template to instantiate. If
            specified, the workflow will be instantiated only if the current
            version of the workflow template has the supplied version.This
            option cannot be used to instantiate a previous version of workflow
            template.
        parameters:
          additionalProperties:
            type: string
          description: >-
            Optional. Map from parameter names to values that should be used for
            those parameters. Values may not exceed 1000 characters.
          type: object
        requestId:
          description: >-
            Optional. A tag that prevents multiple concurrent workflow instances
            with the same tag from running. This mitigates risk of concurrent
            instances started due to retries.It is recommended to always set
            this value to a UUID
            (https://en.wikipedia.org/wiki/Universally_unique_identifier).The
            tag must contain only letters (a-z, A-Z), numbers (0-9), underscores
            (_), and hyphens (-). The maximum length is 40 characters.
          type: string
      description: A request to instantiate a workflow template.
    NamespacedGkeDeploymentTarget:
      deprecated: true
      properties:
        targetGkeCluster:
          type: string
          description: >-
            Optional. The target GKE cluster to deploy to. Format:
            'projects/{project}/locations/{location}/clusters/{cluster_id}'
        clusterNamespace:
          description: Optional. A namespace within the GKE cluster to deploy into.
          type: string
      description: >-
        Deprecated. Used only for the deprecated beta. A full,
        namespace-isolated deployment target for an existing GKE cluster.
      id: NamespacedGkeDeploymentTarget
      type: object
    PyPiRepositoryConfig:
      description: Configuration for PyPi repository
      id: PyPiRepositoryConfig
      properties:
        pypiRepository:
          type: string
          description: Optional. PyPi repository address
      type: object
    ClusterOperationMetadata:
      id: ClusterOperationMetadata
      properties:
        operationType:
          readOnly: true
          type: string
          description: Output only. The operation type.
        status:
          description: Output only. Current operation status.
          readOnly: true
          $ref: '#/components/schemas/ClusterOperationStatus'
        childOperationIds:
          type: array
          readOnly: true
          items:
            type: string
          description: Output only. Child operation ids
        clusterName:
          readOnly: true
          type: string
          description: Output only. Name of the cluster for the operation.
        statusHistory:
          type: array
          description: Output only. The previous operation status.
          items:
            $ref: '#/components/schemas/ClusterOperationStatus'
          readOnly: true
        description:
          description: Output only. Short description of operation.
          type: string
          readOnly: true
        warnings:
          items:
            type: string
          type: array
          readOnly: true
          description: Output only. Errors encountered during operation execution.
        clusterUuid:
          readOnly: true
          type: string
          description: Output only. Cluster UUID for the operation.
        labels:
          type: object
          additionalProperties:
            type: string
          description: Output only. Labels associated with the operation
          readOnly: true
      type: object
      description: Metadata describing the operation.
    GkeNodeConfig:
      properties:
        accelerators:
          items:
            $ref: '#/components/schemas/GkeNodePoolAcceleratorConfig'
          description: >-
            Optional. A list of hardware accelerators
            (https://cloud.google.com/compute/docs/gpus) to attach to each node.
          type: array
        bootDiskKmsKey:
          type: string
          description: >-
            Optional. The Customer Managed Encryption Key (CMEK)
            (https://cloud.google.com/kubernetes-engine/docs/how-to/using-cmek)
            used to encrypt the boot disk attached to each node in the node
            pool. Specify the key using the following format:
            projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}
        machineType:
          type: string
          description: >-
            Optional. The name of a Compute Engine machine type
            (https://cloud.google.com/compute/docs/machine-types).
        minCpuPlatform:
          description: >-
            Optional. Minimum CPU platform
            (https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
            to be used by this instance. The instance may be scheduled on the
            specified or a newer CPU platform. Specify the friendly names of CPU
            platforms, such as "Intel Haswell"` or Intel Sandy Bridge".
          type: string
        spot:
          description: >-
            Optional. Whether the nodes are created as Spot VM instances
            (https://cloud.google.com/compute/docs/instances/spot). Spot VMs are
            the latest update to legacy preemptible VMs. Spot VMs do not have a
            maximum lifetime. Legacy and Spot preemptible nodes cannot be used
            in a node pool with the CONTROLLER role or in the DEFAULT node pool
            if the CONTROLLER role is not assigned (the DEFAULT node pool will
            assume the CONTROLLER role).
          type: boolean
        preemptible:
          type: boolean
          description: >-
            Optional. Whether the nodes are created as legacy preemptible VM
            instances
            (https://cloud.google.com/compute/docs/instances/preemptible). Also
            see Spot VMs, preemptible VM instances without a maximum lifetime.
            Legacy and Spot preemptible nodes cannot be used in a node pool with
            the CONTROLLER role or in the DEFAULT node pool if the CONTROLLER
            role is not assigned (the DEFAULT node pool will assume the
            CONTROLLER role).
        localSsdCount:
          type: integer
          format: int32
          description: >-
            Optional. The number of local SSD disks to attach to the node, which
            is limited by the maximum number of disks allowable per zone (see
            Adding Local SSDs
            (https://cloud.google.com/compute/docs/disks/local-ssd)).
      type: object
      description: Parameters that describe cluster nodes.
      id: GkeNodeConfig
    ListWorkflowTemplatesResponse:
      type: object
      description: A response to a request to list workflow templates in a project.
      id: ListWorkflowTemplatesResponse
      properties:
        nextPageToken:
          readOnly: true
          type: string
          description: >-
            Output only. This token is included in the response if there are
            more results to fetch. To fetch additional results, provide this
            value as the page_token in a subsequent
            ListWorkflowTemplatesRequest.
        unreachable:
          items:
            type: string
          description: >-
            Output only. List of workflow templates that could not be included
            in the response. Attempting to get one of these resources may
            indicate why it was not included in the list response.
          type: array
          readOnly: true
        templates:
          description: Output only. WorkflowTemplates list.
          type: array
          readOnly: true
          items:
            $ref: '#/components/schemas/WorkflowTemplate'
    RegexValidation:
      properties:
        regexes:
          type: array
          description: >-
            Required. RE2 regular expressions used to validate the parameter's
            value. The value must match the regex in its entirety (substring
            matches are not sufficient).
          items:
            type: string
      description: Validation based on regular expressions.
      id: RegexValidation
      type: object
    SparkPlanGraphEdge:
      type: object
      properties:
        fromId:
          format: int64
          type: string
        toId:
          type: string
          format: int64
      description: Represents a directed edge in the spark plan tree from child to parent.
      id: SparkPlanGraphEdge
    NodeInitializationAction:
      id: NodeInitializationAction
      type: object
      properties:
        executionTimeout:
          type: string
          format: google-duration
          description: >-
            Optional. Amount of time executable has to complete. Default is 10
            minutes (see JSON representation of Duration
            (https://developers.google.com/protocol-buffers/docs/proto3#json)).Cluster
            creation fails with an explanatory error message (the name of the
            executable that caused the error and the exceeded timeout period) if
            the executable is not completed at end of the timeout period.
        executableFile:
          description: Required. Cloud Storage URI of executable file.
          type: string
      description: >-
        Specifies an executable to run on a fully configured node and a timeout
        period for executable completion.
    HadoopJob:
      properties:
        jarFileUris:
          type: array
          items:
            type: string
          description: >-
            Optional. Jar file URIs to add to the CLASSPATHs of the Hadoop
            driver and tasks.
        fileUris:
          type: array
          description: >-
            Optional. HCFS (Hadoop Compatible Filesystem) URIs of files to be
            copied to the working directory of Hadoop drivers and distributed
            tasks. Useful for naively parallel tasks.
          items:
            type: string
        args:
          items:
            type: string
          description: >-
            Optional. The arguments to pass to the driver. Do not include
            arguments, such as -libjars or -Dfoo=bar, that can be set as job
            properties, since a collision might occur that causes an incorrect
            job submission.
          type: array
        loggingConfig:
          description: Optional. The runtime log config for job execution.
          $ref: '#/components/schemas/LoggingConfig'
        mainClass:
          description: >-
            The name of the driver's main class. The jar file containing the
            class must be in the default CLASSPATH or specified in
            jar_file_uris.
          type: string
        mainJarFileUri:
          type: string
          description: >-
            The HCFS URI of the jar file containing the main class. Examples:
            'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar'
            'hdfs:/tmp/test-samples/custom-wordcount.jar'
            'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
        archiveUris:
          description: >-
            Optional. HCFS URIs of archives to be extracted in the working
            directory of Hadoop drivers and tasks. Supported file types: .jar,
            .tar, .tar.gz, .tgz, or .zip.
          type: array
          items:
            type: string
        properties:
          type: object
          additionalProperties:
            type: string
          description: >-
            Optional. A mapping of property names to values, used to configure
            Hadoop. Properties that conflict with values set by the Dataproc API
            might be overwritten. Can include properties set in
            /etc/hadoop/conf/*-site and classes in user code.
      id: HadoopJob
      type: object
      description: >-
        A Dataproc job for running Apache Hadoop MapReduce
        (https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)
        jobs on Apache Hadoop YARN
        (https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/YARN.html).
    Operation:
      type: object
      properties:
        error:
          $ref: '#/components/schemas/Status'
          description: >-
            The error result of the operation in case of failure or
            cancellation.
        response:
          type: object
          description: >-
            The normal, successful response of the operation. If the original
            method returns no data on success, such as Delete, the response is
            google.protobuf.Empty. If the original method is standard
            Get/Create/Update, the response should be the resource. For other
            methods, the response should have the type XxxResponse, where Xxx is
            the original method name. For example, if the original method name
            is TakeSnapshot(), the inferred response type is
            TakeSnapshotResponse.
          additionalProperties:
            type: any
            description: Properties of the object. Contains field @type with type URL.
        done:
          description: >-
            If the value is false, it means the operation is still in progress.
            If true, the operation is completed, and either error or response is
            available.
          type: boolean
        metadata:
          description: >-
            Service-specific metadata associated with the operation. It
            typically contains progress information and common metadata such as
            create time. Some services might not provide such metadata. Any
            method that returns a long-running operation should document the
            metadata type, if any.
          type: object
          additionalProperties:
            type: any
            description: Properties of the object. Contains field @type with type URL.
        name:
          description: >-
            The server-assigned name, which is only unique within the same
            service that originally returns it. If you use the default HTTP
            mapping, the name should be a resource name ending with
            operations/{unique_id}.
          type: string
      description: >-
        This resource represents a long-running operation that is the result of
        a network API call.
      id: Operation
    ClusterOperation:
      description: The cluster operation triggered by a workflow.
      type: object
      id: ClusterOperation
      properties:
        operationId:
          readOnly: true
          type: string
          description: Output only. The id of the cluster operation.
        error:
          description: Output only. Error, if operation failed.
          type: string
          readOnly: true
        done:
          readOnly: true
          type: boolean
          description: Output only. Indicates the operation is done.
    SubmitJobRequest:
      type: object
      description: A request to submit a job.
      id: SubmitJobRequest
      properties:
        requestId:
          type: string
          description: >-
            Optional. A unique id used to identify the request. If the server
            receives two SubmitJobRequest
            (https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.SubmitJobRequest)s
            with the same id, then the second request will be ignored and the
            first Job created and stored in the backend is returned.It is
            recommended to always set this value to a UUID
            (https://en.wikipedia.org/wiki/Universally_unique_identifier).The id
            must contain only letters (a-z, A-Z), numbers (0-9), underscores
            (_), and hyphens (-). The maximum length is 40 characters.
        job:
          $ref: '#/components/schemas/Job'
          description: Required. The job resource.
    SparkHistoryServerConfig:
      type: object
      properties:
        dataprocCluster:
          type: string
          description: >-
            Optional. Resource name of an existing Dataproc Cluster to act as a
            Spark History Server for the workload.Example:
            projects/[project_id]/regions/[region]/clusters/[cluster_name]
      description: Spark History Server configuration for the workload.
      id: SparkHistoryServerConfig
    SummarizeSessionSparkApplicationStageAttemptTasksResponse:
      id: SummarizeSessionSparkApplicationStageAttemptTasksResponse
      type: object
      properties:
        stageAttemptTasksSummary:
          description: Summary of tasks for a Spark Application Stage Attempt
          $ref: '#/components/schemas/StageAttemptTasksSummary'
      description: Summary of tasks for a Spark Application stage attempt.
    PeripheralsConfig:
      description: Auxiliary services configuration for a workload.
      type: object
      properties:
        metastoreService:
          description: >-
            Optional. Resource name of an existing Dataproc Metastore
            service.Example:
            projects/[project_id]/locations/[region]/services/[service_id]
          type: string
        sparkHistoryServerConfig:
          description: Optional. The Spark History Server configuration for the workload.
          $ref: '#/components/schemas/SparkHistoryServerConfig'
      id: PeripheralsConfig
    JobPlacement:
      id: JobPlacement
      type: object
      description: Dataproc job config.
      properties:
        clusterUuid:
          type: string
          description: >-
            Output only. A cluster UUID generated by the Dataproc service when
            the job is submitted.
          readOnly: true
        clusterLabels:
          description: >-
            Optional. Cluster labels to identify a cluster where the job will be
            submitted.
          type: object
          additionalProperties:
            type: string
        clusterName:
          description: Required. The name of the cluster where the job will be submitted.
          type: string
    SparkPlanGraphNode:
      description: Represents a node in the spark plan tree.
      type: object
      properties:
        name:
          type: string
        metrics:
          type: array
          items:
            $ref: '#/components/schemas/SqlPlanMetric'
        sparkPlanGraphNodeId:
          type: string
          format: int64
        desc:
          type: string
      id: SparkPlanGraphNode
    ValueInfo:
      description: Annotatated property value.
      type: object
      id: ValueInfo
      properties:
        overriddenValue:
          description: Optional. Value which was replaced by the corresponding component.
          type: string
        value:
          description: Property value.
          type: string
        annotation:
          description: Annotation, comment or explanation why the property was set.
          type: string
    PySparkBatch:
      description: >-
        A configuration for running an Apache PySpark
        (https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html)
        batch workload.
      type: object
      id: PySparkBatch
      properties:
        mainPythonFileUri:
          description: >-
            Required. The HCFS URI of the main Python file to use as the Spark
            driver. Must be a .py file.
          type: string
        pythonFileUris:
          description: >-
            Optional. HCFS file URIs of Python files to pass to the PySpark
            framework. Supported file types: .py, .egg, and .zip.
          items:
            type: string
          type: array
        fileUris:
          type: array
          description: >-
            Optional. HCFS URIs of files to be placed in the working directory
            of each executor.
          items:
            type: string
        archiveUris:
          items:
            type: string
          description: >-
            Optional. HCFS URIs of archives to be extracted into the working
            directory of each executor. Supported file types: .jar, .tar,
            .tar.gz, .tgz, and .zip.
          type: array
        jarFileUris:
          items:
            type: string
          type: array
          description: >-
            Optional. HCFS URIs of jar files to add to the classpath of the
            Spark driver and tasks.
        args:
          items:
            type: string
          type: array
          description: >-
            Optional. The arguments to pass to the driver. Do not include
            arguments that can be set as batch properties, such as --conf, since
            a collision can occur that causes an incorrect batch submission.
    WorkflowGraph:
      type: object
      properties:
        nodes:
          description: Output only. The workflow nodes.
          type: array
          items:
            $ref: '#/components/schemas/WorkflowNode'
          readOnly: true
      id: WorkflowGraph
      description: The workflow graph.
    ListClustersResponse:
      properties:
        clusters:
          description: Output only. The clusters in the project.
          items:
            $ref: '#/components/schemas/Cluster'
          type: array
          readOnly: true
        nextPageToken:
          description: >-
            Output only. This token is included in the response if there are
            more results to fetch. To fetch additional results, provide this
            value as the page_token in a subsequent ListClustersRequest.
          readOnly: true
          type: string
      id: ListClustersResponse
      type: object
      description: The list of all clusters in a project.
    SourceProgress:
      id: SourceProgress
      properties:
        endOffset:
          type: string
        numInputRows:
          format: int64
          type: string
        description:
          type: string
        metrics:
          additionalProperties:
            type: string
          type: object
        latestOffset:
          type: string
        processedRowsPerSecond:
          format: double
          type: number
        inputRowsPerSecond:
          type: number
          format: double
        startOffset:
          type: string
      type: object
    ExecutorMetrics:
      id: ExecutorMetrics
      properties:
        metrics:
          additionalProperties:
            type: string
            format: int64
          type: object
      type: object
    ManagedCluster:
      description: Cluster that is managed by the workflow.
      properties:
        config:
          description: Required. The cluster configuration.
          $ref: '#/components/schemas/ClusterConfig'
        clusterName:
          description: >-
            Required. The cluster name prefix. A unique cluster name will be
            formed by appending a random suffix.The name must contain only
            lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin
            with a letter. Cannot begin or end with hyphen. Must consist of
            between 2 and 35 characters.
          type: string
        labels:
          type: object
          description: >-
            Optional. The labels to associate with this cluster.Label keys must
            be between 1 and 63 characters long, and must conform to the
            following PCRE regular expression: \p{Ll}\p{Lo}{0,62}Label values
            must be between 1 and 63 characters long, and must conform to the
            following PCRE regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more
            than 32 labels can be associated with a given cluster.
          additionalProperties:
            type: string
      id: ManagedCluster
      type: object
    ListSessionsResponse:
      type: object
      description: A list of interactive sessions.
      properties:
        sessions:
          type: array
          items:
            $ref: '#/components/schemas/Session'
          readOnly: true
          description: Output only. The sessions from the specified collection.
        nextPageToken:
          description: >-
            A token, which can be sent as page_token, to retrieve the next page.
            If this field is omitted, there are no subsequent pages.
          type: string
      id: ListSessionsResponse
    MemoryMetrics:
      properties:
        totalOnHeapStorageMemory:
          type: string
          format: int64
        usedOnHeapStorageMemory:
          format: int64
          type: string
        totalOffHeapStorageMemory:
          type: string
          format: int64
        usedOffHeapStorageMemory:
          type: string
          format: int64
      id: MemoryMetrics
      type: object
    StartClusterRequest:
      id: StartClusterRequest
      properties:
        requestId:
          type: string
          description: >-
            Optional. A unique ID used to identify the request. If the server
            receives two StartClusterRequest
            (https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.StartClusterRequest)s
            with the same id, then the second request will be ignored and the
            first google.longrunning.Operation created and stored in the backend
            is returned.Recommendation: Set this value to a UUID
            (https://en.wikipedia.org/wiki/Universally_unique_identifier).The ID
            must contain only letters (a-z, A-Z), numbers (0-9), underscores
            (_), and hyphens (-). The maximum length is 40 characters.
        clusterUuid:
          description: >-
            Optional. Specifying the cluster_uuid means the RPC will fail (with
            error NOT_FOUND) if a cluster with the specified UUID does not
            exist.
          type: string
      type: object
      description: A request to start a cluster.
    StageOutputMetrics:
      properties:
        bytesWritten:
          format: int64
          type: string
        recordsWritten:
          type: string
          format: int64
      description: Metrics about the output written by the stage.
      type: object
      id: StageOutputMetrics
    SparkBatch:
      properties:
        mainJarFileUri:
          type: string
          description: Optional. The HCFS URI of the jar file that contains the main class.
        args:
          description: >-
            Optional. The arguments to pass to the driver. Do not include
            arguments that can be set as batch properties, such as --conf, since
            a collision can occur that causes an incorrect batch submission.
          items:
            type: string
          type: array
        fileUris:
          type: array
          items:
            type: string
          description: >-
            Optional. HCFS URIs of files to be placed in the working directory
            of each executor.
        jarFileUris:
          description: >-
            Optional. HCFS URIs of jar files to add to the classpath of the
            Spark driver and tasks.
          type: array
          items:
            type: string
        archiveUris:
          items:
            type: string
          type: array
          description: >-
            Optional. HCFS URIs of archives to be extracted into the working
            directory of each executor. Supported file types: .jar, .tar,
            .tar.gz, .tgz, and .zip.
        mainClass:
          description: >-
            Optional. The name of the driver main class. The jar file that
            contains the class must be in the classpath or specified in
            jar_file_uris.
          type: string
      type: object
      id: SparkBatch
      description: >-
        A configuration for running an Apache Spark (https://spark.apache.org/)
        batch workload.
    SparkStandaloneAutoscalingConfig:
      properties:
        scaleDownFactor:
          description: >-
            Required. Fraction of required executors to remove from Spark
            Serverless clusters. A scale-down factor of 1.0 will result in
            scaling down so that there are no more executors for the Spark
            Job.(more aggressive scaling). A scale-down factor closer to 0 will
            result in a smaller magnitude of scaling donw (less aggressive
            scaling).Bounds: 0.0, 1.0.
          type: number
          format: double
        scaleDownMinWorkerFraction:
          description: >-
            Optional. Minimum scale-down threshold as a fraction of total
            cluster size before scaling occurs. For example, in a 20-worker
            cluster, a threshold of 0.1 means the autoscaler must recommend at
            least a 2 worker scale-down for the cluster to scale. A threshold of
            0 means the autoscaler will scale down on any recommended
            change.Bounds: 0.0, 1.0. Default: 0.0.
          format: double
          type: number
        scaleUpFactor:
          format: double
          description: >-
            Required. Fraction of required workers to add to Spark Standalone
            clusters. A scale-up factor of 1.0 will result in scaling up so that
            there are no more required workers for the Spark Job (more
            aggressive scaling). A scale-up factor closer to 0 will result in a
            smaller magnitude of scaling up (less aggressive scaling).Bounds:
            0.0, 1.0.
          type: number
        removeOnlyIdleWorkers:
          description: Optional. Remove only idle workers when scaling down cluster
          type: boolean
        scaleUpMinWorkerFraction:
          description: >-
            Optional. Minimum scale-up threshold as a fraction of total cluster
            size before scaling occurs. For example, in a 20-worker cluster, a
            threshold of 0.1 means the autoscaler must recommend at least a
            2-worker scale-up for the cluster to scale. A threshold of 0 means
            the autoscaler will scale up on any recommended change.Bounds: 0.0,
            1.0. Default: 0.0.
          type: number
          format: double
        gracefulDecommissionTimeout:
          description: >-
            Required. Timeout for Spark graceful decommissioning of spark
            workers. Specifies the duration to wait for spark worker to complete
            spark decommissioning tasks before forcefully removing workers. Only
            applicable to downscaling operations.Bounds: 0s, 1d.
          type: string
          format: google-duration
      type: object
      id: SparkStandaloneAutoscalingConfig
      description: Basic autoscaling configurations for Spark Standalone.
    JobMetadata:
      id: JobMetadata
      properties:
        jobId:
          type: string
          readOnly: true
          description: Output only. The job id.
        operationType:
          description: Output only. Operation type.
          readOnly: true
          type: string
        startTime:
          format: google-datetime
          type: string
          readOnly: true
          description: Output only. Job submission time.
        status:
          readOnly: true
          description: Output only. Most recent job status.
          $ref: '#/components/schemas/JobStatus'
      type: object
      description: Job Operation metadata.
    AutoscalingPolicy:
      id: AutoscalingPolicy
      type: object
      properties:
        id:
          type: string
          description: >-
            Required. The policy id.The id must contain only letters (a-z, A-Z),
            numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end
            with underscore or hyphen. Must consist of between 3 and 50
            characters.
        basicAlgorithm:
          $ref: '#/components/schemas/BasicAutoscalingAlgorithm'
        labels:
          additionalProperties:
            type: string
          description: >-
            Optional. The labels to associate with this autoscaling policy.
            Label keys must contain 1 to 63 characters, and must conform to RFC
            1035 (https://www.ietf.org/rfc/rfc1035.txt). Label values may be
            empty, but, if present, must contain 1 to 63 characters, and must
            conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). No more
            than 32 labels can be associated with an autoscaling policy.
          type: object
        workerConfig:
          $ref: '#/components/schemas/InstanceGroupAutoscalingPolicyConfig'
          description: >-
            Required. Describes how the autoscaler will operate for primary
            workers.
        clusterType:
          type: string
          enumDescriptions:
            - Not set.
            - Standard dataproc cluster with a minimum of two primary workers.
            - >-
              Clusters that can use only secondary workers and be scaled down to
              zero secondary worker nodes.
          description: >-
            Optional. The type of the clusters for which this autoscaling policy
            is to be configured.
          enum:
            - CLUSTER_TYPE_UNSPECIFIED
            - STANDARD
            - ZERO_SCALE
        secondaryWorkerConfig:
          description: >-
            Optional. Describes how the autoscaler will operate for secondary
            workers.
          $ref: '#/components/schemas/InstanceGroupAutoscalingPolicyConfig'
        name:
          description: >-
            Output only. The "resource name" of the autoscaling policy, as
            described in https://cloud.google.com/apis/design/resource_names.
            For projects.regions.autoscalingPolicies, the resource name of the
            policy has the following format:
            projects/{project_id}/regions/{region}/autoscalingPolicies/{policy_id}
            For projects.locations.autoscalingPolicies, the resource name of the
            policy has the following format:
            projects/{project_id}/locations/{location}/autoscalingPolicies/{policy_id}
          readOnly: true
          type: string
      description: Describes an autoscaling policy for Dataproc cluster autoscaler.
    WorkflowTemplatePlacement:
      properties:
        clusterSelector:
          description: >-
            Optional. A selector that chooses target cluster for jobs based on
            metadata.The selector is evaluated at the time each job is
            submitted.
          $ref: '#/components/schemas/ClusterSelector'
        managedCluster:
          description: A cluster that is managed by the workflow.
          $ref: '#/components/schemas/ManagedCluster'
      type: object
      description: >-
        Specifies workflow execution target.Either managed_cluster or
        cluster_selector is required.
      id: WorkflowTemplatePlacement
    SparkPlanGraph:
      description: A graph used for storing information of an executionPlan of DataFrame.
      type: object
      properties:
        edges:
          items:
            $ref: '#/components/schemas/SparkPlanGraphEdge'
          type: array
        executionId:
          format: int64
          type: string
        nodes:
          items:
            $ref: '#/components/schemas/SparkPlanGraphNodeWrapper'
          type: array
      id: SparkPlanGraph
    StagesSummary:
      type: object
      properties:
        numSkippedStages:
          type: integer
          format: int32
        applicationId:
          type: string
        numCompletedStages:
          format: int32
          type: integer
        numPendingStages:
          type: integer
          format: int32
        numFailedStages:
          format: int32
          type: integer
        numActiveStages:
          format: int32
          type: integer
      id: StagesSummary
      description: Data related to Stages page summary
    StateHistory:
      properties:
        stateMessage:
          description: Output only. Details about the state at this point in history.
          readOnly: true
          type: string
        state:
          description: Output only. The state of the batch at this point in history.
          type: string
          enum:
            - STATE_UNSPECIFIED
            - PENDING
            - RUNNING
            - CANCELLING
            - CANCELLED
            - SUCCEEDED
            - FAILED
          readOnly: true
          enumDescriptions:
            - The batch state is unknown.
            - The batch is created before running.
            - The batch is running.
            - The batch is cancelling.
            - The batch cancellation was successful.
            - The batch completed successfully.
            - The batch is no longer running due to an error.
        stateStartTime:
          readOnly: true
          type: string
          format: google-datetime
          description: Output only. The time when the batch entered the historical state.
      description: Historical state information.
      type: object
      id: StateHistory
    GkeClusterConfig:
      properties:
        nodePoolTarget:
          items:
            $ref: '#/components/schemas/GkeNodePoolTarget'
          type: array
          description: >-
            Optional. GKE node pools where workloads will be scheduled. At least
            one node pool must be assigned the DEFAULT GkeNodePoolTarget.Role.
            If a GkeNodePoolTarget is not specified, Dataproc constructs a
            DEFAULT GkeNodePoolTarget. Each role can be given to only one
            GkeNodePoolTarget. All node pools must have the same location
            settings.
        namespacedGkeDeploymentTarget:
          $ref: '#/components/schemas/NamespacedGkeDeploymentTarget'
          deprecated: true
          description: >-
            Optional. Deprecated. Use gkeClusterTarget. Used only for the
            deprecated beta. A target for the deployment.
        gkeClusterTarget:
          type: string
          description: >-
            Optional. A target GKE cluster to deploy to. It must be in the same
            project and region as the Dataproc cluster (the GKE cluster can be
            zonal or regional). Format:
            'projects/{project}/locations/{location}/clusters/{cluster_id}'
      description: The cluster's GKE config.
      id: GkeClusterConfig
      type: object
    StageShufflePushReadMetrics:
      id: StageShufflePushReadMetrics
      properties:
        localMergedChunksFetched:
          format: int64
          type: string
        remoteMergedBytesRead:
          format: int64
          type: string
        corruptMergedBlockChunks:
          type: string
          format: int64
        localMergedBlocksFetched:
          type: string
          format: int64
        remoteMergedReqsDuration:
          format: int64
          type: string
        mergedFetchFallbackCount:
          format: int64
          type: string
        remoteMergedChunksFetched:
          format: int64
          type: string
        localMergedBytesRead:
          type: string
          format: int64
        remoteMergedBlocksFetched:
          type: string
          format: int64
      type: object
    StageInputMetrics:
      type: object
      description: Metrics about the input read by the stage.
      id: StageInputMetrics
      properties:
        bytesRead:
          format: int64
          type: string
        recordsRead:
          format: int64
          type: string
    IdentityConfig:
      id: IdentityConfig
      description: >-
        Identity related configuration, including service account based secure
        multi-tenancy user mappings.
      type: object
      properties:
        userServiceAccountMapping:
          additionalProperties:
            type: string
          description: Required. Map of user to service account.
          type: object
    RddPartitionInfo:
      type: object
      description: Information about RDD partitions.
      properties:
        storageLevel:
          type: string
        diskUsed:
          type: string
          format: int64
        blockName:
          type: string
        executors:
          type: array
          items:
            type: string
        memoryUsed:
          format: int64
          type: string
      id: RddPartitionInfo
    DiagnoseClusterRequest:
      id: DiagnoseClusterRequest
      type: object
      description: A request to collect cluster diagnostic information.
      properties:
        yarnApplicationIds:
          items:
            type: string
          type: array
          description: >-
            Optional. Specifies a list of yarn applications on which diagnosis
            is to be performed.
        diagnosisInterval:
          description: >-
            Optional. Time interval in which diagnosis should be carried out on
            the cluster.
          $ref: '#/components/schemas/Interval'
        yarnApplicationId:
          type: string
          description: >-
            Optional. DEPRECATED Specifies the yarn application on which
            diagnosis is to be performed.
          deprecated: true
        job:
          deprecated: true
          description: >-
            Optional. DEPRECATED Specifies the job on which diagnosis is to be
            performed. Format: projects/{project}/regions/{region}/jobs/{job}
          type: string
        tarballAccess:
          enum:
            - TARBALL_ACCESS_UNSPECIFIED
            - GOOGLE_CLOUD_SUPPORT
            - GOOGLE_DATAPROC_DIAGNOSE
          type: string
          description: >-
            Optional. (Optional) The access type to the diagnostic tarball. If
            not specified, falls back to default access of the bucket
          enumDescriptions:
            - >-
              Tarball Access unspecified. Falls back to default access of the
              bucket
            - >-
              Google Cloud Support group has read access to the diagnostic
              tarball
            - >-
              Google Cloud Dataproc Diagnose service account has read access to
              the diagnostic tarball
        tarballGcsDir:
          description: >-
            Optional. (Optional) The output Cloud Storage directory for the
            diagnostic tarball. If not specified, a task-specific directory in
            the cluster's staging bucket will be used.
          type: string
        jobs:
          description: >-
            Optional. Specifies a list of jobs on which diagnosis is to be
            performed. Format: projects/{project}/regions/{region}/jobs/{job}
          items:
            type: string
          type: array
    WorkflowNode:
      properties:
        error:
          description: Output only. The error detail.
          type: string
          readOnly: true
        jobId:
          description: >-
            Output only. The job id; populated after the node enters RUNNING
            state.
          type: string
          readOnly: true
        prerequisiteStepIds:
          description: Output only. Node's prerequisite nodes.
          items:
            type: string
          readOnly: true
          type: array
        stepId:
          type: string
          readOnly: true
          description: Output only. The name of the node.
        state:
          enum:
            - NODE_STATE_UNSPECIFIED
            - BLOCKED
            - RUNNABLE
            - RUNNING
            - COMPLETED
            - FAILED
          description: Output only. The node state.
          readOnly: true
          type: string
          enumDescriptions:
            - State is unspecified.
            - The node is awaiting prerequisite node to finish.
            - The node is runnable but not running.
            - The node is running.
            - The node completed successfully.
            - >-
              The node failed. A node can be marked FAILED because its ancestor
              or peer failed.
      id: WorkflowNode
      description: The workflow node.
      type: object
    SummarizeSessionSparkApplicationStagesResponse:
      description: Summary of a Spark Application stages.
      id: SummarizeSessionSparkApplicationStagesResponse
      properties:
        stagesSummary:
          $ref: '#/components/schemas/StagesSummary'
          description: Summary of a Spark Application Stages
      type: object
    SparkRJob:
      description: >-
        A Dataproc job for running Apache SparkR
        (https://spark.apache.org/docs/latest/sparkr.html) applications on YARN.
      type: object
      id: SparkRJob
      properties:
        mainRFileUri:
          type: string
          description: >-
            Required. The HCFS URI of the main R file to use as the driver. Must
            be a .R file.
        args:
          description: >-
            Optional. The arguments to pass to the driver. Do not include
            arguments, such as --conf, that can be set as job properties, since
            a collision may occur that causes an incorrect job submission.
          items:
            type: string
          type: array
        archiveUris:
          items:
            type: string
          type: array
          description: >-
            Optional. HCFS URIs of archives to be extracted into the working
            directory of each executor. Supported file types: .jar, .tar,
            .tar.gz, .tgz, and .zip.
        loggingConfig:
          description: Optional. The runtime log config for job execution.
          $ref: '#/components/schemas/LoggingConfig'
        properties:
          description: >-
            Optional. A mapping of property names to values, used to configure
            SparkR. Properties that conflict with values set by the Dataproc API
            might be overwritten. Can include properties set in
            /etc/spark/conf/spark-defaults.conf and classes in user code.
          additionalProperties:
            type: string
          type: object
        fileUris:
          items:
            type: string
          type: array
          description: >-
            Optional. HCFS URIs of files to be placed in the working directory
            of each executor. Useful for naively parallel tasks.
    RddOperationEdge:
      id: RddOperationEdge
      properties:
        toId:
          format: int32
          type: integer
        fromId:
          type: integer
          format: int32
      description: A directed edge representing dependency between two RDDs.
      type: object
    ClusterOperationStatus:
      type: object
      properties:
        state:
          enum:
            - UNKNOWN
            - PENDING
            - RUNNING
            - DONE
          readOnly: true
          enumDescriptions:
            - Unused.
            - The operation has been created.
            - The operation is running.
            - The operation is done; either cancelled or completed.
          type: string
          description: Output only. A message containing the operation state.
        details:
          type: string
          readOnly: true
          description: Output only. A message containing any operation metadata details.
        stateStartTime:
          readOnly: true
          type: string
          description: Output only. The time this state was entered.
          format: google-datetime
        innerState:
          type: string
          description: Output only. A message containing the detailed operation state.
          readOnly: true
      id: ClusterOperationStatus
      description: The status of the operation.
    Policy:
      properties:
        bindings:
          type: array
          items:
            $ref: '#/components/schemas/Binding'
          description: >-
            Associates a list of members, or principals, with a role.
            Optionally, may specify a condition that determines how and when the
            bindings are applied. Each of the bindings must contain at least one
            principal.The bindings in a Policy can refer to up to 1,500
            principals; up to 250 of these principals can be Google groups. Each
            occurrence of a principal counts towards these limits. For example,
            if the bindings grant 50 different roles to user:alice@example.com,
            and not to any other principal, then you can add another 1,450
            principals to the bindings in the Policy.
        etag:
          format: byte
          type: string
          description: >-
            etag is used for optimistic concurrency control as a way to help
            prevent simultaneous updates of a policy from overwriting each
            other. It is strongly suggested that systems make use of the etag in
            the read-modify-write cycle to perform policy updates in order to
            avoid race conditions: An etag is returned in the response to
            getIamPolicy, and systems are expected to put that etag in the
            request to setIamPolicy to ensure that their change will be applied
            to the same version of the policy.Important: If you use IAM
            Conditions, you must include the etag field whenever you call
            setIamPolicy. If you omit this field, then IAM allows you to
            overwrite a version 3 policy with a version 1 policy, and all of the
            conditions in the version 3 policy are lost.
        version:
          description: >-
            Specifies the format of the policy.Valid values are 0, 1, and 3.
            Requests that specify an invalid value are rejected.Any operation
            that affects conditional role bindings must specify version 3. This
            requirement applies to the following operations: Getting a policy
            that includes a conditional role binding Adding a conditional role
            binding to a policy Changing a conditional role binding in a policy
            Removing any role binding, with or without a condition, from a
            policy that includes conditionsImportant: If you use IAM Conditions,
            you must include the etag field whenever you call setIamPolicy. If
            you omit this field, then IAM allows you to overwrite a version 3
            policy with a version 1 policy, and all of the conditions in the
            version 3 policy are lost.If a policy does not include any
            conditions, operations on that policy may specify any valid version
            or leave the field unset.To learn which resources support conditions
            in their IAM policies, see the IAM documentation
            (https://cloud.google.com/iam/help/conditions/resource-policies).
          format: int32
          type: integer
      type: object
      id: Policy
      description: >-
        An Identity and Access Management (IAM) policy, which specifies access
        controls for Google Cloud resources.A Policy is a collection of
        bindings. A binding binds one or more members, or principals, to a
        single role. Principals can be user accounts, service accounts, Google
        groups, and domains (such as G Suite). A role is a named list of
        permissions; each role can be an IAM predefined role or a user-created
        custom role.For some types of Google Cloud resources, a binding can also
        specify a condition, which is a logical expression that allows access to
        a resource only if the expression evaluates to true. A condition can add
        constraints based on attributes of the request, the resource, or both.
        To learn which resources support conditions in their IAM policies, see
        the IAM documentation
        (https://cloud.google.com/iam/help/conditions/resource-policies).JSON
        example: { "bindings": [ { "role":
        "roles/resourcemanager.organizationAdmin", "members": [
        "user:mike@example.com", "group:admins@example.com",
        "domain:google.com",
        "serviceAccount:my-project-id@appspot.gserviceaccount.com" ] }, {
        "role": "roles/resourcemanager.organizationViewer", "members": [
        "user:eve@example.com" ], "condition": { "title": "expirable access",
        "description": "Does not grant access after Sep 2020", "expression":
        "request.time < timestamp('2020-10-01T00:00:00.000Z')", } } ], "etag":
        "BwWWja0YfJA=", "version": 3 } YAML example: bindings: - members: -
        user:mike@example.com - group:admins@example.com - domain:google.com -
        serviceAccount:my-project-id@appspot.gserviceaccount.com role:
        roles/resourcemanager.organizationAdmin - members: -
        user:eve@example.com role: roles/resourcemanager.organizationViewer
        condition: title: expirable access description: Does not grant access
        after Sep 2020 expression: request.time <
        timestamp('2020-10-01T00:00:00.000Z') etag: BwWWja0YfJA= version: 3 For
        a description of IAM and its features, see the IAM documentation
        (https://cloud.google.com/iam/docs/).
    NativeSqlExecutionUiData:
      type: object
      properties:
        fallbackDescription:
          type: string
          description: Optional. Description of the fallback.
        numFallbackNodes:
          format: int32
          description: Optional. Number of nodes fallen back to Spark.
          type: integer
        numNativeNodes:
          description: Optional. Number of nodes in Native.
          format: int32
          type: integer
        description:
          type: string
          description: Optional. Description of the execution.
        executionId:
          type: string
          description: Required. Execution ID of the Native SQL Execution.
          format: int64
        fallbackNodeToReason:
          description: Optional. Fallback node to reason.
          items:
            $ref: '#/components/schemas/FallbackReason'
          type: array
      description: Native SQL Execution Data
      id: NativeSqlExecutionUiData
    WriteSessionSparkApplicationContextResponse:
      description: Response returned as an acknowledgement of receipt of data.
      properties: {}
      type: object
      id: WriteSessionSparkApplicationContextResponse
    TemplateParameter:
      type: object
      id: TemplateParameter
      properties:
        fields:
          type: array
          description: >-
            Required. Paths to all fields that the parameter replaces. A field
            is allowed to appear in at most one parameter's list of field
            paths.A field path is similar in syntax to a
            google.protobuf.FieldMask. For example, a field path that references
            the zone field of a workflow template's cluster selector would be
            specified as placement.clusterSelector.zone.Also, field paths can
            reference fields using the following syntax: Values in maps can be
            referenced by key: labels'key'
            placement.clusterSelector.clusterLabels'key'
            placement.managedCluster.labels'key'
            placement.clusterSelector.clusterLabels'key'
            jobs'step-id'.labels'key' Jobs in the jobs list can be referenced by
            step-id: jobs'step-id'.hadoopJob.mainJarFileUri
            jobs'step-id'.hiveJob.queryFileUri
            jobs'step-id'.pySparkJob.mainPythonFileUri
            jobs'step-id'.hadoopJob.jarFileUris0
            jobs'step-id'.hadoopJob.archiveUris0
            jobs'step-id'.hadoopJob.fileUris0
            jobs'step-id'.pySparkJob.pythonFileUris0 Items in repeated fields
            can be referenced by a zero-based index:
            jobs'step-id'.sparkJob.args0 Other examples:
            jobs'step-id'.hadoopJob.properties'key'
            jobs'step-id'.hadoopJob.args0
            jobs'step-id'.hiveJob.scriptVariables'key'
            jobs'step-id'.hadoopJob.mainJarFileUri
            placement.clusterSelector.zoneIt may not be possible to parameterize
            maps and repeated fields in their entirety since only individual map
            values and individual items in repeated fields can be referenced.
            For example, the following field paths are invalid:
            placement.clusterSelector.clusterLabels jobs'step-id'.sparkJob.args
          items:
            type: string
        name:
          type: string
          description: >-
            Required. Parameter name. The parameter name is used as the key, and
            paired with the parameter value, which are passed to the template
            when the template is instantiated. The name must contain only
            capital letters (A-Z), numbers (0-9), and underscores (_), and must
            not start with a number. The maximum length is 40 characters.
        description:
          type: string
          description: >-
            Optional. Brief description of the parameter. Must not exceed 1024
            characters.
        validation:
          description: Optional. Validation rules to be applied to this parameter's value.
          $ref: '#/components/schemas/ParameterValidation'
      description: >-
        A configurable parameter that replaces one or more fields in the
        template. Parameterizable fields: - Labels - File uris - Job properties
        - Job arguments - Script variables - Main class (in HadoopJob and
        SparkJob) - Zone (in ClusterSelector)
    OutputMetrics:
      description: Metrics about the data written by the task.
      type: object
      id: OutputMetrics
      properties:
        recordsWritten:
          type: string
          format: int64
        bytesWritten:
          type: string
          format: int64
    Job:
      properties:
        flinkJob:
          description: Optional. Job is a Flink job.
          $ref: '#/components/schemas/FlinkJob'
        driverControlFilesUri:
          readOnly: true
          description: >-
            Output only. If present, the location of miscellaneous control files
            which can be used as part of job setup and handling. If not present,
            control files might be placed in the same location as
            driver_output_uri.
          type: string
        prestoJob:
          description: Optional. Job is a Presto job.
          $ref: '#/components/schemas/PrestoJob'
        yarnApplications:
          description: >-
            Output only. The collection of YARN applications spun up by this
            job.Beta Feature: This report is available for testing purposes
            only. It might be changed before final release.
          items:
            $ref: '#/components/schemas/YarnApplication'
          readOnly: true
          type: array
        placement:
          $ref: '#/components/schemas/JobPlacement'
          description: >-
            Required. Job information, including how, when, and where to run the
            job.
        status:
          $ref: '#/components/schemas/JobStatus'
          readOnly: true
          description: >-
            Output only. The job status. Additional application-specific status
            information might be contained in the type_job and yarn_applications
            fields.
        pigJob:
          description: Optional. Job is a Pig job.
          $ref: '#/components/schemas/PigJob'
        labels:
          additionalProperties:
            type: string
          description: >-
            Optional. The labels to associate with this job. Label keys must
            contain 1 to 63 characters, and must conform to RFC 1035
            (https://www.ietf.org/rfc/rfc1035.txt). Label values can be empty,
            but, if present, must contain 1 to 63 characters, and must conform
            to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). No more than 32
            labels can be associated with a job.
          type: object
        driverSchedulingConfig:
          $ref: '#/components/schemas/DriverSchedulingConfig'
          description: Optional. Driver scheduling configuration.
        statusHistory:
          type: array
          items:
            $ref: '#/components/schemas/JobStatus'
          description: Output only. The previous job status.
          readOnly: true
        pysparkJob:
          $ref: '#/components/schemas/PySparkJob'
          description: Optional. Job is a PySpark job.
        hadoopJob:
          $ref: '#/components/schemas/HadoopJob'
          description: Optional. Job is a Hadoop job.
        jobUuid:
          type: string
          readOnly: true
          description: >-
            Output only. A UUID that uniquely identifies a job within the
            project over time. This is in contrast to a user-settable
            reference.job_id that might be reused over time.
        sparkRJob:
          description: Optional. Job is a SparkR job.
          $ref: '#/components/schemas/SparkRJob'
        sparkJob:
          $ref: '#/components/schemas/SparkJob'
          description: Optional. Job is a Spark job.
        driverOutputResourceUri:
          type: string
          readOnly: true
          description: >-
            Output only. A URI pointing to the location of the stdout of the
            job's driver program.
        sparkSqlJob:
          $ref: '#/components/schemas/SparkSqlJob'
          description: Optional. Job is a SparkSql job.
        scheduling:
          description: Optional. Job scheduling configuration.
          $ref: '#/components/schemas/JobScheduling'
        done:
          description: >-
            Output only. Indicates whether the job is completed. If the value is
            false, the job is still in progress. If true, the job is completed,
            and status.state field will indicate if it was successful, failed,
            or cancelled.
          type: boolean
          readOnly: true
        hiveJob:
          description: Optional. Job is a Hive job.
          $ref: '#/components/schemas/HiveJob'
        reference:
          description: >-
            Optional. The fully qualified reference to the job, which can be
            used to obtain the equivalent REST path of the job resource. If this
            property is not specified when a job is created, the server
            generates a job_id.
          $ref: '#/components/schemas/JobReference'
        trinoJob:
          $ref: '#/components/schemas/TrinoJob'
          description: Optional. Job is a Trino job.
      type: object
      description: A Dataproc job resource.
      id: Job
    SearchSessionSparkApplicationsResponse:
      properties:
        nextPageToken:
          description: >-
            This token is included in the response if there are more results to
            fetch. To fetch additional results, provide this value as the
            page_token in a subsequent SearchSessionSparkApplicationsRequest.
          type: string
        sparkApplications:
          description: Output only. High level information corresponding to an application.
          readOnly: true
          type: array
          items:
            $ref: '#/components/schemas/SparkApplication'
      description: A list of summary of Spark Applications
      id: SearchSessionSparkApplicationsResponse
      type: object
    RepairNodeGroupRequest:
      properties:
        instanceNames:
          items:
            type: string
          type: array
          description: >-
            Required. Name of instances to be repaired. These instances must
            belong to specified node pool.
        requestId:
          type: string
          description: >-
            Optional. A unique ID used to identify the request. If the server
            receives two RepairNodeGroupRequest with the same ID, the second
            request is ignored and the first google.longrunning.Operation
            created and stored in the backend is returned.Recommendation: Set
            this value to a UUID
            (https://en.wikipedia.org/wiki/Universally_unique_identifier).The ID
            must contain only letters (a-z, A-Z), numbers (0-9), underscores
            (_), and hyphens (-). The maximum length is 40 characters.
        repairAction:
          enumDescriptions:
            - No action will be taken by default.
            - replace the specified list of nodes.
          type: string
          description: >-
            Required. Repair action to take on specified resources of the node
            pool.
          enum:
            - REPAIR_ACTION_UNSPECIFIED
            - REPLACE
      type: object
      id: RepairNodeGroupRequest
    ResourceProfileInfo:
      properties:
        resourceProfileId:
          type: integer
          format: int32
        executorResources:
          additionalProperties:
            $ref: '#/components/schemas/ExecutorResourceRequest'
          type: object
        taskResources:
          additionalProperties:
            $ref: '#/components/schemas/TaskResourceRequest'
          type: object
      type: object
      description: >-
        Resource profile that contains information about all the resources
        required by executors and tasks.
      id: ResourceProfileInfo
    InjectCredentialsRequest:
      id: InjectCredentialsRequest
      properties:
        clusterUuid:
          description: Required. The cluster UUID.
          type: string
        credentialsCiphertext:
          type: string
          description: >-
            Required. The encrypted credentials being injected in to the
            cluster.The client is responsible for encrypting the credentials in
            a way that is supported by the cluster.A wrapped value is used here
            so that the actual contents of the encrypted credentials are not
            written to audit logs.
      type: object
      description: A request to inject credentials into a cluster.
    AccessSparkApplicationSqlSparkPlanGraphResponse:
      type: object
      description: >-
        SparkPlanGraph for a Spark Application execution limited to maximum
        10000 clusters.
      properties:
        sparkPlanGraph:
          description: SparkPlanGraph for a Spark Application execution.
          $ref: '#/components/schemas/SparkPlanGraph'
      id: AccessSparkApplicationSqlSparkPlanGraphResponse
    WorkflowTemplate:
      properties:
        createTime:
          readOnly: true
          format: google-datetime
          description: Output only. The time template was created.
          type: string
        jobs:
          description: Required. The Directed Acyclic Graph of Jobs to submit.
          type: array
          items:
            $ref: '#/components/schemas/OrderedJob'
        encryptionConfig:
          description: >-
            Optional. Encryption settings for encrypting workflow template job
            arguments.
          $ref: >-
            #/components/schemas/GoogleCloudDataprocV1WorkflowTemplateEncryptionConfig
        id:
          type: string
        placement:
          description: Required. WorkflowTemplate scheduling information.
          $ref: '#/components/schemas/WorkflowTemplatePlacement'
        name:
          readOnly: true
          description: >-
            Output only. The resource name of the workflow template, as
            described in https://cloud.google.com/apis/design/resource_names.
            For projects.regions.workflowTemplates, the resource name of the
            template has the following format:
            projects/{project_id}/regions/{region}/workflowTemplates/{template_id}
            For projects.locations.workflowTemplates, the resource name of the
            template has the following format:
            projects/{project_id}/locations/{location}/workflowTemplates/{template_id}
          type: string
        parameters:
          type: array
          description: >-
            Optional. Template parameters whose values are substituted into the
            template. Values for parameters must be provided when the template
            is instantiated.
          items:
            $ref: '#/components/schemas/TemplateParameter'
        labels:
          type: object
          description: >-
            Optional. The labels to associate with this template. These labels
            will be propagated to all jobs and clusters created by the workflow
            instance.Label keys must contain 1 to 63 characters, and must
            conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt).Label
            values may be empty, but, if present, must contain 1 to 63
            characters, and must conform to RFC 1035
            (https://www.ietf.org/rfc/rfc1035.txt).No more than 32 labels can be
            associated with a template.
          additionalProperties:
            type: string
        updateTime:
          description: Output only. The time template was last updated.
          format: google-datetime
          readOnly: true
          type: string
        version:
          format: int32
          description: >-
            Optional. Used to perform a consistent read-modify-write.This field
            should be left blank for a CreateWorkflowTemplate request. It is
            required for an UpdateWorkflowTemplate request, and must match the
            current server version. A typical update template flow would fetch
            the current template with a GetWorkflowTemplate request, which will
            return the current template with the version field filled in with
            the current server version. The user updates other fields in the
            template, then returns it as part of the UpdateWorkflowTemplate
            request.
          type: integer
        dagTimeout:
          type: string
          description: >-
            Optional. Timeout duration for the DAG of jobs, expressed in seconds
            (see JSON representation of duration
            (https://developers.google.com/protocol-buffers/docs/proto3#json)).
            The timeout duration must be from 10 minutes ("600s") to 24 hours
            ("86400s"). The timer begins when the first job is submitted. If the
            workflow is running at the end of the timeout period, any remaining
            jobs are cancelled, the workflow is ended, and if the workflow was
            running on a managed cluster, the cluster is deleted.
          format: google-duration
      description: A Dataproc workflow template resource.
      id: WorkflowTemplate
      type: object
    Status:
      type: object
      description: >-
        The Status type defines a logical error model that is suitable for
        different programming environments, including REST APIs and RPC APIs. It
        is used by gRPC (https://github.com/grpc). Each Status message contains
        three pieces of data: error code, error message, and error details.You
        can find out more about this error model and how to work with it in the
        API Design Guide (https://cloud.google.com/apis/design/errors).
      id: Status
      properties:
        details:
          type: array
          description: >-
            A list of messages that carry the error details. There is a common
            set of message types for APIs to use.
          items:
            additionalProperties:
              description: Properties of the object. Contains field @type with type URL.
              type: any
            type: object
        message:
          description: >-
            A developer-facing error message, which should be in English. Any
            user-facing error message should be localized and sent in the
            google.rpc.Status.details field, or localized by the client.
          type: string
        code:
          description: The status code, which should be an enum value of google.rpc.Code.
          type: integer
          format: int32
    RepositoryConfig:
      type: object
      description: Configuration for dependency repositories
      properties:
        pypiRepositoryConfig:
          description: Optional. Configuration for PyPi repository.
          $ref: '#/components/schemas/PyPiRepositoryConfig'
      id: RepositoryConfig
    CancelJobRequest:
      description: A request to cancel a job.
      type: object
      id: CancelJobRequest
      properties: {}
    TrinoJob:
      description: >-
        A Dataproc job for running Trino (https://trino.io/) queries. IMPORTANT:
        The Dataproc Trino Optional Component
        (https://cloud.google.com/dataproc/docs/concepts/components/trino) must
        be enabled when the cluster is created to submit a Trino job to the
        cluster.
      type: object
      id: TrinoJob
      properties:
        continueOnFailure:
          description: >-
            Optional. Whether to continue executing queries if a query fails.
            The default value is false. Setting to true can be useful when
            executing independent parallel queries.
          type: boolean
        loggingConfig:
          $ref: '#/components/schemas/LoggingConfig'
          description: Optional. The runtime log config for job execution.
        queryFileUri:
          description: The HCFS URI of the script that contains SQL queries.
          type: string
        clientTags:
          type: array
          items:
            type: string
          description: Optional. Trino client tags to attach to this query
        outputFormat:
          type: string
          description: >-
            Optional. The format in which query output will be displayed. See
            the Trino documentation for supported output formats
        properties:
          additionalProperties:
            type: string
          description: >-
            Optional. A mapping of property names to values. Used to set Trino
            session properties
            (https://trino.io/docs/current/sql/set-session.html) Equivalent to
            using the --session flag in the Trino CLI
          type: object
        queryList:
          description: A list of queries.
          $ref: '#/components/schemas/QueryList'
    SparkSqlBatch:
      properties:
        queryFileUri:
          type: string
          description: >-
            Required. The HCFS URI of the script that contains Spark SQL queries
            to execute.
        jarFileUris:
          type: array
          items:
            type: string
          description: Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
        queryVariables:
          type: object
          description: >-
            Optional. Mapping of query variable names to values (equivalent to
            the Spark SQL command: SET name="value";).
          additionalProperties:
            type: string
      id: SparkSqlBatch
      type: object
      description: >-
        A configuration for running Apache Spark SQL
        (https://spark.apache.org/sql/) queries as a batch workload.
    RddStorageInfo:
      type: object
      description: Overall data about RDD storage.
      id: RddStorageInfo
      properties:
        storageLevel:
          type: string
        name:
          type: string
        numPartitions:
          format: int32
          type: integer
        partitions:
          items:
            $ref: '#/components/schemas/RddPartitionInfo'
          type: array
        rddStorageId:
          type: integer
          format: int32
        diskUsed:
          format: int64
          type: string
        dataDistribution:
          items:
            $ref: '#/components/schemas/RddDataDistribution'
          type: array
        memoryUsed:
          type: string
          format: int64
        numCachedPartitions:
          format: int32
          type: integer
    AccessSessionSparkApplicationEnvironmentInfoResponse:
      type: object
      id: AccessSessionSparkApplicationEnvironmentInfoResponse
      description: Environment details of a Saprk Application.
      properties:
        applicationEnvironmentInfo:
          $ref: '#/components/schemas/ApplicationEnvironmentInfo'
          description: Details about the Environment that the application is running in.
    SearchSessionSparkApplicationStagesResponse:
      type: object
      description: A list of stages associated with a Spark Application.
      id: SearchSessionSparkApplicationStagesResponse
      properties:
        sparkApplicationStages:
          type: array
          readOnly: true
          description: Output only. Data corresponding to a stage.
          items:
            $ref: '#/components/schemas/StageData'
        nextPageToken:
          type: string
          description: >-
            This token is included in the response if there are more results to
            fetch. To fetch additional results, provide this value as the
            page_token in a subsequent SearchSessionSparkApplicationStages.
    ProcessSummary:
      description: Process Summary
      type: object
      properties:
        processLogs:
          type: object
          additionalProperties:
            type: string
        hostPort:
          type: string
        processId:
          type: string
        isActive:
          type: boolean
        removeTime:
          type: string
          format: google-datetime
        totalCores:
          format: int32
          type: integer
        addTime:
          format: google-datetime
          type: string
      id: ProcessSummary
    LoggingConfig:
      description: The runtime logging config of the job.
      type: object
      id: LoggingConfig
      properties:
        driverLogLevels:
          description: >-
            The per-package log levels for the driver. This can include "root"
            package name to configure rootLogger. Examples: - 'com.google =
            FATAL' - 'root = INFO' - 'org.apache = DEBUG'
          additionalProperties:
            type: string
            enum:
              - LEVEL_UNSPECIFIED
              - ALL
              - TRACE
              - DEBUG
              - INFO
              - WARN
              - ERROR
              - FATAL
              - 'OFF'
            enumDescriptions:
              - Level is unspecified. Use default level for log4j.
              - Use ALL level for log4j.
              - Use TRACE level for log4j.
              - Use DEBUG level for log4j.
              - Use INFO level for log4j.
              - Use WARN level for log4j.
              - Use ERROR level for log4j.
              - Use FATAL level for log4j.
              - Turn off log4j.
          type: object
    SparkPlanGraphNodeWrapper:
      description: Wrapper user to represent either a node or a cluster.
      type: object
      id: SparkPlanGraphNodeWrapper
      properties:
        cluster:
          $ref: '#/components/schemas/SparkPlanGraphCluster'
        node:
          $ref: '#/components/schemas/SparkPlanGraphNode'
    AuthenticationConfig:
      description: >-
        Authentication configuration for a workload is used to set the default
        identity for the workload execution. The config specifies the type of
        identity (service account or user) that will be used by workloads to
        access resources on the project(s).
      type: object
      id: AuthenticationConfig
      properties:
        userWorkloadAuthenticationType:
          description: >-
            Optional. Authentication type for the user workload running in
            containers.
          enum:
            - AUTHENTICATION_TYPE_UNSPECIFIED
            - SERVICE_ACCOUNT
            - END_USER_CREDENTIALS
          type: string
          enumDescriptions:
            - >-
              If AuthenticationType is unspecified then END_USER_CREDENTIALS is
              used for 3.0 and newer runtimes, and SERVICE_ACCOUNT is used for
              older runtimes.
            - >-
              Use service account credentials for authenticating to other
              services.
            - >-
              Use OAuth credentials associated with the workload creator/user
              for authenticating to other services.
    OrderedJob:
      description: A job executed by the workflow.
      properties:
        scheduling:
          description: Optional. Job scheduling configuration.
          $ref: '#/components/schemas/JobScheduling'
        trinoJob:
          description: Optional. Job is a Trino job.
          $ref: '#/components/schemas/TrinoJob'
        flinkJob:
          description: Optional. Job is a Flink job.
          $ref: '#/components/schemas/FlinkJob'
        pysparkJob:
          $ref: '#/components/schemas/PySparkJob'
          description: Optional. Job is a PySpark job.
        prestoJob:
          description: Optional. Job is a Presto job.
          $ref: '#/components/schemas/PrestoJob'
        pigJob:
          description: Optional. Job is a Pig job.
          $ref: '#/components/schemas/PigJob'
        stepId:
          type: string
          description: >-
            Required. The step id. The id must be unique among all jobs within
            the template.The step id is used as prefix for job id, as job
            goog-dataproc-workflow-step-id label, and in prerequisiteStepIds
            field from other steps.The id must contain only letters (a-z, A-Z),
            numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end
            with underscore or hyphen. Must consist of between 3 and 50
            characters.
        labels:
          additionalProperties:
            type: string
          description: >-
            Optional. The labels to associate with this job.Label keys must be
            between 1 and 63 characters long, and must conform to the following
            regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1
            and 63 characters long, and must conform to the following regular
            expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be
            associated with a given job.
          type: object
        prerequisiteStepIds:
          description: >-
            Optional. The optional list of prerequisite job step_ids. If not
            specified, the job will start at the beginning of workflow.
          items:
            type: string
          type: array
        hiveJob:
          $ref: '#/components/schemas/HiveJob'
          description: Optional. Job is a Hive job.
        hadoopJob:
          $ref: '#/components/schemas/HadoopJob'
          description: Optional. Job is a Hadoop job.
        sparkJob:
          description: Optional. Job is a Spark job.
          $ref: '#/components/schemas/SparkJob'
        sparkRJob:
          $ref: '#/components/schemas/SparkRJob'
          description: Optional. Job is a SparkR job.
        sparkSqlJob:
          description: Optional. Job is a SparkSql job.
          $ref: '#/components/schemas/SparkSqlJob'
      type: object
      id: OrderedJob
    Binding:
      properties:
        members:
          items:
            type: string
          description: >-
            Specifies the principals requesting access for a Google Cloud
            resource. members can have the following values: allUsers: A special
            identifier that represents anyone who is on the internet; with or
            without a Google account. allAuthenticatedUsers: A special
            identifier that represents anyone who is authenticated with a Google
            account or a service account. Does not include identities that come
            from external identity providers (IdPs) through identity federation.
            user:{emailid}: An email address that represents a specific Google
            account. For example, alice@example.com . serviceAccount:{emailid}:
            An email address that represents a Google service account. For
            example, my-other-app@appspot.gserviceaccount.com.
            serviceAccount:{projectid}.svc.id.goog[{namespace}/{kubernetes-sa}]:
            An identifier for a Kubernetes service account
            (https://cloud.google.com/kubernetes-engine/docs/how-to/kubernetes-service-accounts).
            For example, my-project.svc.id.goog[my-namespace/my-kubernetes-sa].
            group:{emailid}: An email address that represents a Google group.
            For example, admins@example.com. domain:{domain}: The G Suite domain
            (primary) that represents all the users of that domain. For example,
            google.com or example.com.
            principal://iam.googleapis.com/locations/global/workforcePools/{pool_id}/subject/{subject_attribute_value}:
            A single identity in a workforce identity pool.
            principalSet://iam.googleapis.com/locations/global/workforcePools/{pool_id}/group/{group_id}:
            All workforce identities in a group.
            principalSet://iam.googleapis.com/locations/global/workforcePools/{pool_id}/attribute.{attribute_name}/{attribute_value}:
            All workforce identities with a specific attribute value.
            principalSet://iam.googleapis.com/locations/global/workforcePools/{pool_id}/*:
            All identities in a workforce identity pool.
            principal://iam.googleapis.com/projects/{project_number}/locations/global/workloadIdentityPools/{pool_id}/subject/{subject_attribute_value}:
            A single identity in a workload identity pool.
            principalSet://iam.googleapis.com/projects/{project_number}/locations/global/workloadIdentityPools/{pool_id}/group/{group_id}:
            A workload identity pool group.
            principalSet://iam.googleapis.com/projects/{project_number}/locations/global/workloadIdentityPools/{pool_id}/attribute.{attribute_name}/{attribute_value}:
            All identities in a workload identity pool with a certain attribute.
            principalSet://iam.googleapis.com/projects/{project_number}/locations/global/workloadIdentityPools/{pool_id}/*:
            All identities in a workload identity pool.
            deleted:user:{emailid}?uid={uniqueid}: An email address (plus unique
            identifier) representing a user that has been recently deleted. For
            example, alice@example.com?uid=123456789012345678901. If the user is
            recovered, this value reverts to user:{emailid} and the recovered
            user retains the role in the binding.
            deleted:serviceAccount:{emailid}?uid={uniqueid}: An email address
            (plus unique identifier) representing a service account that has
            been recently deleted. For example,
            my-other-app@appspot.gserviceaccount.com?uid=123456789012345678901.
            If the service account is undeleted, this value reverts to
            serviceAccount:{emailid} and the undeleted service account retains
            the role in the binding. deleted:group:{emailid}?uid={uniqueid}: An
            email address (plus unique identifier) representing a Google group
            that has been recently deleted. For example,
            admins@example.com?uid=123456789012345678901. If the group is
            recovered, this value reverts to group:{emailid} and the recovered
            group retains the role in the binding.
            deleted:principal://iam.googleapis.com/locations/global/workforcePools/{pool_id}/subject/{subject_attribute_value}:
            Deleted single identity in a workforce identity pool. For example,
            deleted:principal://iam.googleapis.com/locations/global/workforcePools/my-pool-id/subject/my-subject-attribute-value.
          type: array
        condition:
          description: >-
            The condition that is associated with this binding.If the condition
            evaluates to true, then this binding applies to the current
            request.If the condition evaluates to false, then this binding does
            not apply to the current request. However, a different role binding
            might grant the same role to one or more of the principals in this
            binding.To learn which resources support conditions in their IAM
            policies, see the IAM documentation
            (https://cloud.google.com/iam/help/conditions/resource-policies).
          $ref: '#/components/schemas/Expr'
        role:
          description: >-
            Role that is assigned to the list of members, or principals. For
            example, roles/viewer, roles/editor, or roles/owner.For an overview
            of the IAM roles and permissions, see the IAM documentation
            (https://cloud.google.com/iam/docs/roles-overview). For a list of
            the available pre-defined roles, see here
            (https://cloud.google.com/iam/docs/understanding-roles).
          type: string
      description: Associates members, or principals, with a role.
      id: Binding
      type: object
    JobStatus:
      id: JobStatus
      type: object
      properties:
        details:
          readOnly: true
          type: string
          description: >-
            Optional. Output only. Job state details, such as an error
            description if the state is ERROR.
        stateStartTime:
          type: string
          format: google-datetime
          readOnly: true
          description: Output only. The time when this state was entered.
        substate:
          enum:
            - UNSPECIFIED
            - SUBMITTED
            - QUEUED
            - STALE_STATUS
          readOnly: true
          type: string
          enumDescriptions:
            - The job substate is unknown.
            - The Job is submitted to the agent.Applies to RUNNING state.
            - >-
              The Job has been received and is awaiting execution (it might be
              waiting for a condition to be met). See the "details" field for
              the reason for the delay.Applies to RUNNING state.
            - >-
              The agent-reported status is out of date, which can be caused by a
              loss of communication between the agent and Dataproc. If the agent
              does not send a timely update, the job will fail.Applies to
              RUNNING state.
          description: >-
            Output only. Additional state information, which includes status
            reported by the agent.
        state:
          description: Output only. A state message specifying the overall job state.
          type: string
          enumDescriptions:
            - The job state is unknown.
            - The job is pending; it has been submitted, but is not yet running.
            - >-
              Job has been received by the service and completed initial setup;
              it will soon be submitted to the cluster.
            - The job is running on the cluster.
            - A CancelJob request has been received, but is pending.
            - >-
              Transient in-flight resources have been canceled, and the request
              to cancel the running job has been issued to the cluster.
            - The job cancellation was successful.
            - The job has completed successfully.
            - The job has completed, but encountered an error.
            - >-
              Job attempt has failed. The detail field contains failure details
              for this attempt.Applies to restartable jobs only.
          readOnly: true
          enum:
            - STATE_UNSPECIFIED
            - PENDING
            - SETUP_DONE
            - RUNNING
            - CANCEL_PENDING
            - CANCEL_STARTED
            - CANCELLED
            - DONE
            - ERROR
            - ATTEMPT_FAILURE
      description: Dataproc job status.
    InstanceGroupConfig:
      description: >-
        The config settings for Compute Engine resources in an instance group,
        such as a master or worker group.
      type: object
      properties:
        machineTypeUri:
          description: >-
            Optional. The Compute Engine machine type used for cluster
            instances.A full URL, partial URI, or short name are valid.
            Examples:
            https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2
            projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2
            n1-standard-2Auto Zone Exception: If you are using the Dataproc Auto
            Zone Placement
            (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement)
            feature, you must use the short name of the machine type resource,
            for example, n1-standard-2.
          type: string
        diskConfig:
          description: Optional. Disk option config settings.
          $ref: '#/components/schemas/DiskConfig'
        imageUri:
          description: >-
            Optional. The Compute Engine image resource used for cluster
            instances.The URI can represent an image or image family.Image
            examples:
            https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/[image-id]
            projects/[project_id]/global/images/[image-id] image-idImage family
            examples. Dataproc will use the most recent image from the family:
            https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/family/[custom-image-family-name]
            projects/[project_id]/global/images/family/[custom-image-family-name]If
            the URI is unspecified, it will be inferred from
            SoftwareConfig.image_version or the system default.
          type: string
        numInstances:
          description: >-
            Optional. The number of VM instances in the instance group. For HA
            cluster master_config groups, must be set to 3. For standard cluster
            master_config groups, must be set to 1.
          type: integer
          format: int32
        managedGroupConfig:
          readOnly: true
          $ref: '#/components/schemas/ManagedGroupConfig'
          description: >-
            Output only. The config for Compute Engine Instance Group Manager
            that manages this group. This is only used for preemptible instance
            groups.
        minNumInstances:
          type: integer
          format: int32
          description: >-
            Optional. The minimum number of primary worker instances to create.
            If min_num_instances is set, cluster creation will succeed if the
            number of primary workers created is at least equal to the
            min_num_instances number.Example: Cluster creation request with
            num_instances = 5 and min_num_instances = 3: If 4 VMs are created
            and 1 instance fails, the failed VM is deleted. The cluster is
            resized to 4 instances and placed in a RUNNING state. If 2 instances
            are created and 3 instances fail, the cluster in placed in an ERROR
            state. The failed VMs are not deleted.
        preemptibility:
          type: string
          description: >-
            Optional. Specifies the preemptibility of the instance group.The
            default value for master and worker groups is NON_PREEMPTIBLE. This
            default cannot be changed.The default value for secondary instances
            is PREEMPTIBLE.
          enum:
            - PREEMPTIBILITY_UNSPECIFIED
            - NON_PREEMPTIBLE
            - PREEMPTIBLE
            - SPOT
          enumDescriptions:
            - >-
              Preemptibility is unspecified, the system will choose the
              appropriate setting for each instance group.
            - >-
              Instances are non-preemptible.This option is allowed for all
              instance groups and is the only valid value for Master and Worker
              instance groups.
            - >-
              Instances are preemptible
              (https://cloud.google.com/compute/docs/instances/preemptible).This
              option is allowed only for secondary worker
              (https://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms)
              groups.
            - >-
              Instances are Spot VMs
              (https://cloud.google.com/compute/docs/instances/spot).This option
              is allowed only for secondary worker
              (https://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms)
              groups. Spot VMs are the latest version of preemptible VMs
              (https://cloud.google.com/compute/docs/instances/preemptible), and
              provide additional features.
        accelerators:
          type: array
          items:
            $ref: '#/components/schemas/AcceleratorConfig'
          description: >-
            Optional. The Compute Engine accelerator configuration for these
            instances.
        minCpuPlatform:
          description: >-
            Optional. Specifies the minimum cpu platform for the Instance Group.
            See Dataproc -> Minimum CPU Platform
            (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
          type: string
        instanceReferences:
          description: Output only. List of references to Compute Engine instances.
          readOnly: true
          type: array
          items:
            $ref: '#/components/schemas/InstanceReference'
        startupConfig:
          description: >-
            Optional. Configuration to handle the startup of instances during
            cluster create and update process.
          $ref: '#/components/schemas/StartupConfig'
        instanceNames:
          items:
            type: string
          description: >-
            Output only. The list of instance names. Dataproc derives the names
            from cluster_name, num_instances, and the instance group.
          type: array
          readOnly: true
        instanceFlexibilityPolicy:
          description: >-
            Optional. Instance flexibility Policy allowing a mixture of VM
            shapes and provisioning models.
          $ref: '#/components/schemas/InstanceFlexibilityPolicy'
        isPreemptible:
          type: boolean
          description: >-
            Output only. Specifies that this instance group contains preemptible
            instances.
          readOnly: true
      id: InstanceGroupConfig
    GoogleCloudDataprocV1WorkflowTemplateEncryptionConfig:
      description: Encryption settings for encrypting workflow template job arguments.
      properties:
        kmsKey:
          description: >-
            Optional. The Cloud KMS key name to use for encrypting workflow
            template job arguments.When this this key is provided, the following
            workflow template job arguments
            (https://cloud.google.com/dataproc/docs/concepts/workflows/use-workflows#adding_jobs_to_a_template),
            if present, are CMEK encrypted
            (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/customer-managed-encryption#use_cmek_with_workflow_template_data):
            FlinkJob args
            (https://cloud.google.com/dataproc/docs/reference/rest/v1/FlinkJob)
            HadoopJob args
            (https://cloud.google.com/dataproc/docs/reference/rest/v1/HadoopJob)
            SparkJob args
            (https://cloud.google.com/dataproc/docs/reference/rest/v1/SparkJob)
            SparkRJob args
            (https://cloud.google.com/dataproc/docs/reference/rest/v1/SparkRJob)
            PySparkJob args
            (https://cloud.google.com/dataproc/docs/reference/rest/v1/PySparkJob)
            SparkSqlJob
            (https://cloud.google.com/dataproc/docs/reference/rest/v1/SparkSqlJob)
            scriptVariables and queryList.queries HiveJob
            (https://cloud.google.com/dataproc/docs/reference/rest/v1/HiveJob)
            scriptVariables and queryList.queries PigJob
            (https://cloud.google.com/dataproc/docs/reference/rest/v1/PigJob)
            scriptVariables and queryList.queries PrestoJob
            (https://cloud.google.com/dataproc/docs/reference/rest/v1/PrestoJob)
            scriptVariables and queryList.queries
          type: string
      type: object
      id: GoogleCloudDataprocV1WorkflowTemplateEncryptionConfig
    ExecutorMetricsDistributions:
      type: object
      properties:
        taskTimeMillis:
          items:
            type: number
            format: double
          type: array
        shuffleReadRecords:
          type: array
          items:
            format: double
            type: number
        diskBytesSpilled:
          items:
            type: number
            format: double
          type: array
        shuffleWriteRecords:
          type: array
          items:
            type: number
            format: double
        inputRecords:
          items:
            type: number
            format: double
          type: array
        memoryBytesSpilled:
          type: array
          items:
            type: number
            format: double
        peakMemoryMetrics:
          $ref: '#/components/schemas/ExecutorPeakMetricsDistributions'
        shuffleWrite:
          type: array
          items:
            type: number
            format: double
        outputRecords:
          items:
            format: double
            type: number
          type: array
        quantiles:
          type: array
          items:
            format: double
            type: number
        outputBytes:
          items:
            format: double
            type: number
          type: array
        shuffleRead:
          type: array
          items:
            format: double
            type: number
        inputBytes:
          items:
            type: number
            format: double
          type: array
        succeededTasks:
          items:
            format: double
            type: number
          type: array
        failedTasks:
          type: array
          items:
            format: double
            type: number
        killedTasks:
          type: array
          items:
            type: number
            format: double
      id: ExecutorMetricsDistributions
    ShufflePushReadMetrics:
      id: ShufflePushReadMetrics
      properties:
        localMergedBytesRead:
          type: string
          format: int64
        localMergedBlocksFetched:
          type: string
          format: int64
        remoteMergedChunksFetched:
          type: string
          format: int64
        localMergedChunksFetched:
          type: string
          format: int64
        mergedFetchFallbackCount:
          type: string
          format: int64
        remoteMergedBlocksFetched:
          format: int64
          type: string
        corruptMergedBlockChunks:
          format: int64
          type: string
        remoteMergedReqsDuration:
          format: int64
          type: string
        remoteMergedBytesRead:
          type: string
          format: int64
      type: object
    RddOperationCluster:
      id: RddOperationCluster
      properties:
        childClusters:
          type: array
          items:
            $ref: '#/components/schemas/RddOperationCluster'
        name:
          type: string
        rddClusterId:
          type: string
        childNodes:
          items:
            $ref: '#/components/schemas/RddOperationNode'
          type: array
      description: >-
        A grouping of nodes representing higher level constructs (stage, job
        etc.).
      type: object
    Quantiles:
      properties:
        sum:
          type: string
          format: int64
        minimum:
          format: int64
          type: string
        percentile25:
          type: string
          format: int64
        count:
          format: int64
          type: string
        percentile75:
          format: int64
          type: string
        percentile50:
          format: int64
          type: string
        maximum:
          format: int64
          type: string
      id: Quantiles
      type: object
      description: >-
        Quantile metrics data related to Tasks. Units can be seconds, bytes,
        milliseconds, etc depending on the message type.
    Interval:
      id: Interval
      description: >-
        Represents a time interval, encoded as a Timestamp start (inclusive) and
        a Timestamp end (exclusive).The start must be less than or equal to the
        end. When the start equals the end, the interval is empty (matches no
        time). When both start and end are unspecified, the interval matches any
        time.
      type: object
      properties:
        endTime:
          format: google-datetime
          type: string
          description: >-
            Optional. Exclusive end of the interval.If specified, a Timestamp
            matching this interval will have to be before the end.
        startTime:
          format: google-datetime
          type: string
          description: >-
            Optional. Inclusive start of the interval.If specified, a Timestamp
            matching this interval will have to be the same or after the start.
    SearchSessionSparkApplicationJobsResponse:
      id: SearchSessionSparkApplicationJobsResponse
      properties:
        sparkApplicationJobs:
          readOnly: true
          items:
            $ref: '#/components/schemas/JobData'
          description: Output only. Data corresponding to a spark job.
          type: array
        nextPageToken:
          description: >-
            This token is included in the response if there are more results to
            fetch. To fetch additional results, provide this value as the
            page_token in a subsequent SearchSessionSparkApplicationJobsRequest.
          type: string
      type: object
      description: A list of Jobs associated with a Spark Application.
    AccessSessionSparkApplicationResponse:
      properties:
        application:
          description: Output only. High level information corresponding to an application.
          $ref: '#/components/schemas/ApplicationInfo'
          readOnly: true
      id: AccessSessionSparkApplicationResponse
      description: A summary of Spark Application
      type: object
    PropertiesInfo:
      type: object
      description: Properties of the workload organized by origin.
      id: PropertiesInfo
      properties:
        autotuningProperties:
          description: Output only. Properties set by autotuning engine.
          additionalProperties:
            $ref: '#/components/schemas/ValueInfo'
          type: object
          readOnly: true
    ConsolidatedExecutorSummary:
      id: ConsolidatedExecutorSummary
      properties:
        memoryMetrics:
          $ref: '#/components/schemas/MemoryMetrics'
        memoryUsed:
          type: string
          format: int64
        count:
          format: int32
          type: integer
        diskUsed:
          format: int64
          type: string
        totalShuffleRead:
          format: int64
          type: string
        totalShuffleWrite:
          format: int64
          type: string
        isExcluded:
          format: int32
          type: integer
        totalDurationMillis:
          type: string
          format: int64
        totalCores:
          format: int32
          type: integer
        totalTasks:
          type: integer
          format: int32
        rddBlocks:
          type: integer
          format: int32
        totalGcTimeMillis:
          format: int64
          type: string
        failedTasks:
          format: int32
          type: integer
        totalInputBytes:
          format: int64
          type: string
        activeTasks:
          format: int32
          type: integer
        completedTasks:
          type: integer
          format: int32
        maxMemory:
          type: string
          format: int64
      type: object
      description: Consolidated summary about executors used by the application.
    AccessSparkApplicationStageAttemptResponse:
      id: AccessSparkApplicationStageAttemptResponse
      description: Stage Attempt for a Stage of a Spark Application
      type: object
      properties:
        stageData:
          $ref: '#/components/schemas/StageData'
          readOnly: true
          description: Output only. Data corresponding to a stage.
    Metric:
      type: object
      description: A Dataproc custom metric.
      properties:
        metricSource:
          enumDescriptions:
            - Required unspecified metric source.
            - >-
              Monitoring agent metrics. If this source is enabled, Dataproc
              enables the monitoring agent in Compute Engine, and collects
              monitoring agent metrics, which are published with an
              agent.googleapis.com prefix.
            - HDFS metric source.
            - Spark metric source.
            - YARN metric source.
            - Spark History Server metric source.
            - Hiveserver2 metric source.
            - hivemetastore metric source
            - flink metric source
          enum:
            - METRIC_SOURCE_UNSPECIFIED
            - MONITORING_AGENT_DEFAULTS
            - HDFS
            - SPARK
            - YARN
            - SPARK_HISTORY_SERVER
            - HIVESERVER2
            - HIVEMETASTORE
            - FLINK
          description: >-
            Required. A standard set of metrics is collected unless
            metricOverrides are specified for the metric source (see Custom
            metrics
            (https://cloud.google.com/dataproc/docs/guides/dataproc-metrics#custom_metrics)
            for more information).
          type: string
        metricOverrides:
          description: >-
            Optional. Specify one or more Custom metrics
            (https://cloud.google.com/dataproc/docs/guides/dataproc-metrics#custom_metrics)
            to collect for the metric course (for the SPARK metric source (any
            Spark metric
            (https://spark.apache.org/docs/latest/monitoring.html#metrics) can
            be specified).Provide metrics in the following format:
            METRIC_SOURCE: INSTANCE:GROUP:METRIC Use camelcase as
            appropriate.Examples:
            yarn:ResourceManager:QueueMetrics:AppsCompleted
            spark:driver:DAGScheduler:job.allJobs
            sparkHistoryServer:JVM:Memory:NonHeapMemoryUsage.committed
            hiveserver2:JVM:Memory:NonHeapMemoryUsage.used Notes: Only the
            specified overridden metrics are collected for the metric source.
            For example, if one or more spark:executive metrics are listed as
            metric overrides, other SPARK metrics are not collected. The
            collection of the metrics for other enabled custom metric sources is
            unaffected. For example, if both SPARK and YARN metric sources are
            enabled, and overrides are provided for Spark metrics only, all YARN
            metrics are collected.
          type: array
          items:
            type: string
      id: Metric
    NodeGroupOperationMetadata:
      properties:
        clusterUuid:
          type: string
          readOnly: true
          description: Output only. Cluster UUID associated with the node group operation.
        description:
          description: Output only. Short description of operation.
          readOnly: true
          type: string
        status:
          $ref: '#/components/schemas/ClusterOperationStatus'
          description: Output only. Current operation status.
          readOnly: true
        nodeGroupId:
          type: string
          readOnly: true
          description: Output only. Node group ID for the operation.
        statusHistory:
          readOnly: true
          items:
            $ref: '#/components/schemas/ClusterOperationStatus'
          description: Output only. The previous operation status.
          type: array
        operationType:
          type: string
          description: The operation type.
          enumDescriptions:
            - Node group operation type is unknown.
            - Create node group operation type.
            - Update node group operation type.
            - Delete node group operation type.
            - Resize node group operation type.
            - Repair node group operation type.
            - Update node group label operation type.
            - Start node group operation type.
            - Stop node group operation type.
            - >-
              This operation type is used to update the metadata config of a
              node group. We update the metadata of the VMs in the node group
              and await for intended config change to be completed at the node
              group level. Currently, only the identity config update is
              supported.
          enum:
            - NODE_GROUP_OPERATION_TYPE_UNSPECIFIED
            - CREATE
            - UPDATE
            - DELETE
            - RESIZE
            - REPAIR
            - UPDATE_LABELS
            - START
            - STOP
            - UPDATE_METADATA_CONFIG
        labels:
          type: object
          description: Output only. Labels associated with the operation.
          readOnly: true
          additionalProperties:
            type: string
        warnings:
          readOnly: true
          items:
            type: string
          type: array
          description: Output only. Errors encountered during operation execution.
      id: NodeGroupOperationMetadata
      type: object
      description: Metadata describing the node group operation.
    AccessSessionSparkApplicationStageAttemptResponse:
      type: object
      id: AccessSessionSparkApplicationStageAttemptResponse
      description: Stage Attempt for a Stage of a Spark Application
      properties:
        stageData:
          $ref: '#/components/schemas/StageData'
          description: Output only. Data corresponding to a stage.
          readOnly: true
    SearchSparkApplicationJobsResponse:
      description: A list of Jobs associated with a Spark Application.
      type: object
      id: SearchSparkApplicationJobsResponse
      properties:
        nextPageToken:
          description: >-
            This token is included in the response if there are more results to
            fetch. To fetch additional results, provide this value as the
            page_token in a subsequent SearchSparkApplicationJobsRequest.
          type: string
        sparkApplicationJobs:
          readOnly: true
          items:
            $ref: '#/components/schemas/JobData'
          description: Output only. Data corresponding to a spark job.
          type: array
    StreamingQueryProgress:
      type: object
      id: StreamingQueryProgress
      properties:
        runId:
          type: string
        observedMetrics:
          type: object
          additionalProperties:
            type: string
        durationMillis:
          additionalProperties:
            type: string
            format: int64
          type: object
        sink:
          $ref: '#/components/schemas/SinkProgress'
        stateOperators:
          type: array
          items:
            $ref: '#/components/schemas/StateOperatorProgress'
        timestamp:
          type: string
        eventTime:
          additionalProperties:
            type: string
          type: object
        batchDuration:
          format: int64
          type: string
        batchId:
          type: string
          format: int64
        sources:
          type: array
          items:
            $ref: '#/components/schemas/SourceProgress'
        streamingQueryProgressId:
          type: string
        name:
          type: string
    AutoscalingConfig:
      properties:
        policyUri:
          description: >-
            Optional. The autoscaling policy used by the cluster.Only resource
            names including projectid and location (region) are valid. Examples:
            https://www.googleapis.com/compute/v1/projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]
            projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]Note
            that the policy must be in the same project and Dataproc region.
          type: string
      type: object
      description: Autoscaling Policy config associated with the cluster.
      id: AutoscalingConfig
    AccessSessionSparkApplicationJobResponse:
      properties:
        jobData:
          description: Output only. Data corresponding to a spark job.
          readOnly: true
          $ref: '#/components/schemas/JobData'
      type: object
      description: Details of a particular job associated with Spark Application
      id: AccessSessionSparkApplicationJobResponse
    WriteSparkApplicationContextResponse:
      id: WriteSparkApplicationContextResponse
      description: Response returned as an acknowledgement of receipt of data.
      properties: {}
      type: object
    TerminateSessionRequest:
      id: TerminateSessionRequest
      type: object
      properties:
        requestId:
          type: string
          description: >-
            Optional. A unique ID used to identify the request. If the service
            receives two TerminateSessionRequest
            (https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.TerminateSessionRequest)s
            with the same ID, the second request is ignored.Recommendation: Set
            this value to a UUID
            (https://en.wikipedia.org/wiki/Universally_unique_identifier).The
            value must contain only letters (a-z, A-Z), numbers (0-9),
            underscores (_), and hyphens (-). The maximum length is 40
            characters.
      description: A request to terminate an interactive session.
    SoftwareConfig:
      properties:
        optionalComponents:
          type: array
          items:
            enumDescriptions:
              - >-
                Unspecified component. Specifying this will cause Cluster
                creation to fail.
              - >-
                The Anaconda component is no longer supported or applicable to
                supported Dataproc on Compute Engine image versions
                (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-version-clusters#supported-dataproc-image-versions).
                It cannot be activated on clusters created with supported
                Dataproc on Compute Engine image versions.
              - Delta Lake.
              - Docker
              - The Druid query engine. (alpha)
              - Flink
              - HBase. (beta)
              - The Hive Web HCatalog (the REST service for accessing HCatalog).
              - Hudi.
              - Iceberg.
              - The Jupyter Notebook.
              - The Pig component.
              - The Presto query engine.
              - The Trino query engine.
              - The Ranger service.
              - The Solr service.
              - The Zeppelin notebook.
              - The Zookeeper service.
              - The Jupyter Kernel Gateway.
            enum:
              - COMPONENT_UNSPECIFIED
              - ANACONDA
              - DELTA
              - DOCKER
              - DRUID
              - FLINK
              - HBASE
              - HIVE_WEBHCAT
              - HUDI
              - ICEBERG
              - JUPYTER
              - PIG
              - PRESTO
              - TRINO
              - RANGER
              - SOLR
              - ZEPPELIN
              - ZOOKEEPER
              - JUPYTER_KERNEL_GATEWAY
            type: string
          description: Optional. The set of components to activate on the cluster.
        properties:
          description: >-
            Optional. The properties to set on daemon config files.Property keys
            are specified in prefix:property format, for example
            core:hadoop.tmp.dir. The following are supported prefixes and their
            mappings: capacity-scheduler: capacity-scheduler.xml core:
            core-site.xml distcp: distcp-default.xml hdfs: hdfs-site.xml hive:
            hive-site.xml mapred: mapred-site.xml pig: pig.properties spark:
            spark-defaults.conf yarn: yarn-site.xmlFor more information, see
            Cluster properties
            (https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
          additionalProperties:
            type: string
          type: object
        imageVersion:
          type: string
          description: >-
            Optional. The version of software inside the cluster. It must be one
            of the supported Dataproc Versions
            (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#supported-dataproc-image-versions),
            such as "1.2" (including a subminor version, such as "1.2.29"), or
            the "preview" version
            (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions).
            If unspecified, it defaults to the latest Debian version.
      description: Specifies the selection and config of software inside the cluster.
      type: object
      id: SoftwareConfig
    SessionOperationMetadata:
      type: object
      id: SessionOperationMetadata
      description: Metadata describing the Session operation.
      properties:
        labels:
          description: Labels associated with the operation.
          type: object
          additionalProperties:
            type: string
        description:
          type: string
          description: Short description of the operation.
        warnings:
          description: Warnings encountered during operation execution.
          items:
            type: string
          type: array
        operationType:
          enum:
            - SESSION_OPERATION_TYPE_UNSPECIFIED
            - CREATE
            - TERMINATE
            - DELETE
          type: string
          enumDescriptions:
            - Session operation type is unknown.
            - Create Session operation type.
            - Terminate Session operation type.
            - Delete Session operation type.
          description: The operation type.
        session:
          description: Name of the session for the operation.
          type: string
        createTime:
          format: google-datetime
          type: string
          description: The time when the operation was created.
        doneTime:
          type: string
          description: The time when the operation was finished.
          format: google-datetime
        sessionUuid:
          type: string
          description: Session UUID for the operation.
    GetPolicyOptions:
      id: GetPolicyOptions
      description: Encapsulates settings provided to GetIamPolicy.
      type: object
      properties:
        requestedPolicyVersion:
          type: integer
          description: >-
            Optional. The maximum policy version that will be used to format the
            policy.Valid values are 0, 1, and 3. Requests specifying an invalid
            value will be rejected.Requests for policies with any conditional
            role bindings must specify version 3. Policies with no conditional
            role bindings may specify any valid value or leave the field
            unset.The policy in the response might use the policy version that
            you specified, or it might use a lower policy version. For example,
            if you specify version 3, but the policy has no conditional role
            bindings, the response uses version 1.To learn which resources
            support conditions in their IAM policies, see the IAM documentation
            (https://cloud.google.com/iam/help/conditions/resource-policies).
          format: int32
    LifecycleConfig:
      properties:
        idleDeleteTtl:
          format: google-duration
          type: string
          description: >-
            Optional. The duration to keep the cluster alive while idling (when
            no jobs are running). Passing this threshold will cause the cluster
            to be deleted. Minimum value is 5 minutes; maximum value is 14 days
            (see JSON representation of Duration
            (https://developers.google.com/protocol-buffers/docs/proto3#json)).
        autoStopTtl:
          format: google-duration
          type: string
          description: >-
            Optional. The lifetime duration of the cluster. The cluster will be
            auto-stopped at the end of this period, calculated from the time of
            submission of the create or update cluster request. Minimum value is
            10 minutes; maximum value is 14 days (see JSON representation of
            Duration
            (https://developers.google.com/protocol-buffers/docs/proto3#json)).
        autoStopTime:
          description: >-
            Optional. The time when cluster will be auto-stopped (see JSON
            representation of Timestamp
            (https://developers.google.com/protocol-buffers/docs/proto3#json)).
          type: string
          format: google-datetime
        autoDeleteTtl:
          type: string
          description: >-
            Optional. The lifetime duration of cluster. The cluster will be
            auto-deleted at the end of this period. Minimum value is 10 minutes;
            maximum value is 14 days (see JSON representation of Duration
            (https://developers.google.com/protocol-buffers/docs/proto3#json)).
          format: google-duration
        autoDeleteTime:
          description: >-
            Optional. The time when cluster will be auto-deleted (see JSON
            representation of Timestamp
            (https://developers.google.com/protocol-buffers/docs/proto3#json)).
          type: string
          format: google-datetime
        idleStopTtl:
          type: string
          description: >-
            Optional. The duration to keep the cluster started while idling
            (when no jobs are running). Passing this threshold will cause the
            cluster to be stopped. Minimum value is 5 minutes; maximum value is
            14 days (see JSON representation of Duration
            (https://developers.google.com/protocol-buffers/docs/proto3#json)).
          format: google-duration
        idleStartTime:
          readOnly: true
          type: string
          description: >-
            Output only. The time when cluster became idle (most recent job
            finished) and became eligible for deletion due to idleness (see JSON
            representation of Timestamp
            (https://developers.google.com/protocol-buffers/docs/proto3#json)).
          format: google-datetime
      description: Specifies the cluster auto-delete schedule configuration.
      type: object
      id: LifecycleConfig
    JobsSummary:
      type: object
      description: Data related to Jobs page summary
      id: JobsSummary
      properties:
        attempts:
          description: Attempts info
          items:
            $ref: '#/components/schemas/ApplicationAttemptInfo'
          type: array
        failedJobs:
          type: integer
          description: Number of failed jobs
          format: int32
        applicationId:
          description: Spark Application Id
          type: string
        schedulingMode:
          description: Spark Scheduling mode
          type: string
        completedJobs:
          format: int32
          type: integer
          description: Number of completed jobs
        activeJobs:
          format: int32
          description: Number of active jobs
          type: integer
    ApplicationAttemptInfo:
      properties:
        startTime:
          type: string
          format: google-datetime
        sparkUser:
          type: string
        durationMillis:
          type: string
          format: int64
        lastUpdated:
          type: string
          format: google-datetime
        completed:
          type: boolean
        attemptId:
          type: string
        endTime:
          type: string
          format: google-datetime
        appSparkVersion:
          type: string
      description: Specific attempt of an application.
      type: object
      id: ApplicationAttemptInfo
    NodePool:
      id: NodePool
      type: object
      properties:
        instanceNames:
          items:
            type: string
          type: array
          description: >-
            Name of instances to be repaired. These instances must belong to
            specified node pool.
        repairAction:
          enum:
            - REPAIR_ACTION_UNSPECIFIED
            - DELETE
          type: string
          description: >-
            Required. Repair action to take on specified resources of the node
            pool.
          enumDescriptions:
            - No action will be taken by default.
            - delete the specified list of nodes.
        id:
          type: string
          description: >-
            Required. A unique id of the node pool. Primary and Secondary
            workers can be specified using special reserved ids
            PRIMARY_WORKER_POOL and SECONDARY_WORKER_POOL respectively. Aux node
            pools can be referenced using corresponding pool id.
      description: indicating a list of workers of same type
    TaskQuantileMetrics:
      id: TaskQuantileMetrics
      type: object
      properties:
        resultSize:
          $ref: '#/components/schemas/Quantiles'
        diskBytesSpilled:
          $ref: '#/components/schemas/Quantiles'
        peakExecutionMemoryBytes:
          $ref: '#/components/schemas/Quantiles'
        outputMetrics:
          $ref: '#/components/schemas/OutputQuantileMetrics'
        memoryBytesSpilled:
          $ref: '#/components/schemas/Quantiles'
        schedulerDelayMillis:
          $ref: '#/components/schemas/Quantiles'
        executorDeserializeTimeMillis:
          $ref: '#/components/schemas/Quantiles'
        shuffleWriteMetrics:
          $ref: '#/components/schemas/ShuffleWriteQuantileMetrics'
        executorDeserializeCpuTimeNanos:
          $ref: '#/components/schemas/Quantiles'
        inputMetrics:
          $ref: '#/components/schemas/InputQuantileMetrics'
        executorRunTimeMillis:
          $ref: '#/components/schemas/Quantiles'
        executorCpuTimeNanos:
          $ref: '#/components/schemas/Quantiles'
        gettingResultTimeMillis:
          $ref: '#/components/schemas/Quantiles'
        resultSerializationTimeMillis:
          $ref: '#/components/schemas/Quantiles'
        durationMillis:
          $ref: '#/components/schemas/Quantiles'
        jvmGcTimeMillis:
          $ref: '#/components/schemas/Quantiles'
        shuffleReadMetrics:
          $ref: '#/components/schemas/ShuffleReadQuantileMetrics'
    StopClusterRequest:
      id: StopClusterRequest
      type: object
      properties:
        clusterUuid:
          type: string
          description: >-
            Optional. Specifying the cluster_uuid means the RPC will fail (with
            error NOT_FOUND) if a cluster with the specified UUID does not
            exist.
        requestId:
          description: >-
            Optional. A unique ID used to identify the request. If the server
            receives two StopClusterRequest
            (https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.StopClusterRequest)s
            with the same id, then the second request will be ignored and the
            first google.longrunning.Operation created and stored in the backend
            is returned.Recommendation: Set this value to a UUID
            (https://en.wikipedia.org/wiki/Universally_unique_identifier).The ID
            must contain only letters (a-z, A-Z), numbers (0-9), underscores
            (_), and hyphens (-). The maximum length is 40 characters.
          type: string
      description: A request to stop a cluster.
    NodeGroupAffinity:
      type: object
      properties:
        nodeGroupUri:
          type: string
          description: >-
            Required. The URI of a sole-tenant node group resource
            (https://cloud.google.com/compute/docs/reference/rest/v1/nodeGroups)
            that the cluster will be created on.A full URL, partial URI, or node
            group name are valid. Examples:
            https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/nodeGroups/node-group-1
            projects/[project_id]/zones/[zone]/nodeGroups/node-group-1
            node-group-1
      description: >-
        Node Group Affinity for clusters using sole-tenant node groups. The
        Dataproc NodeGroupAffinity resource is not related to the Dataproc
        NodeGroup resource.
      id: NodeGroupAffinity
    InstanceGroupAutoscalingPolicyConfig:
      description: >-
        Configuration for the size bounds of an instance group, including its
        proportional size to other groups.
      properties:
        weight:
          format: int32
          description: >-
            Optional. Weight for the instance group, which is used to determine
            the fraction of total workers in the cluster from this instance
            group. For example, if primary workers have weight 2, and secondary
            workers have weight 1, the cluster will have approximately 2 primary
            workers for each secondary worker.The cluster may not reach the
            specified balance if constrained by min/max bounds or other
            autoscaling settings. For example, if max_instances for secondary
            workers is 0, then only primary workers will be added. The cluster
            can also be out of balance when created.If weight is not set on any
            instance group, the cluster will default to equal weight for all
            groups: the cluster will attempt to maintain an equal number of
            workers in each group within the configured size bounds for each
            group. If weight is set for one group only, the cluster will default
            to zero weight on the unset group. For example if weight is set only
            on primary workers, the cluster will use primary workers only and no
            secondary workers.
          type: integer
        minInstances:
          type: integer
          format: int32
          description: >-
            Optional. Minimum number of instances for this group.Primary workers
            - Bounds: 2, max_instances. Default: 2. Secondary workers - Bounds:
            0, max_instances. Default: 0.
        maxInstances:
          type: integer
          description: >-
            Required. Maximum number of instances for this group. Required for
            primary workers. Note that by default, clusters will not use
            secondary workers. Required for secondary workers if the minimum
            secondary instances is set.Primary workers - Bounds: [min_instances,
            ). Secondary workers - Bounds: [min_instances, ). Default: 0.
          format: int32
      type: object
      id: InstanceGroupAutoscalingPolicyConfig
    SparkApplication:
      id: SparkApplication
      properties:
        name:
          type: string
          description: Identifier. Name of the spark application
        application:
          description: Output only. High level information corresponding to an application.
          $ref: '#/components/schemas/ApplicationInfo'
          readOnly: true
      description: A summary of Spark Application
      type: object
    SetIamPolicyRequest:
      properties:
        policy:
          description: >-
            REQUIRED: The complete policy to be applied to the resource. The
            size of the policy is limited to a few 10s of KB. An empty policy is
            a valid policy but certain Google Cloud services (such as Projects)
            might reject them.
          $ref: '#/components/schemas/Policy'
      description: Request message for SetIamPolicy method.
      type: object
      id: SetIamPolicyRequest
    AccessSessionSparkApplicationStageRddOperationGraphResponse:
      id: AccessSessionSparkApplicationStageRddOperationGraphResponse
      description: >-
        RDD operation graph for a Spark Application Stage limited to maximum
        10000 clusters.
      type: object
      properties:
        rddOperationGraph:
          $ref: '#/components/schemas/RddOperationGraph'
          description: RDD operation graph for a Spark Application Stage.
    ApplicationInfo:
      properties:
        coresPerExecutor:
          type: integer
          format: int32
        maxCores:
          type: integer
          format: int32
        memoryPerExecutorMb:
          type: integer
          format: int32
        applicationId:
          type: string
        coresGranted:
          type: integer
          format: int32
        attempts:
          items:
            $ref: '#/components/schemas/ApplicationAttemptInfo'
          type: array
        quantileDataStatus:
          enum:
            - QUANTILE_DATA_STATUS_UNSPECIFIED
            - QUANTILE_DATA_STATUS_COMPLETED
            - QUANTILE_DATA_STATUS_FAILED
          type: string
          enumDescriptions:
            - ''
            - ''
            - ''
        name:
          type: string
        applicationContextIngestionStatus:
          enumDescriptions:
            - ''
            - ''
          enum:
            - APPLICATION_CONTEXT_INGESTION_STATUS_UNSPECIFIED
            - APPLICATION_CONTEXT_INGESTION_STATUS_COMPLETED
          type: string
      id: ApplicationInfo
      type: object
      description: High level information corresponding to an application.
    AccessSparkApplicationSqlQueryResponse:
      id: AccessSparkApplicationSqlQueryResponse
      description: Details of a query for a Spark Application
      type: object
      properties:
        executionData:
          $ref: '#/components/schemas/SqlExecutionUiData'
          description: SQL Execution Data
    ShufflePushReadQuantileMetrics:
      id: ShufflePushReadQuantileMetrics
      properties:
        corruptMergedBlockChunks:
          $ref: '#/components/schemas/Quantiles'
        localMergedBytesRead:
          $ref: '#/components/schemas/Quantiles'
        mergedFetchFallbackCount:
          $ref: '#/components/schemas/Quantiles'
        remoteMergedBlocksFetched:
          $ref: '#/components/schemas/Quantiles'
        remoteMergedChunksFetched:
          $ref: '#/components/schemas/Quantiles'
        localMergedBlocksFetched:
          $ref: '#/components/schemas/Quantiles'
        remoteMergedBytesRead:
          $ref: '#/components/schemas/Quantiles'
        remoteMergedReqsDuration:
          $ref: '#/components/schemas/Quantiles'
        localMergedChunksFetched:
          $ref: '#/components/schemas/Quantiles'
      type: object
    NodeGroup:
      id: NodeGroup
      type: object
      properties:
        roles:
          description: Required. Node group roles.
          type: array
          items:
            enumDescriptions:
              - Required unspecified role.
              - Job drivers run on the node pool.
            enum:
              - ROLE_UNSPECIFIED
              - DRIVER
            type: string
        labels:
          description: >-
            Optional. Node group labels. Label keys must consist of from 1 to 63
            characters and conform to RFC 1035
            (https://www.ietf.org/rfc/rfc1035.txt). Label values can be empty.
            If specified, they must consist of from 1 to 63 characters and
            conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). The node
            group must have no more than 32 labels.
          type: object
          additionalProperties:
            type: string
        name:
          description: The Node group resource name (https://aip.dev/122).
          type: string
        nodeGroupConfig:
          description: Optional. The node group instance group configuration.
          $ref: '#/components/schemas/InstanceGroupConfig'
      description: >-
        Dataproc Node Group. The Dataproc NodeGroup resource is not related to
        the Dataproc NodeGroupAffinity resource.
    RepairClusterRequest:
      properties:
        gracefulDecommissionTimeout:
          format: google-duration
          type: string
          description: >-
            Optional. Timeout for graceful YARN decommissioning. Graceful
            decommissioning facilitates the removal of cluster nodes without
            interrupting jobs in progress. The timeout specifies the amount of
            time to wait for jobs finish before forcefully removing nodes. The
            default timeout is 0 for forceful decommissioning, and the maximum
            timeout period is 1 day. (see JSON MappingDuration
            (https://developers.google.com/protocol-buffers/docs/proto3#json)).graceful_decommission_timeout
            is supported in Dataproc image versions 1.2+.
        clusterUuid:
          type: string
          description: >-
            Optional. Specifying the cluster_uuid means the RPC will fail (with
            error NOT_FOUND) if a cluster with the specified UUID does not
            exist.
        nodePools:
          type: array
          items:
            $ref: '#/components/schemas/NodePool'
          description: >-
            Optional. Node pools and corresponding repair action to be taken.
            All node pools should be unique in this request. i.e. Multiple
            entries for the same node pool id are not allowed.
        parentOperationId:
          type: string
          description: >-
            Optional. operation id of the parent operation sending the repair
            request
        dataprocSuperUser:
          type: boolean
          description: >-
            Optional. Whether the request is submitted by Dataproc super user.
            If true, IAM will check 'dataproc.clusters.repair' permission
            instead of 'dataproc.clusters.update' permission. This is to give
            Dataproc superuser the ability to repair clusters without granting
            the overly broad update permission.
        cluster:
          description: Optional. Cluster to be repaired
          $ref: '#/components/schemas/ClusterToRepair'
        requestId:
          type: string
          description: >-
            Optional. A unique ID used to identify the request. If the server
            receives two RepairClusterRequests with the same ID, the second
            request is ignored, and the first google.longrunning.Operation
            created and stored in the backend is returned.Recommendation: Set
            this value to a UUID
            (https://en.wikipedia.org/wiki/Universally_unique_identifier).The ID
            must contain only letters (a-z, A-Z), numbers (0-9), underscores
            (_), and hyphens (-). The maximum length is 40 characters.
      type: object
      description: A request to repair a cluster.
      id: RepairClusterRequest
    FlinkJob:
      id: FlinkJob
      properties:
        properties:
          type: object
          additionalProperties:
            type: string
          description: >-
            Optional. A mapping of property names to values, used to configure
            Flink. Properties that conflict with values set by the Dataproc API
            might be overwritten. Can include properties set in
            /etc/flink/conf/flink-defaults.conf and classes in user code.
        jarFileUris:
          items:
            type: string
          description: >-
            Optional. HCFS URIs of jar files to add to the CLASSPATHs of the
            Flink driver and tasks.
          type: array
        savepointUri:
          description: >-
            Optional. HCFS URI of the savepoint, which contains the last saved
            progress for starting the current job.
          type: string
        args:
          items:
            type: string
          type: array
          description: >-
            Optional. The arguments to pass to the driver. Do not include
            arguments, such as --conf, that can be set as job properties, since
            a collision might occur that causes an incorrect job submission.
        mainJarFileUri:
          type: string
          description: The HCFS URI of the jar file that contains the main class.
        mainClass:
          description: >-
            The name of the driver's main class. The jar file that contains the
            class must be in the default CLASSPATH or specified in jarFileUris.
          type: string
        loggingConfig:
          $ref: '#/components/schemas/LoggingConfig'
          description: Optional. The runtime log config for job execution.
      type: object
      description: A Dataproc job for running Apache Flink applications on YARN.
    ValueValidation:
      properties:
        values:
          items:
            type: string
          description: Required. List of allowed values for the parameter.
          type: array
      description: Validation based on a list of allowed values.
      id: ValueValidation
      type: object
    ShuffleReadMetrics:
      type: object
      properties:
        remoteReqsDuration:
          type: string
          format: int64
        fetchWaitTimeMillis:
          type: string
          format: int64
        localBytesRead:
          type: string
          format: int64
        recordsRead:
          format: int64
          type: string
        localBlocksFetched:
          format: int64
          type: string
        remoteBytesReadToDisk:
          type: string
          format: int64
        remoteBytesRead:
          type: string
          format: int64
        remoteBlocksFetched:
          format: int64
          type: string
        shufflePushReadMetrics:
          $ref: '#/components/schemas/ShufflePushReadMetrics'
      description: Shuffle data read by the task.
      id: ShuffleReadMetrics
    SessionStateHistory:
      id: SessionStateHistory
      type: object
      description: Historical state information.
      properties:
        stateMessage:
          type: string
          readOnly: true
          description: >-
            Output only. Details about the state at this point in the session
            history.
        state:
          enum:
            - STATE_UNSPECIFIED
            - CREATING
            - ACTIVE
            - TERMINATING
            - TERMINATED
            - FAILED
          description: >-
            Output only. The state of the session at this point in the session
            history.
          enumDescriptions:
            - The session state is unknown.
            - The session is created prior to running.
            - The session is running.
            - The session is terminating.
            - The session is terminated successfully.
            - The session is no longer running due to an error.
          type: string
          readOnly: true
        stateStartTime:
          readOnly: true
          type: string
          description: Output only. The time when the session entered the historical state.
          format: google-datetime
    SparkRuntimeInfo:
      id: SparkRuntimeInfo
      properties:
        javaVersion:
          type: string
        javaHome:
          type: string
        scalaVersion:
          type: string
      type: object
    VirtualClusterConfig:
      properties:
        auxiliaryServicesConfig:
          description: Optional. Configuration of auxiliary services used by this cluster.
          $ref: '#/components/schemas/AuxiliaryServicesConfig'
        kubernetesClusterConfig:
          description: >-
            Required. The configuration for running the Dataproc cluster on
            Kubernetes.
          $ref: '#/components/schemas/KubernetesClusterConfig'
        stagingBucket:
          description: >-
            Optional. A Cloud Storage bucket used to stage job dependencies,
            config files, and job driver console output. If you do not specify a
            staging bucket, Cloud Dataproc will determine a Cloud Storage
            location (US, ASIA, or EU) for your cluster's staging bucket
            according to the Compute Engine zone where your cluster is deployed,
            and then create and manage this project-level, per-location bucket
            (see Dataproc staging and temp buckets
            (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)).
            This field requires a Cloud Storage bucket name, not a gs://... URI
            to a Cloud Storage bucket.
          type: string
      id: VirtualClusterConfig
      description: >-
        The Dataproc cluster config for a cluster that does not directly control
        the underlying compute resources, such as a Dataproc-on-GKE cluster
        (https://cloud.google.com/dataproc/docs/guides/dpgke/dataproc-gke-overview).
      type: object
    BasicYarnAutoscalingConfig:
      type: object
      properties:
        scaleUpFactor:
          type: number
          format: double
          description: >-
            Required. Fraction of average YARN pending memory in the last
            cooldown period for which to add workers. A scale-up factor of 1.0
            will result in scaling up so that there is no pending memory
            remaining after the update (more aggressive scaling). A scale-up
            factor closer to 0 will result in a smaller magnitude of scaling up
            (less aggressive scaling). See How autoscaling works
            (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works)
            for more information.Bounds: 0.0, 1.0.
        scaleDownMinWorkerFraction:
          description: >-
            Optional. Minimum scale-down threshold as a fraction of total
            cluster size before scaling occurs. For example, in a 20-worker
            cluster, a threshold of 0.1 means the autoscaler must recommend at
            least a 2 worker scale-down for the cluster to scale. A threshold of
            0 means the autoscaler will scale down on any recommended
            change.Bounds: 0.0, 1.0. Default: 0.0.
          type: number
          format: double
        gracefulDecommissionTimeout:
          format: google-duration
          type: string
          description: >-
            Required. Timeout for YARN graceful decommissioning of Node
            Managers. Specifies the duration to wait for jobs to complete before
            forcefully removing workers (and potentially interrupting jobs).
            Only applicable to downscaling operations.Bounds: 0s, 1d.
        scaleUpMinWorkerFraction:
          type: number
          format: double
          description: >-
            Optional. Minimum scale-up threshold as a fraction of total cluster
            size before scaling occurs. For example, in a 20-worker cluster, a
            threshold of 0.1 means the autoscaler must recommend at least a
            2-worker scale-up for the cluster to scale. A threshold of 0 means
            the autoscaler will scale up on any recommended change.Bounds: 0.0,
            1.0. Default: 0.0.
        scaleDownFactor:
          format: double
          type: number
          description: >-
            Required. Fraction of average YARN pending memory in the last
            cooldown period for which to remove workers. A scale-down factor of
            1 will result in scaling down so that there is no available memory
            remaining after the update (more aggressive scaling). A scale-down
            factor of 0 disables removing workers, which can be beneficial for
            autoscaling a single job. See How autoscaling works
            (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works)
            for more information.Bounds: 0.0, 1.0.
      id: BasicYarnAutoscalingConfig
      description: Basic autoscaling configurations for YARN.
    ConfidentialInstanceConfig:
      id: ConfidentialInstanceConfig
      description: >-
        Confidential Instance Config for clusters using Confidential VMs
        (https://cloud.google.com/compute/confidential-vm/docs)
      type: object
      properties:
        enableConfidentialCompute:
          description: >-
            Optional. Defines whether the instance should have confidential
            compute enabled.
          type: boolean
    HiveJob:
      description: >-
        A Dataproc job for running Apache Hive (https://hive.apache.org/)
        queries on YARN.
      properties:
        scriptVariables:
          additionalProperties:
            type: string
          description: >-
            Optional. Mapping of query variable names to values (equivalent to
            the Hive command: SET name="value";).
          type: object
        queryFileUri:
          type: string
          description: The HCFS URI of the script that contains Hive queries.
        properties:
          type: object
          description: >-
            Optional. A mapping of property names and values, used to configure
            Hive. Properties that conflict with values set by the Dataproc API
            might be overwritten. Can include properties set in
            /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and
            classes in user code.
          additionalProperties:
            type: string
        continueOnFailure:
          description: >-
            Optional. Whether to continue executing queries if a query fails.
            The default value is false. Setting to true can be useful when
            executing independent parallel queries.
          type: boolean
        queryList:
          $ref: '#/components/schemas/QueryList'
          description: A list of queries.
        jarFileUris:
          description: >-
            Optional. HCFS URIs of jar files to add to the CLASSPATH of the Hive
            server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and
            UDFs.
          type: array
          items:
            type: string
      type: object
      id: HiveJob
    InstanceSelectionResult:
      properties:
        vmCount:
          readOnly: true
          type: integer
          description: Output only. Number of VM provisioned with the machine_type.
          format: int32
        machineType:
          description: Output only. Full machine-type names, e.g. "n1-standard-16".
          readOnly: true
          type: string
      id: InstanceSelectionResult
      type: object
      description: >-
        Defines a mapping from machine types to the number of VMs that are
        created with each machine type.
    ParameterValidation:
      description: Configuration for parameter validation.
      type: object
      id: ParameterValidation
      properties:
        regex:
          description: Validation based on regular expressions.
          $ref: '#/components/schemas/RegexValidation'
        values:
          description: Validation based on a list of allowed values.
          $ref: '#/components/schemas/ValueValidation'
    GkeNodePoolConfig:
      description: >-
        The configuration of a GKE node pool used by a Dataproc-on-GKE cluster
        (https://cloud.google.com/dataproc/docs/concepts/jobs/dataproc-gke#create-a-dataproc-on-gke-cluster).
      id: GkeNodePoolConfig
      properties:
        locations:
          description: >-
            Optional. The list of Compute Engine zones
            (https://cloud.google.com/compute/docs/zones#available) where node
            pool nodes associated with a Dataproc on GKE virtual cluster will be
            located.Note: All node pools associated with a virtual cluster must
            be located in the same region as the virtual cluster, and they must
            be located in the same zone within that region.If a location is not
            specified during node pool creation, Dataproc on GKE will choose the
            zone.
          items:
            type: string
          type: array
        config:
          $ref: '#/components/schemas/GkeNodeConfig'
          description: Optional. The node pool configuration.
        autoscaling:
          description: >-
            Optional. The autoscaler configuration for this node pool. The
            autoscaler is enabled only when a valid configuration is present.
          $ref: '#/components/schemas/GkeNodePoolAutoscalingConfig'
      type: object
    AcceleratorConfig:
      description: >-
        Specifies the type and number of accelerator cards attached to the
        instances of an instance. See GPUs on Compute Engine
        (https://cloud.google.com/compute/docs/gpus/).
      id: AcceleratorConfig
      properties:
        acceleratorCount:
          format: int32
          type: integer
          description: >-
            The number of the accelerator cards of this type exposed to this
            instance.
        acceleratorTypeUri:
          description: >-
            Full URL, partial URI, or short name of the accelerator type
            resource to expose to this instance. See Compute Engine
            AcceleratorTypes
            (https://cloud.google.com/compute/docs/reference/v1/acceleratorTypes).Examples:
            https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/acceleratorTypes/nvidia-tesla-t4
            projects/[project_id]/zones/[zone]/acceleratorTypes/nvidia-tesla-t4
            nvidia-tesla-t4Auto Zone Exception: If you are using the Dataproc
            Auto Zone Placement
            (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement)
            feature, you must use the short name of the accelerator type
            resource, for example, nvidia-tesla-t4.
          type: string
      type: object
    ResizeNodeGroupRequest:
      properties:
        size:
          type: integer
          description: >-
            Required. The number of running instances for the node group to
            maintain. The group adds or removes instances to maintain the number
            of instances specified by this parameter.
          format: int32
        requestId:
          description: >-
            Optional. A unique ID used to identify the request. If the server
            receives two ResizeNodeGroupRequest
            (https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.ResizeNodeGroupRequests)
            with the same ID, the second request is ignored and the first
            google.longrunning.Operation created and stored in the backend is
            returned.Recommendation: Set this value to a UUID
            (https://en.wikipedia.org/wiki/Universally_unique_identifier).The ID
            must contain only letters (a-z, A-Z), numbers (0-9), underscores
            (_), and hyphens (-). The maximum length is 40 characters.
          type: string
        gracefulDecommissionTimeout:
          type: string
          description: >-
            Optional. Timeout for graceful YARN decommissioning. Graceful
            decommissioning
            (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/scaling-clusters#graceful_decommissioning)
            allows the removal of nodes from the Compute Engine node group
            without interrupting jobs in progress. This timeout specifies how
            long to wait for jobs in progress to finish before forcefully
            removing nodes (and potentially interrupting jobs). Default timeout
            is 0 (for forceful decommission), and the maximum allowed timeout is
            1 day. (see JSON representation of Duration
            (https://developers.google.com/protocol-buffers/docs/proto3#json)).Only
            supported on Dataproc image versions 1.2 and higher.
          format: google-duration
        parentOperationId:
          type: string
          description: >-
            Optional. operation id of the parent operation sending the resize
            request
      id: ResizeNodeGroupRequest
      description: A request to resize a node group.
      type: object
    ShuffleWriteQuantileMetrics:
      id: ShuffleWriteQuantileMetrics
      type: object
      properties:
        writeRecords:
          $ref: '#/components/schemas/Quantiles'
        writeTimeNanos:
          $ref: '#/components/schemas/Quantiles'
        writeBytes:
          $ref: '#/components/schemas/Quantiles'
    ListBatchesResponse:
      properties:
        nextPageToken:
          type: string
          description: >-
            A token, which can be sent as page_token to retrieve the next page.
            If this field is omitted, there are no subsequent pages.
        batches:
          type: array
          readOnly: true
          description: Output only. The batches from the specified collection.
          items:
            $ref: '#/components/schemas/Batch'
        unreachable:
          readOnly: true
          type: array
          description: >-
            Output only. List of Batches that could not be included in the
            response. Attempting to get one of these resources may indicate why
            it was not included in the list response.
          items:
            type: string
      description: A list of batch workloads.
      id: ListBatchesResponse
      type: object
    Empty:
      type: object
      id: Empty
      properties: {}
      description: >-
        A generic empty message that you can re-use to avoid defining duplicated
        empty messages in your APIs. A typical example is to use it as the
        request or the response type of an API method. For instance: service Foo
        { rpc Bar(google.protobuf.Empty) returns (google.protobuf.Empty); } 
    StageShuffleWriteMetrics:
      id: StageShuffleWriteMetrics
      properties:
        recordsWritten:
          type: string
          format: int64
        writeTimeNanos:
          type: string
          format: int64
        bytesWritten:
          type: string
          format: int64
      description: Shuffle data written for the stage.
      type: object
    SessionTemplate:
      id: SessionTemplate
      type: object
      description: A representation of a session template.
      properties:
        updateTime:
          format: google-datetime
          readOnly: true
          type: string
          description: Output only. The time the template was last updated.
        creator:
          readOnly: true
          description: Output only. The email address of the user who created the template.
          type: string
        environmentConfig:
          description: Optional. Environment configuration for session execution.
          $ref: '#/components/schemas/EnvironmentConfig'
        description:
          type: string
          description: Optional. Brief description of the template.
        createTime:
          type: string
          readOnly: true
          description: Output only. The time when the template was created.
          format: google-datetime
        labels:
          description: >-
            Optional. Labels to associate with sessions created using this
            template. Label keys must contain 1 to 63 characters, and must
            conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). Label
            values can be empty, but, if present, must contain 1 to 63
            characters and conform to RFC 1035
            (https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can
            be associated with a session.
          additionalProperties:
            type: string
          type: object
        jupyterSession:
          description: Optional. Jupyter session config.
          $ref: '#/components/schemas/JupyterConfig'
        name:
          type: string
          description: Required. Identifier. The resource name of the session template.
        uuid:
          description: >-
            Output only. A session template UUID (Unique Universal Identifier).
            The service generates this value when it creates the session
            template.
          readOnly: true
          type: string
        runtimeConfig:
          $ref: '#/components/schemas/RuntimeConfig'
          description: Optional. Runtime configuration for session execution.
        sparkConnectSession:
          $ref: '#/components/schemas/SparkConnectConfig'
          description: Optional. Spark connect session config.
    SearchSessionSparkApplicationStageAttemptsResponse:
      type: object
      properties:
        sparkApplicationStageAttempts:
          readOnly: true
          type: array
          description: Output only. Data corresponding to a stage attempts
          items:
            $ref: '#/components/schemas/StageData'
        nextPageToken:
          type: string
          description: >-
            This token is included in the response if there are more results to
            fetch. To fetch additional results, provide this value as the
            page_token in a subsequent
            SearchSessionSparkApplicationStageAttemptsRequest.
      description: A list of Stage Attempts for a Stage of a Spark Application.
      id: SearchSessionSparkApplicationStageAttemptsResponse
    ListJobsResponse:
      properties:
        unreachable:
          readOnly: true
          description: >-
            Output only. List of jobs with kms_key-encrypted parameters that
            could not be decrypted. A response to a jobs.get request may
            indicate the reason for the decryption failure for a specific job.
          type: array
          items:
            type: string
        jobs:
          items:
            $ref: '#/components/schemas/Job'
          description: Output only. Jobs list.
          type: array
          readOnly: true
        nextPageToken:
          description: >-
            Optional. This token is included in the response if there are more
            results to fetch. To fetch additional results, provide this value as
            the page_token in a subsequent ListJobsRequest.
          type: string
      id: ListJobsResponse
      type: object
      description: A list of jobs in a project.
    SearchSparkApplicationStagesResponse:
      properties:
        nextPageToken:
          description: >-
            This token is included in the response if there are more results to
            fetch. To fetch additional results, provide this value as the
            page_token in a subsequent SearchSparkApplicationStages.
          type: string
        sparkApplicationStages:
          type: array
          description: Output only. Data corresponding to a stage.
          readOnly: true
          items:
            $ref: '#/components/schemas/StageData'
      id: SearchSparkApplicationStagesResponse
      type: object
      description: A list of stages associated with a Spark Application.
    Cluster:
      description: >-
        Describes the identifying information, config, and status of a Dataproc
        cluster
      id: Cluster
      type: object
      properties:
        projectId:
          description: >-
            Required. The Google Cloud Platform project ID that the cluster
            belongs to.
          type: string
        clusterUuid:
          type: string
          description: >-
            Output only. A cluster UUID (Unique Universal Identifier). Dataproc
            generates this value when it creates the cluster.
          readOnly: true
        status:
          $ref: '#/components/schemas/ClusterStatus'
          description: Output only. Cluster status.
          readOnly: true
        config:
          $ref: '#/components/schemas/ClusterConfig'
          description: >-
            Optional. The cluster config for a cluster of Compute Engine
            Instances. Note that Dataproc may set default values, and values may
            change when clusters are updated.Exactly one of ClusterConfig or
            VirtualClusterConfig must be specified.
        clusterName:
          description: >-
            Required. The cluster name, which must be unique within a project.
            The name must start with a lowercase letter, and can contain up to
            51 lowercase letters, numbers, and hyphens. It cannot end with a
            hyphen. The name of a deleted cluster can be reused.
          type: string
        labels:
          description: >-
            Optional. The labels to associate with this cluster. Label keys must
            contain 1 to 63 characters, and must conform to RFC 1035
            (https://www.ietf.org/rfc/rfc1035.txt). Label values may be empty,
            but, if present, must contain 1 to 63 characters, and must conform
            to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). No more than 32
            labels can be associated with a cluster.
          additionalProperties:
            type: string
          type: object
        statusHistory:
          type: array
          description: Output only. The previous cluster status.
          items:
            $ref: '#/components/schemas/ClusterStatus'
          readOnly: true
        metrics:
          readOnly: true
          $ref: '#/components/schemas/ClusterMetrics'
          description: >-
            Output only. Contains cluster daemon metrics such as HDFS and YARN
            stats.Beta Feature: This report is available for testing purposes
            only. It may be changed before final release.
        virtualClusterConfig:
          $ref: '#/components/schemas/VirtualClusterConfig'
          description: >-
            Optional. The virtual cluster config is used when creating a
            Dataproc cluster that does not directly control the underlying
            compute resources, for example, when creating a Dataproc-on-GKE
            cluster
            (https://cloud.google.com/dataproc/docs/guides/dpgke/dataproc-gke-overview).
            Dataproc may set default values, and values may change when clusters
            are updated. Exactly one of config or virtual_cluster_config must be
            specified.
    FallbackReason:
      properties:
        fallbackReason:
          description: Optional. Fallback to Spark reason.
          type: string
        fallbackNode:
          type: string
          description: Optional. Fallback node information.
      description: Native SQL Execution Data
      type: object
      id: FallbackReason
    DiskConfig:
      description: >-
        Specifies the config of boot disk and attached disk options for a group
        of VM instances.
      id: DiskConfig
      properties:
        localSsdInterface:
          description: >-
            Optional. Interface type of local SSDs (default is "scsi"). Valid
            values: "scsi" (Small Computer System Interface), "nvme"
            (Non-Volatile Memory Express). See local SSD performance
            (https://cloud.google.com/compute/docs/disks/local-ssd#performance).
          type: string
        numLocalSsds:
          description: >-
            Optional. Number of attached SSDs, from 0 to 8 (default is 0). If
            SSDs are not attached, the boot disk is used to store runtime logs
            and HDFS
            (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data.
            If one or more SSDs are attached, this runtime bulk data is spread
            across them, and the boot disk contains only basic config and
            installed binaries.Note: Local SSD options may vary by machine type
            and number of vCPUs selected.
          format: int32
          type: integer
        bootDiskProvisionedIops:
          type: string
          description: >-
            Optional. Indicates how many IOPS to provision for the disk. This
            sets the number of I/O operations per second that the disk can
            handle. This field is supported only if boot_disk_type is
            hyperdisk-balanced.
          format: int64
        bootDiskProvisionedThroughput:
          format: int64
          description: >-
            Optional. Indicates how much throughput to provision for the disk.
            This sets the number of throughput mb per second that the disk can
            handle. Values must be greater than or equal to 1. This field is
            supported only if boot_disk_type is hyperdisk-balanced.
          type: string
        bootDiskSizeGb:
          format: int32
          description: Optional. Size in GB of the boot disk (default is 500GB).
          type: integer
        bootDiskType:
          type: string
          description: >-
            Optional. Type of the boot disk (default is "pd-standard"). Valid
            values: "pd-balanced" (Persistent Disk Balanced Solid State Drive),
            "pd-ssd" (Persistent Disk Solid State Drive), or "pd-standard"
            (Persistent Disk Hard Disk Drive). See Disk types
            (https://cloud.google.com/compute/docs/disks#disk-types).
      type: object
    TaskResourceRequest:
      properties:
        resourceName:
          type: string
        amount:
          type: number
          format: double
      description: Resources used per task created by the application.
      type: object
      id: TaskResourceRequest
    RddDataDistribution:
      description: Details about RDD usage.
      properties:
        memoryUsed:
          format: int64
          type: string
        offHeapMemoryUsed:
          type: string
          format: int64
        address:
          type: string
        onHeapMemoryRemaining:
          type: string
          format: int64
        memoryRemaining:
          type: string
          format: int64
        onHeapMemoryUsed:
          format: int64
          type: string
        diskUsed:
          format: int64
          type: string
        offHeapMemoryRemaining:
          format: int64
          type: string
      id: RddDataDistribution
      type: object
    AccessSessionSparkApplicationSqlQueryResponse:
      id: AccessSessionSparkApplicationSqlQueryResponse
      type: object
      properties:
        executionData:
          description: SQL Execution Data
          $ref: '#/components/schemas/SqlExecutionUiData'
      description: Details of a query for a Spark Application
    StreamBlockData:
      description: Stream Block Data.
      id: StreamBlockData
      type: object
      properties:
        useDisk:
          type: boolean
        diskSize:
          format: int64
          type: string
        deserialized:
          type: boolean
        memSize:
          format: int64
          type: string
        storageLevel:
          type: string
        useMemory:
          type: boolean
        executorId:
          type: string
        hostPort:
          type: string
        name:
          type: string
    RuntimeConfig:
      description: Runtime configuration for a workload.
      id: RuntimeConfig
      type: object
      properties:
        repositoryConfig:
          $ref: '#/components/schemas/RepositoryConfig'
          description: Optional. Dependency repository configuration.
        autotuningConfig:
          description: Optional. Autotuning configuration of the workload.
          $ref: '#/components/schemas/AutotuningConfig'
        version:
          description: Optional. Version of the batch runtime.
          type: string
        properties:
          additionalProperties:
            type: string
          description: >-
            Optional. A mapping of property names to values, which are used to
            configure workload execution.
          type: object
        containerImage:
          description: >-
            Optional. Optional custom container image for the job runtime
            environment. If not specified, a default container image will be
            used.
          type: string
        cohort:
          description: >-
            Optional. Cohort identifier. Identifies families of the workloads
            having the same shape, e.g. daily ETL jobs.
          type: string
    StageData:
      description: Data corresponding to a stage.
      id: StageData
      type: object
      properties:
        schedulingPool:
          type: string
        executorMetricsDistributions:
          $ref: '#/components/schemas/ExecutorMetricsDistributions'
        numFailedTasks:
          format: int32
          type: integer
        numActiveTasks:
          format: int32
          type: integer
        status:
          enumDescriptions:
            - ''
            - ''
            - ''
            - ''
            - ''
            - ''
          enum:
            - STAGE_STATUS_UNSPECIFIED
            - STAGE_STATUS_ACTIVE
            - STAGE_STATUS_COMPLETE
            - STAGE_STATUS_FAILED
            - STAGE_STATUS_PENDING
            - STAGE_STATUS_SKIPPED
          type: string
        name:
          type: string
        executorSummary:
          type: object
          additionalProperties:
            $ref: '#/components/schemas/ExecutorStageSummary'
        rddIds:
          items:
            format: int64
            type: string
          type: array
        peakExecutorMetrics:
          $ref: '#/components/schemas/ExecutorMetrics'
        stageId:
          type: string
          format: int64
        killedTasksSummary:
          type: object
          additionalProperties:
            format: int32
            type: integer
        accumulatorUpdates:
          type: array
          items:
            $ref: '#/components/schemas/AccumulableInfo'
        stageAttemptId:
          format: int32
          type: integer
        locality:
          additionalProperties:
            type: string
            format: int64
          type: object
        firstTaskLaunchedTime:
          format: google-datetime
          type: string
        numTasks:
          format: int32
          type: integer
        completionTime:
          type: string
          format: google-datetime
        numCompletedIndices:
          type: integer
          format: int32
        numKilledTasks:
          format: int32
          type: integer
        shuffleMergersCount:
          type: integer
          format: int32
        numCompleteTasks:
          type: integer
          format: int32
        jobIds:
          type: array
          items:
            format: int64
            type: string
        failureReason:
          type: string
        submissionTime:
          format: google-datetime
          type: string
        parentStageIds:
          items:
            type: string
            format: int64
          type: array
        taskQuantileMetrics:
          $ref: '#/components/schemas/TaskQuantileMetrics'
          description: >-
            Summary metrics fields. These are included in response only if
            present in summary_metrics_mask field in request
        details:
          type: string
        description:
          type: string
        isShufflePushEnabled:
          type: boolean
        stageMetrics:
          $ref: '#/components/schemas/StageMetrics'
        tasks:
          additionalProperties:
            $ref: '#/components/schemas/TaskData'
          type: object
        resourceProfileId:
          type: integer
          format: int32
        speculationSummary:
          $ref: '#/components/schemas/SpeculationStageSummary'
    ClusterSelector:
      type: object
      description: A selector that chooses target cluster for jobs based on metadata.
      properties:
        clusterLabels:
          additionalProperties:
            type: string
          type: object
          description: Required. The cluster labels. Cluster must have all labels to match.
        zone:
          description: >-
            Optional. The zone where workflow process executes. This parameter
            does not affect the selection of the cluster.If unspecified, the
            zone of the first cluster matching the selector is used.
          type: string
      id: ClusterSelector
    SearchSparkApplicationExecutorsResponse:
      description: List of Executors associated with a Spark Application.
      id: SearchSparkApplicationExecutorsResponse
      properties:
        nextPageToken:
          type: string
          description: >-
            This token is included in the response if there are more results to
            fetch. To fetch additional results, provide this value as the
            page_token in a subsequent
            SearchSparkApplicationExecutorsListRequest.
        sparkApplicationExecutors:
          description: Details about executors used by the application.
          items:
            $ref: '#/components/schemas/ExecutorSummary'
          type: array
      type: object
  parameters:
    oauth_token:
      description: OAuth 2.0 token for the current user.
      in: query
      name: oauth_token
      schema:
        type: string
    fields:
      description: Selector specifying which fields to include in a partial response.
      in: query
      name: fields
      schema:
        type: string
    alt:
      description: Data format for response.
      in: query
      name: alt
      schema:
        type: string
        enum:
          - json
          - media
          - proto
    _.xgafv:
      description: V1 error format.
      in: query
      name: $.xgafv
      schema:
        type: string
        enum:
          - '1'
          - '2'
    key:
      description: >-
        API key. Your API key identifies your project and provides you with API
        access, quota, and reports. Required unless you provide an OAuth 2.0
        token.
      in: query
      name: key
      schema:
        type: string
    uploadType:
      description: Legacy upload protocol for media (e.g. "media", "multipart").
      in: query
      name: uploadType
      schema:
        type: string
    upload_protocol:
      description: Upload protocol for media (e.g. "raw", "multipart").
      in: query
      name: upload_protocol
      schema:
        type: string
    quotaUser:
      description: >-
        Available to use for quota purposes for server-side applications. Can be
        any arbitrary string assigned to a user, but should not exceed 40
        characters.
      in: query
      name: quotaUser
      schema:
        type: string
    callback:
      description: JSONP
      in: query
      name: callback
      schema:
        type: string
    access_token:
      description: OAuth access token.
      in: query
      name: access_token
      schema:
        type: string
    prettyPrint:
      description: Returns response with indentations and line breaks.
      in: query
      name: prettyPrint
      schema:
        type: boolean
  x-stackQL-resources:
    workflow_templates:
      id: google.dataproc.workflow_templates
      name: workflow_templates
      title: Workflow_templates
      methods:
        projects_regions_workflow_templates_instantiate_inline:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1workflowTemplates:instantiateInline/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_workflow_templates_update:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1workflowTemplates~1{workflowTemplatesId}/put
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_workflow_templates_delete:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1workflowTemplates~1{workflowTemplatesId}/delete
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_workflow_templates_get:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1workflowTemplates~1{workflowTemplatesId}/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_workflow_templates_list:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1workflowTemplates/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
            objectKey: $.unreachable
        projects_regions_workflow_templates_create:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1workflowTemplates/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_workflow_templates_instantiate:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1workflowTemplates~1{workflowTemplatesId}:instantiate/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_workflow_templates_instantiate:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1workflowTemplates~1{workflowTemplatesId}:instantiate/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_workflow_templates_create:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1workflowTemplates/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_workflow_templates_list:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1workflowTemplates/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
            objectKey: $.unreachable
        projects_locations_workflow_templates_delete:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1workflowTemplates~1{workflowTemplatesId}/delete
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_workflow_templates_get:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1workflowTemplates~1{workflowTemplatesId}/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_workflow_templates_update:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1workflowTemplates~1{workflowTemplatesId}/put
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_workflow_templates_instantiate_inline:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1workflowTemplates:instantiateInline/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/workflow_templates/methods/projects_regions_workflow_templates_get
          - $ref: >-
              #/components/x-stackQL-resources/workflow_templates/methods/projects_locations_workflow_templates_get
          - $ref: >-
              #/components/x-stackQL-resources/workflow_templates/methods/projects_regions_workflow_templates_list
          - $ref: >-
              #/components/x-stackQL-resources/workflow_templates/methods/projects_locations_workflow_templates_list
        insert:
          - $ref: >-
              #/components/x-stackQL-resources/workflow_templates/methods/projects_regions_workflow_templates_create
          - $ref: >-
              #/components/x-stackQL-resources/workflow_templates/methods/projects_locations_workflow_templates_create
        update: []
        replace:
          - $ref: >-
              #/components/x-stackQL-resources/workflow_templates/methods/projects_regions_workflow_templates_update
          - $ref: >-
              #/components/x-stackQL-resources/workflow_templates/methods/projects_locations_workflow_templates_update
        delete:
          - $ref: >-
              #/components/x-stackQL-resources/workflow_templates/methods/projects_regions_workflow_templates_delete
          - $ref: >-
              #/components/x-stackQL-resources/workflow_templates/methods/projects_locations_workflow_templates_delete
    workflow_templates_iam_policies:
      id: google.dataproc.workflow_templates_iam_policies
      name: workflow_templates_iam_policies
      title: Workflow_templates_iam_policies
      methods:
        projects_regions_workflow_templates_test_iam_permissions:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1workflowTemplates~1{workflowTemplatesId}:testIamPermissions/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_workflow_templates_get_iam_policy:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1workflowTemplates~1{workflowTemplatesId}:getIamPolicy/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
            objectKey: $.bindings
        projects_regions_workflow_templates_set_iam_policy:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1workflowTemplates~1{workflowTemplatesId}:setIamPolicy/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_workflow_templates_set_iam_policy:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1workflowTemplates~1{workflowTemplatesId}:setIamPolicy/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_workflow_templates_get_iam_policy:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1workflowTemplates~1{workflowTemplatesId}:getIamPolicy/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
            objectKey: $.bindings
        projects_locations_workflow_templates_test_iam_permissions:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1workflowTemplates~1{workflowTemplatesId}:testIamPermissions/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/workflow_templates_iam_policies/methods/projects_regions_workflow_templates_get_iam_policy
          - $ref: >-
              #/components/x-stackQL-resources/workflow_templates_iam_policies/methods/projects_locations_workflow_templates_get_iam_policy
        insert: []
        update: []
        replace:
          - $ref: >-
              #/components/x-stackQL-resources/workflow_templates_iam_policies/methods/projects_regions_workflow_templates_set_iam_policy
          - $ref: >-
              #/components/x-stackQL-resources/workflow_templates_iam_policies/methods/projects_locations_workflow_templates_set_iam_policy
        delete: []
    jobs:
      id: google.dataproc.jobs
      name: jobs
      title: Jobs
      methods:
        projects_regions_jobs_cancel:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectId}~1regions~1{region}~1jobs~1{jobId}:cancel/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_jobs_submit:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectId}~1regions~1{region}~1jobs:submit/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_jobs_delete:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectId}~1regions~1{region}~1jobs~1{jobId}/delete
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_jobs_patch:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectId}~1regions~1{region}~1jobs~1{jobId}/patch
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_jobs_get:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectId}~1regions~1{region}~1jobs~1{jobId}/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_jobs_list:
          operation:
            $ref: '#/paths/~1v1~1projects~1{projectId}~1regions~1{region}~1jobs/get'
          response:
            mediaType: application/json
            openAPIDocKey: '200'
            objectKey: $.unreachable
        projects_regions_jobs_submit_as_operation:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectId}~1regions~1{region}~1jobs:submitAsOperation/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/jobs/methods/projects_regions_jobs_get
          - $ref: >-
              #/components/x-stackQL-resources/jobs/methods/projects_regions_jobs_list
        insert: []
        update:
          - $ref: >-
              #/components/x-stackQL-resources/jobs/methods/projects_regions_jobs_patch
        replace: []
        delete:
          - $ref: >-
              #/components/x-stackQL-resources/jobs/methods/projects_regions_jobs_delete
    jobs_iam_policies:
      id: google.dataproc.jobs_iam_policies
      name: jobs_iam_policies
      title: Jobs_iam_policies
      methods:
        projects_regions_jobs_test_iam_permissions:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1jobs~1{jobsId}:testIamPermissions/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_jobs_set_iam_policy:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1jobs~1{jobsId}:setIamPolicy/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_jobs_get_iam_policy:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1jobs~1{jobsId}:getIamPolicy/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
            objectKey: $.bindings
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/jobs_iam_policies/methods/projects_regions_jobs_get_iam_policy
        insert: []
        update: []
        replace:
          - $ref: >-
              #/components/x-stackQL-resources/jobs_iam_policies/methods/projects_regions_jobs_set_iam_policy
        delete: []
    autoscaling_policies:
      id: google.dataproc.autoscaling_policies
      name: autoscaling_policies
      title: Autoscaling_policies
      methods:
        projects_regions_autoscaling_policies_update:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1autoscalingPolicies~1{autoscalingPoliciesId}/put
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_autoscaling_policies_get:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1autoscalingPolicies~1{autoscalingPoliciesId}/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_autoscaling_policies_delete:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1autoscalingPolicies~1{autoscalingPoliciesId}/delete
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_autoscaling_policies_list:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1autoscalingPolicies/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
            objectKey: $.policies
        projects_regions_autoscaling_policies_create:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1autoscalingPolicies/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_autoscaling_policies_create:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1autoscalingPolicies/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_autoscaling_policies_list:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1autoscalingPolicies/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
            objectKey: $.policies
        projects_locations_autoscaling_policies_delete:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1autoscalingPolicies~1{autoscalingPoliciesId}/delete
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_autoscaling_policies_update:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1autoscalingPolicies~1{autoscalingPoliciesId}/put
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_autoscaling_policies_get:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1autoscalingPolicies~1{autoscalingPoliciesId}/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/autoscaling_policies/methods/projects_regions_autoscaling_policies_get
          - $ref: >-
              #/components/x-stackQL-resources/autoscaling_policies/methods/projects_locations_autoscaling_policies_get
          - $ref: >-
              #/components/x-stackQL-resources/autoscaling_policies/methods/projects_regions_autoscaling_policies_list
          - $ref: >-
              #/components/x-stackQL-resources/autoscaling_policies/methods/projects_locations_autoscaling_policies_list
        insert:
          - $ref: >-
              #/components/x-stackQL-resources/autoscaling_policies/methods/projects_regions_autoscaling_policies_create
          - $ref: >-
              #/components/x-stackQL-resources/autoscaling_policies/methods/projects_locations_autoscaling_policies_create
        update: []
        replace:
          - $ref: >-
              #/components/x-stackQL-resources/autoscaling_policies/methods/projects_regions_autoscaling_policies_update
          - $ref: >-
              #/components/x-stackQL-resources/autoscaling_policies/methods/projects_locations_autoscaling_policies_update
        delete:
          - $ref: >-
              #/components/x-stackQL-resources/autoscaling_policies/methods/projects_regions_autoscaling_policies_delete
          - $ref: >-
              #/components/x-stackQL-resources/autoscaling_policies/methods/projects_locations_autoscaling_policies_delete
    autoscaling_policies_iam_policies:
      id: google.dataproc.autoscaling_policies_iam_policies
      name: autoscaling_policies_iam_policies
      title: Autoscaling_policies_iam_policies
      methods:
        projects_regions_autoscaling_policies_get_iam_policy:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1autoscalingPolicies~1{autoscalingPoliciesId}:getIamPolicy/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
            objectKey: $.bindings
        projects_regions_autoscaling_policies_test_iam_permissions:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1autoscalingPolicies~1{autoscalingPoliciesId}:testIamPermissions/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_autoscaling_policies_set_iam_policy:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1autoscalingPolicies~1{autoscalingPoliciesId}:setIamPolicy/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_autoscaling_policies_get_iam_policy:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1autoscalingPolicies~1{autoscalingPoliciesId}:getIamPolicy/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
            objectKey: $.bindings
        projects_locations_autoscaling_policies_set_iam_policy:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1autoscalingPolicies~1{autoscalingPoliciesId}:setIamPolicy/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_autoscaling_policies_test_iam_permissions:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1autoscalingPolicies~1{autoscalingPoliciesId}:testIamPermissions/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/autoscaling_policies_iam_policies/methods/projects_regions_autoscaling_policies_get_iam_policy
          - $ref: >-
              #/components/x-stackQL-resources/autoscaling_policies_iam_policies/methods/projects_locations_autoscaling_policies_get_iam_policy
        insert: []
        update: []
        replace:
          - $ref: >-
              #/components/x-stackQL-resources/autoscaling_policies_iam_policies/methods/projects_regions_autoscaling_policies_set_iam_policy
          - $ref: >-
              #/components/x-stackQL-resources/autoscaling_policies_iam_policies/methods/projects_locations_autoscaling_policies_set_iam_policy
        delete: []
    clusters:
      id: google.dataproc.clusters
      name: clusters
      title: Clusters
      methods:
        projects_regions_clusters_create:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectId}~1regions~1{region}~1clusters/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_clusters_list:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectId}~1regions~1{region}~1clusters/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
            objectKey: $.clusters
        projects_regions_clusters_delete:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectId}~1regions~1{region}~1clusters~1{clusterName}/delete
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_clusters_get:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectId}~1regions~1{region}~1clusters~1{clusterName}/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_clusters_patch:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectId}~1regions~1{region}~1clusters~1{clusterName}/patch
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_clusters_start:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectId}~1regions~1{region}~1clusters~1{clusterName}:start/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_clusters_stop:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectId}~1regions~1{region}~1clusters~1{clusterName}:stop/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_clusters_repair:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectId}~1regions~1{region}~1clusters~1{clusterName}:repair/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_clusters_inject_credentials:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1clusters~1{clustersId}:injectCredentials/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_clusters_diagnose:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectId}~1regions~1{region}~1clusters~1{clusterName}:diagnose/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/clusters/methods/projects_regions_clusters_get
          - $ref: >-
              #/components/x-stackQL-resources/clusters/methods/projects_regions_clusters_list
        insert:
          - $ref: >-
              #/components/x-stackQL-resources/clusters/methods/projects_regions_clusters_create
        update:
          - $ref: >-
              #/components/x-stackQL-resources/clusters/methods/projects_regions_clusters_patch
        replace: []
        delete:
          - $ref: >-
              #/components/x-stackQL-resources/clusters/methods/projects_regions_clusters_delete
    clusters_iam_policies:
      id: google.dataproc.clusters_iam_policies
      name: clusters_iam_policies
      title: Clusters_iam_policies
      methods:
        projects_regions_clusters_set_iam_policy:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1clusters~1{clustersId}:setIamPolicy/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_clusters_test_iam_permissions:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1clusters~1{clustersId}:testIamPermissions/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_clusters_get_iam_policy:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1clusters~1{clustersId}:getIamPolicy/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
            objectKey: $.bindings
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/clusters_iam_policies/methods/projects_regions_clusters_get_iam_policy
        insert: []
        update: []
        replace:
          - $ref: >-
              #/components/x-stackQL-resources/clusters_iam_policies/methods/projects_regions_clusters_set_iam_policy
        delete: []
    node_groups:
      id: google.dataproc.node_groups
      name: node_groups
      title: Node_groups
      methods:
        projects_regions_clusters_node_groups_resize:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1clusters~1{clustersId}~1nodeGroups~1{nodeGroupsId}:resize/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_clusters_node_groups_get:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1clusters~1{clustersId}~1nodeGroups~1{nodeGroupsId}/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_clusters_node_groups_repair:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1clusters~1{clustersId}~1nodeGroups~1{nodeGroupsId}:repair/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_clusters_node_groups_create:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1clusters~1{clustersId}~1nodeGroups/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/node_groups/methods/projects_regions_clusters_node_groups_get
        insert:
          - $ref: >-
              #/components/x-stackQL-resources/node_groups/methods/projects_regions_clusters_node_groups_create
        update: []
        replace: []
        delete: []
    operations_iam_policies:
      id: google.dataproc.operations_iam_policies
      name: operations_iam_policies
      title: Operations_iam_policies
      methods:
        projects_regions_operations_get_iam_policy:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1operations~1{operationsId}:getIamPolicy/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
            objectKey: $.bindings
        projects_regions_operations_set_iam_policy:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1operations~1{operationsId}:setIamPolicy/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_operations_test_iam_permissions:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1operations~1{operationsId}:testIamPermissions/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/operations_iam_policies/methods/projects_regions_operations_get_iam_policy
        insert: []
        update: []
        replace:
          - $ref: >-
              #/components/x-stackQL-resources/operations_iam_policies/methods/projects_regions_operations_set_iam_policy
        delete: []
    operations:
      id: google.dataproc.operations
      name: operations
      title: Operations
      methods:
        projects_regions_operations_get:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1operations~1{operationsId}/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_operations_delete:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1operations~1{operationsId}/delete
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_regions_operations_list:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1operations/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
            objectKey: $.unreachable
        projects_regions_operations_cancel:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1regions~1{regionsId}~1operations~1{operationsId}:cancel/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_operations_get:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1operations~1{operationsId}/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_operations_delete:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1operations~1{operationsId}/delete
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_operations_cancel:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1operations~1{operationsId}:cancel/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_operations_list:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1operations/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
            objectKey: $.unreachable
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/operations/methods/projects_regions_operations_get
          - $ref: >-
              #/components/x-stackQL-resources/operations/methods/projects_locations_operations_get
          - $ref: >-
              #/components/x-stackQL-resources/operations/methods/projects_regions_operations_list
          - $ref: >-
              #/components/x-stackQL-resources/operations/methods/projects_locations_operations_list
        insert: []
        update: []
        replace: []
        delete:
          - $ref: >-
              #/components/x-stackQL-resources/operations/methods/projects_regions_operations_delete
          - $ref: >-
              #/components/x-stackQL-resources/operations/methods/projects_locations_operations_delete
    session_templates:
      id: google.dataproc.session_templates
      name: session_templates
      title: Session_templates
      methods:
        projects_locations_session_templates_get:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessionTemplates~1{sessionTemplatesId}/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_session_templates_patch:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessionTemplates~1{sessionTemplatesId}/patch
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_session_templates_delete:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessionTemplates~1{sessionTemplatesId}/delete
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_session_templates_create:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessionTemplates/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_session_templates_list:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessionTemplates/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
            objectKey: $.sessionTemplates
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/session_templates/methods/projects_locations_session_templates_get
          - $ref: >-
              #/components/x-stackQL-resources/session_templates/methods/projects_locations_session_templates_list
        insert:
          - $ref: >-
              #/components/x-stackQL-resources/session_templates/methods/projects_locations_session_templates_create
        update:
          - $ref: >-
              #/components/x-stackQL-resources/session_templates/methods/projects_locations_session_templates_patch
        replace: []
        delete:
          - $ref: >-
              #/components/x-stackQL-resources/session_templates/methods/projects_locations_session_templates_delete
    spark_applications:
      id: google.dataproc.spark_applications
      name: spark_applications
      title: Spark_applications
      methods:
        projects_locations_sessions_spark_applications_search_stage_attempt_tasks:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions~1{sessionsId}~1sparkApplications~1{sparkApplicationsId}:searchStageAttemptTasks/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_sessions_spark_applications_access_sql_query:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions~1{sessionsId}~1sparkApplications~1{sparkApplicationsId}:accessSqlQuery/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_sessions_spark_applications_access_environment_info:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions~1{sessionsId}~1sparkApplications~1{sparkApplicationsId}:accessEnvironmentInfo/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_sessions_spark_applications_search_executors:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions~1{sessionsId}~1sparkApplications~1{sparkApplicationsId}:searchExecutors/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_sessions_spark_applications_search_jobs:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions~1{sessionsId}~1sparkApplications~1{sparkApplicationsId}:searchJobs/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_sessions_spark_applications_access_sql_plan:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions~1{sessionsId}~1sparkApplications~1{sparkApplicationsId}:accessSqlPlan/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_sessions_spark_applications_access_job:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions~1{sessionsId}~1sparkApplications~1{sparkApplicationsId}:accessJob/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_sessions_spark_applications_access:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions~1{sessionsId}~1sparkApplications~1{sparkApplicationsId}:access/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_sessions_spark_applications_search:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions~1{sessionsId}~1sparkApplications:search/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_sessions_spark_applications_access_stage_rdd_graph:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions~1{sessionsId}~1sparkApplications~1{sparkApplicationsId}:accessStageRddGraph/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_sessions_spark_applications_write:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions~1{sessionsId}~1sparkApplications~1{sparkApplicationsId}:write/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_sessions_spark_applications_search_executor_stage_summary:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions~1{sessionsId}~1sparkApplications~1{sparkApplicationsId}:searchExecutorStageSummary/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_sessions_spark_applications_summarize_stage_attempt_tasks:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions~1{sessionsId}~1sparkApplications~1{sparkApplicationsId}:summarizeStageAttemptTasks/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_sessions_spark_applications_summarize_executors:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions~1{sessionsId}~1sparkApplications~1{sparkApplicationsId}:summarizeExecutors/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_sessions_spark_applications_summarize_stages:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions~1{sessionsId}~1sparkApplications~1{sparkApplicationsId}:summarizeStages/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_sessions_spark_applications_search_sql_queries:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions~1{sessionsId}~1sparkApplications~1{sparkApplicationsId}:searchSqlQueries/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_sessions_spark_applications_summarize_jobs:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions~1{sessionsId}~1sparkApplications~1{sparkApplicationsId}:summarizeJobs/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_sessions_spark_applications_search_stages:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions~1{sessionsId}~1sparkApplications~1{sparkApplicationsId}:searchStages/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_sessions_spark_applications_access_stage_attempt:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions~1{sessionsId}~1sparkApplications~1{sparkApplicationsId}:accessStageAttempt/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_sessions_spark_applications_search_stage_attempts:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions~1{sessionsId}~1sparkApplications~1{sparkApplicationsId}:searchStageAttempts/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_batches_spark_applications_search_executor_stage_summary:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches~1{batchesId}~1sparkApplications~1{sparkApplicationsId}:searchExecutorStageSummary/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_batches_spark_applications_search_stage_attempts:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches~1{batchesId}~1sparkApplications~1{sparkApplicationsId}:searchStageAttempts/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_batches_spark_applications_summarize_stages:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches~1{batchesId}~1sparkApplications~1{sparkApplicationsId}:summarizeStages/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_batches_spark_applications_search_sql_queries:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches~1{batchesId}~1sparkApplications~1{sparkApplicationsId}:searchSqlQueries/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_batches_spark_applications_access_stage_attempt:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches~1{batchesId}~1sparkApplications~1{sparkApplicationsId}:accessStageAttempt/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_batches_spark_applications_search_stage_attempt_tasks:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches~1{batchesId}~1sparkApplications~1{sparkApplicationsId}:searchStageAttemptTasks/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_batches_spark_applications_access_stage_rdd_graph:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches~1{batchesId}~1sparkApplications~1{sparkApplicationsId}:accessStageRddGraph/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_batches_spark_applications_access:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches~1{batchesId}~1sparkApplications~1{sparkApplicationsId}:access/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_batches_spark_applications_write:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches~1{batchesId}~1sparkApplications~1{sparkApplicationsId}:write/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_batches_spark_applications_search_jobs:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches~1{batchesId}~1sparkApplications~1{sparkApplicationsId}:searchJobs/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_batches_spark_applications_search:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches~1{batchesId}~1sparkApplications:search/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_batches_spark_applications_access_environment_info:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches~1{batchesId}~1sparkApplications~1{sparkApplicationsId}:accessEnvironmentInfo/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_batches_spark_applications_search_executors:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches~1{batchesId}~1sparkApplications~1{sparkApplicationsId}:searchExecutors/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_batches_spark_applications_summarize_executors:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches~1{batchesId}~1sparkApplications~1{sparkApplicationsId}:summarizeExecutors/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_batches_spark_applications_access_sql_query:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches~1{batchesId}~1sparkApplications~1{sparkApplicationsId}:accessSqlQuery/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_batches_spark_applications_access_job:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches~1{batchesId}~1sparkApplications~1{sparkApplicationsId}:accessJob/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_batches_spark_applications_access_sql_plan:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches~1{batchesId}~1sparkApplications~1{sparkApplicationsId}:accessSqlPlan/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_batches_spark_applications_summarize_stage_attempt_tasks:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches~1{batchesId}~1sparkApplications~1{sparkApplicationsId}:summarizeStageAttemptTasks/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_batches_spark_applications_summarize_jobs:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches~1{batchesId}~1sparkApplications~1{sparkApplicationsId}:summarizeJobs/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_batches_spark_applications_search_stages:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches~1{batchesId}~1sparkApplications~1{sparkApplicationsId}:searchStages/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
      sqlVerbs:
        select: []
        insert: []
        update: []
        replace: []
        delete: []
    sessions:
      id: google.dataproc.sessions
      name: sessions
      title: Sessions
      methods:
        projects_locations_sessions_terminate:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions~1{sessionsId}:terminate/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_sessions_create:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_sessions_list:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
            objectKey: $.sessions
        projects_locations_sessions_delete:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions~1{sessionsId}/delete
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_sessions_get:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1sessions~1{sessionsId}/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/sessions/methods/projects_locations_sessions_get
          - $ref: >-
              #/components/x-stackQL-resources/sessions/methods/projects_locations_sessions_list
        insert:
          - $ref: >-
              #/components/x-stackQL-resources/sessions/methods/projects_locations_sessions_create
        update: []
        replace: []
        delete:
          - $ref: >-
              #/components/x-stackQL-resources/sessions/methods/projects_locations_sessions_delete
    batches:
      id: google.dataproc.batches
      name: batches
      title: Batches
      methods:
        projects_locations_batches_create:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_batches_list:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
            objectKey: $.batches
        projects_locations_batches_get:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches~1{batchesId}/get
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_batches_delete:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches~1{batchesId}/delete
          response:
            mediaType: application/json
            openAPIDocKey: '200'
        projects_locations_batches_analyze:
          operation:
            $ref: >-
              #/paths/~1v1~1projects~1{projectsId}~1locations~1{locationsId}~1batches~1{batchesId}:analyze/post
          response:
            mediaType: application/json
            openAPIDocKey: '200'
      sqlVerbs:
        select:
          - $ref: >-
              #/components/x-stackQL-resources/batches/methods/projects_locations_batches_get
          - $ref: >-
              #/components/x-stackQL-resources/batches/methods/projects_locations_batches_list
        insert:
          - $ref: >-
              #/components/x-stackQL-resources/batches/methods/projects_locations_batches_create
        update: []
        replace: []
        delete:
          - $ref: >-
              #/components/x-stackQL-resources/batches/methods/projects_locations_batches_delete
paths:
  /v1/projects/{projectsId}/regions/{regionsId}/workflowTemplates:instantiateInline:
    parameters: &ref_1
      - $ref: '#/components/parameters/oauth_token'
      - $ref: '#/components/parameters/fields'
      - $ref: '#/components/parameters/alt'
      - $ref: '#/components/parameters/_.xgafv'
      - $ref: '#/components/parameters/key'
      - $ref: '#/components/parameters/uploadType'
      - $ref: '#/components/parameters/upload_protocol'
      - $ref: '#/components/parameters/quotaUser'
      - $ref: '#/components/parameters/callback'
      - $ref: '#/components/parameters/access_token'
      - $ref: '#/components/parameters/prettyPrint'
    post:
      description: >-
        Instantiates a template and begins execution.This method is equivalent
        to executing the sequence CreateWorkflowTemplate,
        InstantiateWorkflowTemplate, DeleteWorkflowTemplate.The returned
        Operation can be used to track execution of workflow by polling
        operations.get. The Operation will complete when entire workflow is
        finished.The running workflow can be aborted via operations.cancel. This
        will cause any inflight jobs to be cancelled and workflow-owned clusters
        to be deleted.The Operation.metadata will be WorkflowMetadata
        (https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#workflowmetadata).
        Also see Using WorkflowMetadata
        (https://cloud.google.com/dataproc/docs/concepts/workflows/debugging#using_workflowmetadata).On
        successful completion, Operation.response will be Empty.
      operationId: dataproc.projects.regions.workflowTemplates.instantiateInline
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WorkflowTemplate'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Operation'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: query
          name: requestId
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/workflowTemplates/{workflowTemplatesId}:
    parameters: *ref_1
    put:
      description: >-
        Updates (replaces) workflow template. The updated template must contain
        version that matches the current server version.
      operationId: dataproc.projects.regions.workflowTemplates.update
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WorkflowTemplate'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/WorkflowTemplate'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: workflowTemplatesId
          required: true
          schema:
            type: string
    delete:
      description: Deletes a workflow template. It does not cancel in-progress workflows.
      operationId: dataproc.projects.regions.workflowTemplates.delete
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Empty'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: workflowTemplatesId
          required: true
          schema:
            type: string
        - in: query
          name: version
          schema:
            type: integer
            format: int32
    get:
      description: >-
        Retrieves the latest workflow template.Can retrieve previously
        instantiated template by specifying optional version parameter.
      operationId: dataproc.projects.regions.workflowTemplates.get
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/WorkflowTemplate'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: workflowTemplatesId
          required: true
          schema:
            type: string
        - in: query
          name: version
          schema:
            type: integer
            format: int32
  /v1/projects/{projectsId}/regions/{regionsId}/workflowTemplates/{workflowTemplatesId}:testIamPermissions:
    parameters: *ref_1
    post:
      description: >-
        Returns permissions that a caller has on the specified resource. If the
        resource does not exist, this will return an empty set of permissions,
        not a NOT_FOUND error.Note: This operation is designed to be used for
        building permission-aware UIs and command-line tools, not for
        authorization checking. This operation may "fail open" without warning.
      operationId: dataproc.projects.regions.workflowTemplates.testIamPermissions
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/TestIamPermissionsRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/TestIamPermissionsResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: workflowTemplatesId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/workflowTemplates:
    parameters: *ref_1
    get:
      description: Lists workflows that match the specified filter in the request.
      operationId: dataproc.projects.regions.workflowTemplates.list
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListWorkflowTemplatesResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
        - in: query
          name: pageToken
          schema:
            type: string
    post:
      description: Creates new workflow template.
      operationId: dataproc.projects.regions.workflowTemplates.create
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WorkflowTemplate'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/WorkflowTemplate'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/workflowTemplates/{workflowTemplatesId}:instantiate:
    parameters: *ref_1
    post:
      description: >-
        Instantiates a template and begins execution.The returned Operation can
        be used to track execution of workflow by polling operations.get. The
        Operation will complete when entire workflow is finished.The running
        workflow can be aborted via operations.cancel. This will cause any
        inflight jobs to be cancelled and workflow-owned clusters to be
        deleted.The Operation.metadata will be WorkflowMetadata
        (https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#workflowmetadata).
        Also see Using WorkflowMetadata
        (https://cloud.google.com/dataproc/docs/concepts/workflows/debugging#using_workflowmetadata).On
        successful completion, Operation.response will be Empty.
      operationId: dataproc.projects.regions.workflowTemplates.instantiate
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/InstantiateWorkflowTemplateRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Operation'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: workflowTemplatesId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/workflowTemplates/{workflowTemplatesId}:getIamPolicy:
    parameters: *ref_1
    post:
      description: >-
        Gets the access control policy for a resource. Returns an empty policy
        if the resource exists and does not have a policy set.
      operationId: dataproc.projects.regions.workflowTemplates.getIamPolicy
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/GetIamPolicyRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Policy'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: workflowTemplatesId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/workflowTemplates/{workflowTemplatesId}:setIamPolicy:
    parameters: *ref_1
    post:
      description: >-
        Sets the access control policy on the specified resource. Replaces any
        existing policy.Can return NOT_FOUND, INVALID_ARGUMENT, and
        PERMISSION_DENIED errors.
      operationId: dataproc.projects.regions.workflowTemplates.setIamPolicy
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/SetIamPolicyRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Policy'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: workflowTemplatesId
          required: true
          schema:
            type: string
  /v1/projects/{projectId}/regions/{region}/jobs/{jobId}:cancel:
    parameters: *ref_1
    post:
      description: >-
        Starts a job cancellation request. To access the job resource after
        cancellation, call regions/{region}/jobs.list
        (https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs/list)
        or regions/{region}/jobs.get
        (https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs/get).
      operationId: dataproc.projects.regions.jobs.cancel
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CancelJobRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Job'
      parameters:
        - in: path
          name: projectId
          required: true
          schema:
            type: string
        - in: path
          name: region
          required: true
          schema:
            type: string
        - in: path
          name: jobId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/jobs/{jobsId}:testIamPermissions:
    parameters: *ref_1
    post:
      description: >-
        Returns permissions that a caller has on the specified resource. If the
        resource does not exist, this will return an empty set of permissions,
        not a NOT_FOUND error.Note: This operation is designed to be used for
        building permission-aware UIs and command-line tools, not for
        authorization checking. This operation may "fail open" without warning.
      operationId: dataproc.projects.regions.jobs.testIamPermissions
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/TestIamPermissionsRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/TestIamPermissionsResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: jobsId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/jobs/{jobsId}:setIamPolicy:
    parameters: *ref_1
    post:
      description: >-
        Sets the access control policy on the specified resource. Replaces any
        existing policy.Can return NOT_FOUND, INVALID_ARGUMENT, and
        PERMISSION_DENIED errors.
      operationId: dataproc.projects.regions.jobs.setIamPolicy
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/SetIamPolicyRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Policy'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: jobsId
          required: true
          schema:
            type: string
  /v1/projects/{projectId}/regions/{region}/jobs:submit:
    parameters: *ref_1
    post:
      description: Submits a job to a cluster.
      operationId: dataproc.projects.regions.jobs.submit
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/SubmitJobRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Job'
      parameters:
        - in: path
          name: projectId
          required: true
          schema:
            type: string
        - in: path
          name: region
          required: true
          schema:
            type: string
  /v1/projects/{projectId}/regions/{region}/jobs/{jobId}:
    parameters: *ref_1
    delete:
      description: >-
        Deletes the job from the project. If the job is active, the delete
        fails, and the response returns FAILED_PRECONDITION.
      operationId: dataproc.projects.regions.jobs.delete
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Empty'
      parameters:
        - in: path
          name: projectId
          required: true
          schema:
            type: string
        - in: path
          name: region
          required: true
          schema:
            type: string
        - in: path
          name: jobId
          required: true
          schema:
            type: string
    patch:
      description: Updates a job in a project.
      operationId: dataproc.projects.regions.jobs.patch
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/Job'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Job'
      parameters:
        - in: path
          name: projectId
          required: true
          schema:
            type: string
        - in: path
          name: region
          required: true
          schema:
            type: string
        - in: path
          name: jobId
          required: true
          schema:
            type: string
        - in: query
          name: updateMask
          schema:
            type: string
            format: google-fieldmask
    get:
      description: Gets the resource representation for a job in a project.
      operationId: dataproc.projects.regions.jobs.get
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Job'
      parameters:
        - in: path
          name: projectId
          required: true
          schema:
            type: string
        - in: path
          name: region
          required: true
          schema:
            type: string
        - in: path
          name: jobId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/jobs/{jobsId}:getIamPolicy:
    parameters: *ref_1
    post:
      description: >-
        Gets the access control policy for a resource. Returns an empty policy
        if the resource exists and does not have a policy set.
      operationId: dataproc.projects.regions.jobs.getIamPolicy
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/GetIamPolicyRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Policy'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: jobsId
          required: true
          schema:
            type: string
  /v1/projects/{projectId}/regions/{region}/jobs:
    parameters: *ref_1
    get:
      description: Lists regions/{region}/jobs in a project.
      operationId: dataproc.projects.regions.jobs.list
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListJobsResponse'
      parameters:
        - in: path
          name: projectId
          required: true
          schema:
            type: string
        - in: path
          name: region
          required: true
          schema:
            type: string
        - in: query
          name: clusterName
          schema:
            type: string
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
        - in: query
          name: pageToken
          schema:
            type: string
        - in: query
          name: filter
          schema:
            type: string
        - in: query
          name: jobStateMatcher
          schema:
            type: string
  /v1/projects/{projectId}/regions/{region}/jobs:submitAsOperation:
    parameters: *ref_1
    post:
      description: Submits job to a cluster.
      operationId: dataproc.projects.regions.jobs.submitAsOperation
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/SubmitJobRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Operation'
      parameters:
        - in: path
          name: projectId
          required: true
          schema:
            type: string
        - in: path
          name: region
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/autoscalingPolicies/{autoscalingPoliciesId}:
    parameters: *ref_1
    put:
      description: >-
        Updates (replaces) autoscaling policy.Disabled check for update_mask,
        because all updates will be full replacements.
      operationId: dataproc.projects.regions.autoscalingPolicies.update
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/AutoscalingPolicy'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/AutoscalingPolicy'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: autoscalingPoliciesId
          required: true
          schema:
            type: string
    get:
      description: Retrieves autoscaling policy.
      operationId: dataproc.projects.regions.autoscalingPolicies.get
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/AutoscalingPolicy'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: autoscalingPoliciesId
          required: true
          schema:
            type: string
    delete:
      description: >-
        Deletes an autoscaling policy. It is an error to delete an autoscaling
        policy that is in use by one or more clusters.
      operationId: dataproc.projects.regions.autoscalingPolicies.delete
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Empty'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: autoscalingPoliciesId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/autoscalingPolicies:
    parameters: *ref_1
    get:
      description: Lists autoscaling policies in the project.
      operationId: dataproc.projects.regions.autoscalingPolicies.list
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListAutoscalingPoliciesResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
        - in: query
          name: pageToken
          schema:
            type: string
    post:
      description: Creates new autoscaling policy.
      operationId: dataproc.projects.regions.autoscalingPolicies.create
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/AutoscalingPolicy'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/AutoscalingPolicy'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/autoscalingPolicies/{autoscalingPoliciesId}:getIamPolicy:
    parameters: *ref_1
    post:
      description: >-
        Gets the access control policy for a resource. Returns an empty policy
        if the resource exists and does not have a policy set.
      operationId: dataproc.projects.regions.autoscalingPolicies.getIamPolicy
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/GetIamPolicyRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Policy'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: autoscalingPoliciesId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/autoscalingPolicies/{autoscalingPoliciesId}:testIamPermissions:
    parameters: *ref_1
    post:
      description: >-
        Returns permissions that a caller has on the specified resource. If the
        resource does not exist, this will return an empty set of permissions,
        not a NOT_FOUND error.Note: This operation is designed to be used for
        building permission-aware UIs and command-line tools, not for
        authorization checking. This operation may "fail open" without warning.
      operationId: dataproc.projects.regions.autoscalingPolicies.testIamPermissions
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/TestIamPermissionsRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/TestIamPermissionsResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: autoscalingPoliciesId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/autoscalingPolicies/{autoscalingPoliciesId}:setIamPolicy:
    parameters: *ref_1
    post:
      description: >-
        Sets the access control policy on the specified resource. Replaces any
        existing policy.Can return NOT_FOUND, INVALID_ARGUMENT, and
        PERMISSION_DENIED errors.
      operationId: dataproc.projects.regions.autoscalingPolicies.setIamPolicy
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/SetIamPolicyRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Policy'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: autoscalingPoliciesId
          required: true
          schema:
            type: string
  /v1/projects/{projectId}/regions/{region}/clusters:
    parameters: *ref_1
    post:
      description: >-
        Creates a cluster in a project. The returned Operation.metadata will be
        ClusterOperationMetadata
        (https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#clusteroperationmetadata).
      operationId: dataproc.projects.regions.clusters.create
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/Cluster'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Operation'
      parameters:
        - in: path
          name: projectId
          required: true
          schema:
            type: string
        - in: path
          name: region
          required: true
          schema:
            type: string
        - in: query
          name: actionOnFailedPrimaryWorkers
          schema:
            type: string
        - in: query
          name: requestId
          schema:
            type: string
    get:
      description: Lists all regions/{region}/clusters in a project alphabetically.
      operationId: dataproc.projects.regions.clusters.list
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListClustersResponse'
      parameters:
        - in: path
          name: projectId
          required: true
          schema:
            type: string
        - in: path
          name: region
          required: true
          schema:
            type: string
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
        - in: query
          name: pageToken
          schema:
            type: string
        - in: query
          name: filter
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/clusters/{clustersId}:setIamPolicy:
    parameters: *ref_1
    post:
      description: >-
        Sets the access control policy on the specified resource. Replaces any
        existing policy.Can return NOT_FOUND, INVALID_ARGUMENT, and
        PERMISSION_DENIED errors.
      operationId: dataproc.projects.regions.clusters.setIamPolicy
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/SetIamPolicyRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Policy'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: clustersId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/clusters/{clustersId}:testIamPermissions:
    parameters: *ref_1
    post:
      description: >-
        Returns permissions that a caller has on the specified resource. If the
        resource does not exist, this will return an empty set of permissions,
        not a NOT_FOUND error.Note: This operation is designed to be used for
        building permission-aware UIs and command-line tools, not for
        authorization checking. This operation may "fail open" without warning.
      operationId: dataproc.projects.regions.clusters.testIamPermissions
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/TestIamPermissionsRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/TestIamPermissionsResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: clustersId
          required: true
          schema:
            type: string
  /v1/projects/{projectId}/regions/{region}/clusters/{clusterName}:
    parameters: *ref_1
    delete:
      description: >-
        Deletes a cluster in a project. The returned Operation.metadata will be
        ClusterOperationMetadata
        (https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#clusteroperationmetadata).
      operationId: dataproc.projects.regions.clusters.delete
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Operation'
      parameters:
        - in: path
          name: projectId
          required: true
          schema:
            type: string
        - in: path
          name: region
          required: true
          schema:
            type: string
        - in: path
          name: clusterName
          required: true
          schema:
            type: string
        - in: query
          name: clusterUuid
          schema:
            type: string
        - in: query
          name: gracefulTerminationTimeout
          schema:
            type: string
            format: google-duration
        - in: query
          name: requestId
          schema:
            type: string
    get:
      description: Gets the resource representation for a cluster in a project.
      operationId: dataproc.projects.regions.clusters.get
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Cluster'
      parameters:
        - in: path
          name: projectId
          required: true
          schema:
            type: string
        - in: path
          name: region
          required: true
          schema:
            type: string
        - in: path
          name: clusterName
          required: true
          schema:
            type: string
    patch:
      description: >-
        Updates a cluster in a project. The returned Operation.metadata will be
        ClusterOperationMetadata
        (https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#clusteroperationmetadata).
        The cluster must be in a RUNNING state or an error is returned.
      operationId: dataproc.projects.regions.clusters.patch
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/Cluster'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Operation'
      parameters:
        - in: path
          name: projectId
          required: true
          schema:
            type: string
        - in: path
          name: region
          required: true
          schema:
            type: string
        - in: path
          name: clusterName
          required: true
          schema:
            type: string
        - in: query
          name: requestId
          schema:
            type: string
        - in: query
          name: updateMask
          schema:
            type: string
            format: google-fieldmask
        - in: query
          name: gracefulDecommissionTimeout
          schema:
            type: string
            format: google-duration
  /v1/projects/{projectId}/regions/{region}/clusters/{clusterName}:start:
    parameters: *ref_1
    post:
      description: Starts a cluster in a project.
      operationId: dataproc.projects.regions.clusters.start
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/StartClusterRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Operation'
      parameters:
        - in: path
          name: projectId
          required: true
          schema:
            type: string
        - in: path
          name: region
          required: true
          schema:
            type: string
        - in: path
          name: clusterName
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/clusters/{clustersId}:getIamPolicy:
    parameters: *ref_1
    post:
      description: >-
        Gets the access control policy for a resource. Returns an empty policy
        if the resource exists and does not have a policy set.
      operationId: dataproc.projects.regions.clusters.getIamPolicy
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/GetIamPolicyRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Policy'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: clustersId
          required: true
          schema:
            type: string
  /v1/projects/{projectId}/regions/{region}/clusters/{clusterName}:stop:
    parameters: *ref_1
    post:
      description: Stops a cluster in a project.
      operationId: dataproc.projects.regions.clusters.stop
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/StopClusterRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Operation'
      parameters:
        - in: path
          name: projectId
          required: true
          schema:
            type: string
        - in: path
          name: region
          required: true
          schema:
            type: string
        - in: path
          name: clusterName
          required: true
          schema:
            type: string
  /v1/projects/{projectId}/regions/{region}/clusters/{clusterName}:repair:
    parameters: *ref_1
    post:
      description: Repairs a cluster.
      operationId: dataproc.projects.regions.clusters.repair
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/RepairClusterRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Operation'
      parameters:
        - in: path
          name: projectId
          required: true
          schema:
            type: string
        - in: path
          name: region
          required: true
          schema:
            type: string
        - in: path
          name: clusterName
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/clusters/{clustersId}:injectCredentials:
    parameters: *ref_1
    post:
      description: >-
        Inject encrypted credentials into all of the VMs in a cluster.The target
        cluster must be a personal auth cluster assigned to the user who is
        issuing the RPC.
      operationId: dataproc.projects.regions.clusters.injectCredentials
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/InjectCredentialsRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Operation'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: clustersId
          required: true
          schema:
            type: string
  /v1/projects/{projectId}/regions/{region}/clusters/{clusterName}:diagnose:
    parameters: *ref_1
    post:
      description: >-
        Gets cluster diagnostic information. The returned Operation.metadata
        will be ClusterOperationMetadata
        (https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#clusteroperationmetadata).
        After the operation completes, Operation.response contains
        DiagnoseClusterResults
        (https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#diagnoseclusterresults).
      operationId: dataproc.projects.regions.clusters.diagnose
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/DiagnoseClusterRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Operation'
      parameters:
        - in: path
          name: projectId
          required: true
          schema:
            type: string
        - in: path
          name: region
          required: true
          schema:
            type: string
        - in: path
          name: clusterName
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/clusters/{clustersId}/nodeGroups/{nodeGroupsId}:resize:
    parameters: *ref_1
    post:
      description: >-
        Resizes a node group in a cluster. The returned Operation.metadata is
        NodeGroupOperationMetadata
        (https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#nodegroupoperationmetadata).
      operationId: dataproc.projects.regions.clusters.nodeGroups.resize
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ResizeNodeGroupRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Operation'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: clustersId
          required: true
          schema:
            type: string
        - in: path
          name: nodeGroupsId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/clusters/{clustersId}/nodeGroups/{nodeGroupsId}:
    parameters: *ref_1
    get:
      description: Gets the resource representation for a node group in a cluster.
      operationId: dataproc.projects.regions.clusters.nodeGroups.get
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/NodeGroup'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: clustersId
          required: true
          schema:
            type: string
        - in: path
          name: nodeGroupsId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/clusters/{clustersId}/nodeGroups/{nodeGroupsId}:repair:
    parameters: *ref_1
    post:
      description: Repair nodes in a node group.
      operationId: dataproc.projects.regions.clusters.nodeGroups.repair
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/RepairNodeGroupRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Operation'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: clustersId
          required: true
          schema:
            type: string
        - in: path
          name: nodeGroupsId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/clusters/{clustersId}/nodeGroups:
    parameters: *ref_1
    post:
      description: >-
        Creates a node group in a cluster. The returned Operation.metadata is
        NodeGroupOperationMetadata
        (https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#nodegroupoperationmetadata).
      operationId: dataproc.projects.regions.clusters.nodeGroups.create
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/NodeGroup'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Operation'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: clustersId
          required: true
          schema:
            type: string
        - in: query
          name: requestId
          schema:
            type: string
        - in: query
          name: nodeGroupId
          schema:
            type: string
        - in: query
          name: parentOperationId
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/operations/{operationsId}:getIamPolicy:
    parameters: *ref_1
    post:
      description: >-
        Gets the access control policy for a resource. Returns an empty policy
        if the resource exists and does not have a policy set.
      operationId: dataproc.projects.regions.operations.getIamPolicy
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/GetIamPolicyRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Policy'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: operationsId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/operations/{operationsId}:setIamPolicy:
    parameters: *ref_1
    post:
      description: >-
        Sets the access control policy on the specified resource. Replaces any
        existing policy.Can return NOT_FOUND, INVALID_ARGUMENT, and
        PERMISSION_DENIED errors.
      operationId: dataproc.projects.regions.operations.setIamPolicy
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/SetIamPolicyRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Policy'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: operationsId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/operations/{operationsId}:
    parameters: *ref_1
    get:
      description: >-
        Gets the latest state of a long-running operation. Clients can use this
        method to poll the operation result at intervals as recommended by the
        API service.
      operationId: dataproc.projects.regions.operations.get
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Operation'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: operationsId
          required: true
          schema:
            type: string
    delete:
      description: >-
        Deletes a long-running operation. This method indicates that the client
        is no longer interested in the operation result. It does not cancel the
        operation. If the server doesn't support this method, it returns
        google.rpc.Code.UNIMPLEMENTED.
      operationId: dataproc.projects.regions.operations.delete
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Empty'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: operationsId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/operations:
    parameters: *ref_1
    get:
      description: >-
        Lists operations that match the specified filter in the request. If the
        server doesn't support this method, it returns UNIMPLEMENTED.
      operationId: dataproc.projects.regions.operations.list
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListOperationsResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
        - in: query
          name: returnPartialSuccess
          schema:
            type: boolean
        - in: query
          name: pageToken
          schema:
            type: string
        - in: query
          name: filter
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/operations/{operationsId}:testIamPermissions:
    parameters: *ref_1
    post:
      description: >-
        Returns permissions that a caller has on the specified resource. If the
        resource does not exist, this will return an empty set of permissions,
        not a NOT_FOUND error.Note: This operation is designed to be used for
        building permission-aware UIs and command-line tools, not for
        authorization checking. This operation may "fail open" without warning.
      operationId: dataproc.projects.regions.operations.testIamPermissions
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/TestIamPermissionsRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/TestIamPermissionsResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: operationsId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/regions/{regionsId}/operations/{operationsId}:cancel:
    parameters: *ref_1
    post:
      description: >-
        Starts asynchronous cancellation on a long-running operation. The server
        makes a best effort to cancel the operation, but success is not
        guaranteed. If the server doesn't support this method, it returns
        google.rpc.Code.UNIMPLEMENTED. Clients can use Operations.GetOperation
        or other methods to check whether the cancellation succeeded or whether
        the operation completed despite cancellation. On successful
        cancellation, the operation is not deleted; instead, it becomes an
        operation with an Operation.error value with a google.rpc.Status.code of
        1, corresponding to Code.CANCELLED.
      operationId: dataproc.projects.regions.operations.cancel
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Empty'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: regionsId
          required: true
          schema:
            type: string
        - in: path
          name: operationsId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/sessionTemplates/{sessionTemplatesId}:
    parameters: *ref_1
    get:
      description: Gets the resource representation for a session template.
      operationId: dataproc.projects.locations.sessionTemplates.get
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/SessionTemplate'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionTemplatesId
          required: true
          schema:
            type: string
    patch:
      description: Updates the session template synchronously.
      operationId: dataproc.projects.locations.sessionTemplates.patch
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/SessionTemplate'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/SessionTemplate'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionTemplatesId
          required: true
          schema:
            type: string
    delete:
      description: Deletes a session template.
      operationId: dataproc.projects.locations.sessionTemplates.delete
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Empty'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionTemplatesId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/sessionTemplates:
    parameters: *ref_1
    post:
      description: Create a session template synchronously.
      operationId: dataproc.projects.locations.sessionTemplates.create
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/SessionTemplate'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/SessionTemplate'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
    get:
      description: Lists session templates.
      operationId: dataproc.projects.locations.sessionTemplates.list
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListSessionTemplatesResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
        - in: query
          name: filter
          schema:
            type: string
        - in: query
          name: pageToken
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/sessions/{sessionsId}/sparkApplications/{sparkApplicationsId}:searchStageAttemptTasks:
    parameters: *ref_1
    get:
      description: >-
        Obtain data corresponding to tasks for a spark stage attempt for a Spark
        Application.
      operationId: >-
        dataproc.projects.locations.sessions.sparkApplications.searchStageAttemptTasks
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/SearchSessionSparkApplicationStageAttemptTasksResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionsId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
        - in: query
          name: pageToken
          schema:
            type: string
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
        - in: query
          name: taskStatus
          schema:
            type: string
        - in: query
          name: sortRuntime
          schema:
            type: boolean
        - in: query
          name: stageId
          schema:
            type: string
            format: int64
        - in: query
          name: stageAttemptId
          schema:
            type: integer
            format: int32
  /v1/projects/{projectsId}/locations/{locationsId}/sessions/{sessionsId}/sparkApplications/{sparkApplicationsId}:accessSqlQuery:
    parameters: *ref_1
    get:
      description: >-
        Obtain data corresponding to a particular SQL Query for a Spark
        Application.
      operationId: dataproc.projects.locations.sessions.sparkApplications.accessSqlQuery
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/AccessSessionSparkApplicationSqlQueryResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionsId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
        - in: query
          name: planDescription
          schema:
            type: boolean
        - in: query
          name: details
          schema:
            type: boolean
        - in: query
          name: executionId
          schema:
            type: string
            format: int64
  /v1/projects/{projectsId}/locations/{locationsId}/sessions/{sessionsId}/sparkApplications/{sparkApplicationsId}:accessEnvironmentInfo:
    parameters: *ref_1
    get:
      description: Obtain environment details for a Spark Application
      operationId: >-
        dataproc.projects.locations.sessions.sparkApplications.accessEnvironmentInfo
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/AccessSessionSparkApplicationEnvironmentInfoResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionsId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/sessions/{sessionsId}/sparkApplications/{sparkApplicationsId}:searchExecutors:
    parameters: *ref_1
    get:
      description: Obtain data corresponding to executors for a Spark Application.
      operationId: dataproc.projects.locations.sessions.sparkApplications.searchExecutors
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/SearchSessionSparkApplicationExecutorsResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionsId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
        - in: query
          name: pageToken
          schema:
            type: string
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
        - in: query
          name: executorStatus
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/sessions/{sessionsId}/sparkApplications/{sparkApplicationsId}:searchJobs:
    parameters: *ref_1
    get:
      description: Obtain list of spark jobs corresponding to a Spark Application.
      operationId: dataproc.projects.locations.sessions.sparkApplications.searchJobs
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/SearchSessionSparkApplicationJobsResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionsId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: jobIds
          schema:
            type: string
            format: int64
        - in: query
          name: pageToken
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
        - in: query
          name: jobStatus
          schema:
            type: string
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
  /v1/projects/{projectsId}/locations/{locationsId}/sessions/{sessionsId}/sparkApplications/{sparkApplicationsId}:accessSqlPlan:
    parameters: *ref_1
    get:
      description: >-
        Obtain Spark Plan Graph for a Spark Application SQL execution. Limits
        the number of clusters returned as part of the graph to 10000.
      operationId: dataproc.projects.locations.sessions.sparkApplications.accessSqlPlan
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/AccessSessionSparkApplicationSqlSparkPlanGraphResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionsId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: executionId
          schema:
            type: string
            format: int64
        - in: query
          name: parent
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/sessions/{sessionsId}/sparkApplications/{sparkApplicationsId}:accessJob:
    parameters: *ref_1
    get:
      description: Obtain data corresponding to a spark job for a Spark Application.
      operationId: dataproc.projects.locations.sessions.sparkApplications.accessJob
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/AccessSessionSparkApplicationJobResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionsId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: jobId
          schema:
            type: string
            format: int64
        - in: query
          name: parent
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/sessions/{sessionsId}/sparkApplications/{sparkApplicationsId}:access:
    parameters: *ref_1
    get:
      description: >-
        Obtain high level information corresponding to a single Spark
        Application.
      operationId: dataproc.projects.locations.sessions.sparkApplications.access
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/AccessSessionSparkApplicationResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionsId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/sessions/{sessionsId}/sparkApplications:search:
    parameters: *ref_1
    get:
      description: >-
        Obtain high level information and list of Spark Applications
        corresponding to a batch
      operationId: dataproc.projects.locations.sessions.sparkApplications.search
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/SearchSessionSparkApplicationsResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionsId
          required: true
          schema:
            type: string
        - in: query
          name: maxTime
          schema:
            type: string
            format: google-datetime
        - in: query
          name: maxEndTime
          schema:
            type: string
            format: google-datetime
        - in: query
          name: minTime
          schema:
            type: string
            format: google-datetime
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
        - in: query
          name: pageToken
          schema:
            type: string
        - in: query
          name: applicationStatus
          schema:
            type: string
        - in: query
          name: minEndTime
          schema:
            type: string
            format: google-datetime
  /v1/projects/{projectsId}/locations/{locationsId}/sessions/{sessionsId}/sparkApplications/{sparkApplicationsId}:accessStageRddGraph:
    parameters: *ref_1
    get:
      description: >-
        Obtain RDD operation graph for a Spark Application Stage. Limits the
        number of clusters returned as part of the graph to 10000.
      operationId: >-
        dataproc.projects.locations.sessions.sparkApplications.accessStageRddGraph
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/AccessSessionSparkApplicationStageRddOperationGraphResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionsId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: stageId
          schema:
            type: string
            format: int64
        - in: query
          name: parent
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/sessions/{sessionsId}/sparkApplications/{sparkApplicationsId}:write:
    parameters: *ref_1
    post:
      description: Write wrapper objects from dataplane to spanner
      operationId: dataproc.projects.locations.sessions.sparkApplications.write
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WriteSessionSparkApplicationContextRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/WriteSessionSparkApplicationContextResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionsId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/sessions/{sessionsId}/sparkApplications/{sparkApplicationsId}:searchExecutorStageSummary:
    parameters: *ref_1
    get:
      description: Obtain executor summary with respect to a spark stage attempt.
      operationId: >-
        dataproc.projects.locations.sessions.sparkApplications.searchExecutorStageSummary
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/SearchSessionSparkApplicationExecutorStageSummaryResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionsId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: pageToken
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
        - in: query
          name: stageId
          schema:
            type: string
            format: int64
        - in: query
          name: stageAttemptId
          schema:
            type: integer
            format: int32
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
  /v1/projects/{projectsId}/locations/{locationsId}/sessions/{sessionsId}/sparkApplications/{sparkApplicationsId}:summarizeStageAttemptTasks:
    parameters: *ref_1
    get:
      description: Obtain summary of Tasks for a Spark Application Stage Attempt
      operationId: >-
        dataproc.projects.locations.sessions.sparkApplications.summarizeStageAttemptTasks
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/SummarizeSessionSparkApplicationStageAttemptTasksResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionsId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
        - in: query
          name: stageId
          schema:
            type: string
            format: int64
        - in: query
          name: stageAttemptId
          schema:
            type: integer
            format: int32
  /v1/projects/{projectsId}/locations/{locationsId}/sessions/{sessionsId}/sparkApplications/{sparkApplicationsId}:summarizeExecutors:
    parameters: *ref_1
    get:
      description: Obtain summary of Executor Summary for a Spark Application
      operationId: >-
        dataproc.projects.locations.sessions.sparkApplications.summarizeExecutors
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/SummarizeSessionSparkApplicationExecutorsResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionsId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/sessions/{sessionsId}/sparkApplications/{sparkApplicationsId}:summarizeStages:
    parameters: *ref_1
    get:
      description: Obtain summary of Stages for a Spark Application
      operationId: dataproc.projects.locations.sessions.sparkApplications.summarizeStages
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/SummarizeSessionSparkApplicationStagesResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionsId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: stageIds
          schema:
            type: string
            format: int64
        - in: query
          name: parent
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/sessions/{sessionsId}/sparkApplications/{sparkApplicationsId}:searchSqlQueries:
    parameters: *ref_1
    get:
      description: Obtain data corresponding to SQL Queries for a Spark Application.
      operationId: dataproc.projects.locations.sessions.sparkApplications.searchSqlQueries
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/SearchSessionSparkApplicationSqlQueriesResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionsId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: details
          schema:
            type: boolean
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
        - in: query
          name: parent
          schema:
            type: string
        - in: query
          name: planDescription
          schema:
            type: boolean
        - in: query
          name: pageToken
          schema:
            type: string
        - in: query
          name: operationIds
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/sessions/{sessionsId}/sparkApplications/{sparkApplicationsId}:summarizeJobs:
    parameters: *ref_1
    get:
      description: Obtain summary of Jobs for a Spark Application
      operationId: dataproc.projects.locations.sessions.sparkApplications.summarizeJobs
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/SummarizeSessionSparkApplicationJobsResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionsId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
        - in: query
          name: jobIds
          schema:
            type: string
            format: int64
  /v1/projects/{projectsId}/locations/{locationsId}/sessions/{sessionsId}/sparkApplications/{sparkApplicationsId}:searchStages:
    parameters: *ref_1
    get:
      description: Obtain data corresponding to stages for a Spark Application.
      operationId: dataproc.projects.locations.sessions.sparkApplications.searchStages
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/SearchSessionSparkApplicationStagesResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionsId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
        - in: query
          name: summaryMetricsMask
          schema:
            type: string
            format: google-fieldmask
        - in: query
          name: stageIds
          schema:
            type: string
            format: int64
        - in: query
          name: stageStatus
          schema:
            type: string
        - in: query
          name: pageToken
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/sessions/{sessionsId}/sparkApplications/{sparkApplicationsId}:accessStageAttempt:
    parameters: *ref_1
    get:
      description: >-
        Obtain data corresponding to a spark stage attempt for a Spark
        Application.
      operationId: >-
        dataproc.projects.locations.sessions.sparkApplications.accessStageAttempt
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/AccessSessionSparkApplicationStageAttemptResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionsId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
        - in: query
          name: stageAttemptId
          schema:
            type: integer
            format: int32
        - in: query
          name: stageId
          schema:
            type: string
            format: int64
        - in: query
          name: summaryMetricsMask
          schema:
            type: string
            format: google-fieldmask
  /v1/projects/{projectsId}/locations/{locationsId}/sessions/{sessionsId}/sparkApplications/{sparkApplicationsId}:searchStageAttempts:
    parameters: *ref_1
    get:
      description: >-
        Obtain data corresponding to a spark stage attempts for a Spark
        Application.
      operationId: >-
        dataproc.projects.locations.sessions.sparkApplications.searchStageAttempts
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/SearchSessionSparkApplicationStageAttemptsResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionsId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: pageToken
          schema:
            type: string
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
        - in: query
          name: stageId
          schema:
            type: string
            format: int64
        - in: query
          name: parent
          schema:
            type: string
        - in: query
          name: summaryMetricsMask
          schema:
            type: string
            format: google-fieldmask
  /v1/projects/{projectsId}/locations/{locationsId}/sessions/{sessionsId}:terminate:
    parameters: *ref_1
    post:
      description: Terminates the interactive session.
      operationId: dataproc.projects.locations.sessions.terminate
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/TerminateSessionRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Operation'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionsId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/sessions:
    parameters: *ref_1
    post:
      description: Create an interactive session asynchronously.
      operationId: dataproc.projects.locations.sessions.create
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/Session'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Operation'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: query
          name: requestId
          schema:
            type: string
        - in: query
          name: sessionId
          schema:
            type: string
    get:
      description: Lists interactive sessions.
      operationId: dataproc.projects.locations.sessions.list
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListSessionsResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: query
          name: filter
          schema:
            type: string
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
        - in: query
          name: pageToken
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/sessions/{sessionsId}:
    parameters: *ref_1
    delete:
      description: >-
        Deletes the interactive session resource. If the session is not in
        terminal state, it is terminated, and then deleted.
      operationId: dataproc.projects.locations.sessions.delete
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Operation'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionsId
          required: true
          schema:
            type: string
        - in: query
          name: requestId
          schema:
            type: string
    get:
      description: Gets the resource representation for an interactive session.
      operationId: dataproc.projects.locations.sessions.get
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Session'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: sessionsId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/operations/{operationsId}:
    parameters: *ref_1
    get:
      description: >-
        Gets the latest state of a long-running operation. Clients can use this
        method to poll the operation result at intervals as recommended by the
        API service.
      operationId: dataproc.projects.locations.operations.get
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Operation'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: operationsId
          required: true
          schema:
            type: string
    delete:
      description: >-
        Deletes a long-running operation. This method indicates that the client
        is no longer interested in the operation result. It does not cancel the
        operation. If the server doesn't support this method, it returns
        google.rpc.Code.UNIMPLEMENTED.
      operationId: dataproc.projects.locations.operations.delete
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Empty'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: operationsId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/operations/{operationsId}:cancel:
    parameters: *ref_1
    post:
      description: >-
        Starts asynchronous cancellation on a long-running operation. The server
        makes a best effort to cancel the operation, but success is not
        guaranteed. If the server doesn't support this method, it returns
        google.rpc.Code.UNIMPLEMENTED. Clients can use Operations.GetOperation
        or other methods to check whether the cancellation succeeded or whether
        the operation completed despite cancellation. On successful
        cancellation, the operation is not deleted; instead, it becomes an
        operation with an Operation.error value with a google.rpc.Status.code of
        1, corresponding to Code.CANCELLED.
      operationId: dataproc.projects.locations.operations.cancel
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Empty'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: operationsId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/operations:
    parameters: *ref_1
    get:
      description: >-
        Lists operations that match the specified filter in the request. If the
        server doesn't support this method, it returns UNIMPLEMENTED.
      operationId: dataproc.projects.locations.operations.list
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListOperationsResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: query
          name: returnPartialSuccess
          schema:
            type: boolean
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
        - in: query
          name: filter
          schema:
            type: string
        - in: query
          name: pageToken
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/autoscalingPolicies:
    parameters: *ref_1
    post:
      description: Creates new autoscaling policy.
      operationId: dataproc.projects.locations.autoscalingPolicies.create
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/AutoscalingPolicy'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/AutoscalingPolicy'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
    get:
      description: Lists autoscaling policies in the project.
      operationId: dataproc.projects.locations.autoscalingPolicies.list
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListAutoscalingPoliciesResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: query
          name: pageToken
          schema:
            type: string
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
  /v1/projects/{projectsId}/locations/{locationsId}/autoscalingPolicies/{autoscalingPoliciesId}:getIamPolicy:
    parameters: *ref_1
    post:
      description: >-
        Gets the access control policy for a resource. Returns an empty policy
        if the resource exists and does not have a policy set.
      operationId: dataproc.projects.locations.autoscalingPolicies.getIamPolicy
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/GetIamPolicyRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Policy'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: autoscalingPoliciesId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/autoscalingPolicies/{autoscalingPoliciesId}:
    parameters: *ref_1
    delete:
      description: >-
        Deletes an autoscaling policy. It is an error to delete an autoscaling
        policy that is in use by one or more clusters.
      operationId: dataproc.projects.locations.autoscalingPolicies.delete
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Empty'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: autoscalingPoliciesId
          required: true
          schema:
            type: string
    put:
      description: >-
        Updates (replaces) autoscaling policy.Disabled check for update_mask,
        because all updates will be full replacements.
      operationId: dataproc.projects.locations.autoscalingPolicies.update
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/AutoscalingPolicy'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/AutoscalingPolicy'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: autoscalingPoliciesId
          required: true
          schema:
            type: string
    get:
      description: Retrieves autoscaling policy.
      operationId: dataproc.projects.locations.autoscalingPolicies.get
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/AutoscalingPolicy'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: autoscalingPoliciesId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/autoscalingPolicies/{autoscalingPoliciesId}:setIamPolicy:
    parameters: *ref_1
    post:
      description: >-
        Sets the access control policy on the specified resource. Replaces any
        existing policy.Can return NOT_FOUND, INVALID_ARGUMENT, and
        PERMISSION_DENIED errors.
      operationId: dataproc.projects.locations.autoscalingPolicies.setIamPolicy
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/SetIamPolicyRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Policy'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: autoscalingPoliciesId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/autoscalingPolicies/{autoscalingPoliciesId}:testIamPermissions:
    parameters: *ref_1
    post:
      description: >-
        Returns permissions that a caller has on the specified resource. If the
        resource does not exist, this will return an empty set of permissions,
        not a NOT_FOUND error.Note: This operation is designed to be used for
        building permission-aware UIs and command-line tools, not for
        authorization checking. This operation may "fail open" without warning.
      operationId: dataproc.projects.locations.autoscalingPolicies.testIamPermissions
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/TestIamPermissionsRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/TestIamPermissionsResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: autoscalingPoliciesId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/batches/{batchesId}/sparkApplications/{sparkApplicationsId}:searchExecutorStageSummary:
    parameters: *ref_1
    get:
      description: Obtain executor summary with respect to a spark stage attempt.
      operationId: >-
        dataproc.projects.locations.batches.sparkApplications.searchExecutorStageSummary
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/SearchSparkApplicationExecutorStageSummaryResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: batchesId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: stageId
          schema:
            type: string
            format: int64
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
        - in: query
          name: parent
          schema:
            type: string
        - in: query
          name: stageAttemptId
          schema:
            type: integer
            format: int32
        - in: query
          name: pageToken
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/batches/{batchesId}/sparkApplications/{sparkApplicationsId}:searchStageAttempts:
    parameters: *ref_1
    get:
      description: >-
        Obtain data corresponding to a spark stage attempts for a Spark
        Application.
      operationId: >-
        dataproc.projects.locations.batches.sparkApplications.searchStageAttempts
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/SearchSparkApplicationStageAttemptsResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: batchesId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
        - in: query
          name: pageToken
          schema:
            type: string
        - in: query
          name: stageId
          schema:
            type: string
            format: int64
        - in: query
          name: summaryMetricsMask
          schema:
            type: string
            format: google-fieldmask
  /v1/projects/{projectsId}/locations/{locationsId}/batches/{batchesId}/sparkApplications/{sparkApplicationsId}:summarizeStages:
    parameters: *ref_1
    get:
      description: Obtain summary of Stages for a Spark Application
      operationId: dataproc.projects.locations.batches.sparkApplications.summarizeStages
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/SummarizeSparkApplicationStagesResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: batchesId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/batches/{batchesId}/sparkApplications/{sparkApplicationsId}:searchSqlQueries:
    parameters: *ref_1
    get:
      description: Obtain data corresponding to SQL Queries for a Spark Application.
      operationId: dataproc.projects.locations.batches.sparkApplications.searchSqlQueries
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/SearchSparkApplicationSqlQueriesResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: batchesId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: details
          schema:
            type: boolean
        - in: query
          name: parent
          schema:
            type: string
        - in: query
          name: pageToken
          schema:
            type: string
        - in: query
          name: planDescription
          schema:
            type: boolean
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
  /v1/projects/{projectsId}/locations/{locationsId}/batches/{batchesId}/sparkApplications/{sparkApplicationsId}:accessStageAttempt:
    parameters: *ref_1
    get:
      description: >-
        Obtain data corresponding to a spark stage attempt for a Spark
        Application.
      operationId: dataproc.projects.locations.batches.sparkApplications.accessStageAttempt
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/AccessSparkApplicationStageAttemptResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: batchesId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: summaryMetricsMask
          schema:
            type: string
            format: google-fieldmask
        - in: query
          name: stageId
          schema:
            type: string
            format: int64
        - in: query
          name: stageAttemptId
          schema:
            type: integer
            format: int32
        - in: query
          name: parent
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/batches/{batchesId}/sparkApplications/{sparkApplicationsId}:searchStageAttemptTasks:
    parameters: *ref_1
    get:
      description: >-
        Obtain data corresponding to tasks for a spark stage attempt for a Spark
        Application.
      operationId: >-
        dataproc.projects.locations.batches.sparkApplications.searchStageAttemptTasks
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/SearchSparkApplicationStageAttemptTasksResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: batchesId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
        - in: query
          name: sortRuntime
          schema:
            type: boolean
        - in: query
          name: stageId
          schema:
            type: string
            format: int64
        - in: query
          name: pageToken
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
        - in: query
          name: stageAttemptId
          schema:
            type: integer
            format: int32
        - in: query
          name: taskStatus
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/batches/{batchesId}/sparkApplications/{sparkApplicationsId}:accessStageRddGraph:
    parameters: *ref_1
    get:
      description: >-
        Obtain RDD operation graph for a Spark Application Stage. Limits the
        number of clusters returned as part of the graph to 10000.
      operationId: >-
        dataproc.projects.locations.batches.sparkApplications.accessStageRddGraph
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/AccessSparkApplicationStageRddOperationGraphResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: batchesId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
        - in: query
          name: stageId
          schema:
            type: string
            format: int64
  /v1/projects/{projectsId}/locations/{locationsId}/batches/{batchesId}/sparkApplications/{sparkApplicationsId}:access:
    parameters: *ref_1
    get:
      description: >-
        Obtain high level information corresponding to a single Spark
        Application.
      operationId: dataproc.projects.locations.batches.sparkApplications.access
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/AccessSparkApplicationResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: batchesId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/batches/{batchesId}/sparkApplications/{sparkApplicationsId}:write:
    parameters: *ref_1
    post:
      description: Write wrapper objects from dataplane to spanner
      operationId: dataproc.projects.locations.batches.sparkApplications.write
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WriteSparkApplicationContextRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/WriteSparkApplicationContextResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: batchesId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/batches/{batchesId}/sparkApplications/{sparkApplicationsId}:searchJobs:
    parameters: *ref_1
    get:
      description: Obtain list of spark jobs corresponding to a Spark Application.
      operationId: dataproc.projects.locations.batches.sparkApplications.searchJobs
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/SearchSparkApplicationJobsResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: batchesId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: jobStatus
          schema:
            type: string
        - in: query
          name: pageToken
          schema:
            type: string
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
        - in: query
          name: parent
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/batches/{batchesId}/sparkApplications:search:
    parameters: *ref_1
    get:
      description: >-
        Obtain high level information and list of Spark Applications
        corresponding to a batch
      operationId: dataproc.projects.locations.batches.sparkApplications.search
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/SearchSparkApplicationsResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: batchesId
          required: true
          schema:
            type: string
        - in: query
          name: minTime
          schema:
            type: string
            format: google-datetime
        - in: query
          name: applicationStatus
          schema:
            type: string
        - in: query
          name: maxEndTime
          schema:
            type: string
            format: google-datetime
        - in: query
          name: minEndTime
          schema:
            type: string
            format: google-datetime
        - in: query
          name: pageToken
          schema:
            type: string
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
        - in: query
          name: maxTime
          schema:
            type: string
            format: google-datetime
  /v1/projects/{projectsId}/locations/{locationsId}/batches/{batchesId}/sparkApplications/{sparkApplicationsId}:accessEnvironmentInfo:
    parameters: *ref_1
    get:
      description: Obtain environment details for a Spark Application
      operationId: >-
        dataproc.projects.locations.batches.sparkApplications.accessEnvironmentInfo
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/AccessSparkApplicationEnvironmentInfoResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: batchesId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/batches/{batchesId}/sparkApplications/{sparkApplicationsId}:searchExecutors:
    parameters: *ref_1
    get:
      description: Obtain data corresponding to executors for a Spark Application.
      operationId: dataproc.projects.locations.batches.sparkApplications.searchExecutors
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/SearchSparkApplicationExecutorsResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: batchesId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: executorStatus
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
        - in: query
          name: pageToken
          schema:
            type: string
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
  /v1/projects/{projectsId}/locations/{locationsId}/batches/{batchesId}/sparkApplications/{sparkApplicationsId}:summarizeExecutors:
    parameters: *ref_1
    get:
      description: Obtain summary of Executor Summary for a Spark Application
      operationId: dataproc.projects.locations.batches.sparkApplications.summarizeExecutors
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/SummarizeSparkApplicationExecutorsResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: batchesId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/batches/{batchesId}/sparkApplications/{sparkApplicationsId}:accessSqlQuery:
    parameters: *ref_1
    get:
      description: >-
        Obtain data corresponding to a particular SQL Query for a Spark
        Application.
      operationId: dataproc.projects.locations.batches.sparkApplications.accessSqlQuery
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/AccessSparkApplicationSqlQueryResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: batchesId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
        - in: query
          name: planDescription
          schema:
            type: boolean
        - in: query
          name: details
          schema:
            type: boolean
        - in: query
          name: executionId
          schema:
            type: string
            format: int64
  /v1/projects/{projectsId}/locations/{locationsId}/batches/{batchesId}/sparkApplications/{sparkApplicationsId}:accessJob:
    parameters: *ref_1
    get:
      description: Obtain data corresponding to a spark job for a Spark Application.
      operationId: dataproc.projects.locations.batches.sparkApplications.accessJob
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/AccessSparkApplicationJobResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: batchesId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: jobId
          schema:
            type: string
            format: int64
        - in: query
          name: parent
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/batches/{batchesId}/sparkApplications/{sparkApplicationsId}:accessSqlPlan:
    parameters: *ref_1
    get:
      description: >-
        Obtain Spark Plan Graph for a Spark Application SQL execution. Limits
        the number of clusters returned as part of the graph to 10000.
      operationId: dataproc.projects.locations.batches.sparkApplications.accessSqlPlan
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/AccessSparkApplicationSqlSparkPlanGraphResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: batchesId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: executionId
          schema:
            type: string
            format: int64
        - in: query
          name: parent
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/batches/{batchesId}/sparkApplications/{sparkApplicationsId}:summarizeStageAttemptTasks:
    parameters: *ref_1
    get:
      description: Obtain summary of Tasks for a Spark Application Stage Attempt
      operationId: >-
        dataproc.projects.locations.batches.sparkApplications.summarizeStageAttemptTasks
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: >-
                  #/components/schemas/SummarizeSparkApplicationStageAttemptTasksResponse
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: batchesId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
        - in: query
          name: stageAttemptId
          schema:
            type: integer
            format: int32
        - in: query
          name: stageId
          schema:
            type: string
            format: int64
  /v1/projects/{projectsId}/locations/{locationsId}/batches/{batchesId}/sparkApplications/{sparkApplicationsId}:summarizeJobs:
    parameters: *ref_1
    get:
      description: Obtain summary of Jobs for a Spark Application
      operationId: dataproc.projects.locations.batches.sparkApplications.summarizeJobs
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/SummarizeSparkApplicationJobsResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: batchesId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/batches/{batchesId}/sparkApplications/{sparkApplicationsId}:searchStages:
    parameters: *ref_1
    get:
      description: Obtain data corresponding to stages for a Spark Application.
      operationId: dataproc.projects.locations.batches.sparkApplications.searchStages
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/SearchSparkApplicationStagesResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: batchesId
          required: true
          schema:
            type: string
        - in: path
          name: sparkApplicationsId
          required: true
          schema:
            type: string
        - in: query
          name: summaryMetricsMask
          schema:
            type: string
            format: google-fieldmask
        - in: query
          name: pageToken
          schema:
            type: string
        - in: query
          name: parent
          schema:
            type: string
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
        - in: query
          name: stageStatus
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/batches:
    parameters: *ref_1
    post:
      description: Creates a batch workload that executes asynchronously.
      operationId: dataproc.projects.locations.batches.create
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/Batch'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Operation'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: query
          name: requestId
          schema:
            type: string
        - in: query
          name: batchId
          schema:
            type: string
    get:
      description: Lists batch workloads.
      operationId: dataproc.projects.locations.batches.list
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListBatchesResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
        - in: query
          name: filter
          schema:
            type: string
        - in: query
          name: orderBy
          schema:
            type: string
        - in: query
          name: pageToken
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/batches/{batchesId}:
    parameters: *ref_1
    get:
      description: Gets the batch workload resource representation.
      operationId: dataproc.projects.locations.batches.get
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Batch'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: batchesId
          required: true
          schema:
            type: string
    delete:
      description: >-
        Deletes the batch workload resource. If the batch is not in a CANCELLED,
        SUCCEEDED or FAILED State, the delete operation fails and the response
        returns FAILED_PRECONDITION.
      operationId: dataproc.projects.locations.batches.delete
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Empty'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: batchesId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/batches/{batchesId}:analyze:
    parameters: *ref_1
    post:
      description: Analyze a Batch for possible recommendations and insights.
      operationId: dataproc.projects.locations.batches.analyze
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/AnalyzeBatchRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Operation'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: batchesId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/workflowTemplates/{workflowTemplatesId}:setIamPolicy:
    parameters: *ref_1
    post:
      description: >-
        Sets the access control policy on the specified resource. Replaces any
        existing policy.Can return NOT_FOUND, INVALID_ARGUMENT, and
        PERMISSION_DENIED errors.
      operationId: dataproc.projects.locations.workflowTemplates.setIamPolicy
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/SetIamPolicyRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Policy'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: workflowTemplatesId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/workflowTemplates/{workflowTemplatesId}:getIamPolicy:
    parameters: *ref_1
    post:
      description: >-
        Gets the access control policy for a resource. Returns an empty policy
        if the resource exists and does not have a policy set.
      operationId: dataproc.projects.locations.workflowTemplates.getIamPolicy
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/GetIamPolicyRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Policy'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: workflowTemplatesId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/workflowTemplates/{workflowTemplatesId}:instantiate:
    parameters: *ref_1
    post:
      description: >-
        Instantiates a template and begins execution.The returned Operation can
        be used to track execution of workflow by polling operations.get. The
        Operation will complete when entire workflow is finished.The running
        workflow can be aborted via operations.cancel. This will cause any
        inflight jobs to be cancelled and workflow-owned clusters to be
        deleted.The Operation.metadata will be WorkflowMetadata
        (https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#workflowmetadata).
        Also see Using WorkflowMetadata
        (https://cloud.google.com/dataproc/docs/concepts/workflows/debugging#using_workflowmetadata).On
        successful completion, Operation.response will be Empty.
      operationId: dataproc.projects.locations.workflowTemplates.instantiate
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/InstantiateWorkflowTemplateRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Operation'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: workflowTemplatesId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/workflowTemplates/{workflowTemplatesId}:testIamPermissions:
    parameters: *ref_1
    post:
      description: >-
        Returns permissions that a caller has on the specified resource. If the
        resource does not exist, this will return an empty set of permissions,
        not a NOT_FOUND error.Note: This operation is designed to be used for
        building permission-aware UIs and command-line tools, not for
        authorization checking. This operation may "fail open" without warning.
      operationId: dataproc.projects.locations.workflowTemplates.testIamPermissions
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/TestIamPermissionsRequest'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/TestIamPermissionsResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: workflowTemplatesId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/workflowTemplates:
    parameters: *ref_1
    post:
      description: Creates new workflow template.
      operationId: dataproc.projects.locations.workflowTemplates.create
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WorkflowTemplate'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/WorkflowTemplate'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
    get:
      description: Lists workflows that match the specified filter in the request.
      operationId: dataproc.projects.locations.workflowTemplates.list
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListWorkflowTemplatesResponse'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: query
          name: pageSize
          schema:
            type: integer
            format: int32
        - in: query
          name: pageToken
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/workflowTemplates/{workflowTemplatesId}:
    parameters: *ref_1
    delete:
      description: Deletes a workflow template. It does not cancel in-progress workflows.
      operationId: dataproc.projects.locations.workflowTemplates.delete
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Empty'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: workflowTemplatesId
          required: true
          schema:
            type: string
        - in: query
          name: version
          schema:
            type: integer
            format: int32
    get:
      description: >-
        Retrieves the latest workflow template.Can retrieve previously
        instantiated template by specifying optional version parameter.
      operationId: dataproc.projects.locations.workflowTemplates.get
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/WorkflowTemplate'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: workflowTemplatesId
          required: true
          schema:
            type: string
        - in: query
          name: version
          schema:
            type: integer
            format: int32
    put:
      description: >-
        Updates (replaces) workflow template. The updated template must contain
        version that matches the current server version.
      operationId: dataproc.projects.locations.workflowTemplates.update
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WorkflowTemplate'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/WorkflowTemplate'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: path
          name: workflowTemplatesId
          required: true
          schema:
            type: string
  /v1/projects/{projectsId}/locations/{locationsId}/workflowTemplates:instantiateInline:
    parameters: *ref_1
    post:
      description: >-
        Instantiates a template and begins execution.This method is equivalent
        to executing the sequence CreateWorkflowTemplate,
        InstantiateWorkflowTemplate, DeleteWorkflowTemplate.The returned
        Operation can be used to track execution of workflow by polling
        operations.get. The Operation will complete when entire workflow is
        finished.The running workflow can be aborted via operations.cancel. This
        will cause any inflight jobs to be cancelled and workflow-owned clusters
        to be deleted.The Operation.metadata will be WorkflowMetadata
        (https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#workflowmetadata).
        Also see Using WorkflowMetadata
        (https://cloud.google.com/dataproc/docs/concepts/workflows/debugging#using_workflowmetadata).On
        successful completion, Operation.response will be Empty.
      operationId: dataproc.projects.locations.workflowTemplates.instantiateInline
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WorkflowTemplate'
      security:
        - Oauth2:
            - https://www.googleapis.com/auth/cloud-platform
          Oauth2c:
            - https://www.googleapis.com/auth/cloud-platform
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Operation'
      parameters:
        - in: path
          name: projectsId
          required: true
          schema:
            type: string
        - in: path
          name: locationsId
          required: true
          schema:
            type: string
        - in: query
          name: requestId
          schema:
            type: string
